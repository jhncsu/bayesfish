# Model Fitting {#Model-fit}

## Introduction

What is a model\index{model fitting} in a fisheries context? It typically means one or more equations that describe a biological process of interest. There are several good reasons for developing a model. A model that simulates a field study can be used for planning purposes, before field work is conducted. Such a model might be used to determine the number of fish to tag or the level of electrofishing effort, in order for the field work to produce a useful result. A model can also be used to test an analytical approach. For example, we could generate simulated data and see whether our analysis agrees with the known true value(s). The other main use of a model is to analyze data from field work. One example from Chapter \@ref(R-intro) is a two-sample mark-recapture\index{mark-recapture} study to estimate population size. Fish in the first sample are tagged, and the fraction of marked fish in the second sample provides the information needed to estimate population size. The model in that case is the equation for population size (N~hat~), along with some assumptions about sampling. The equation is based on the assumption that the marked fraction of the second sample should (on average) be the same as the marked fraction in the underlying population. This method also requires some other assumptions that are more practical in nature, to help ensure that our field methods approximate the underlying model structure. For example, we assume that tagged and untagged fish are well mixed. In practice, this means that tagging effort should be well distributed throughout the study area so that our recapture effort does not encounter pockets with too many or too few tagged fish. We also assume that tags are not shed\index{tag loss} and that there are no deaths due to tagging\index{tagging mortality}. The mark-recapture model assumes that we know how many fish are in the tagged subpopulation. Tag loss or tagging mortality affect that number and would introduce bias.

A key result of model fitting is estimating the level of uncertainty in parameter estimates. The sections that follow illustrate two approaches for estimating that uncertainty, first through simulation then a Bayesian statistical framework. Hopefully the comparison between approaches will be helpful in understanding how a Bayesian analysis works.

## Using simulation {#Model-fit-sim}

The equation for a two-sample mark-recapture\index{mark-recapture} study provides a point estimate of population size, but we could improve the model by including the sampling process. For example, we could assume that the number of marked fish in the second sample comes from a binomial distribution\index{binomial distribution}. That distributional assumption allows us to generate estimates of uncertainty as well as the point estimate. The key to this extended model is to think about the process generating the field data and determine which statistical distribution is appropriate.

We begin with the deterministic version, using the same estimator as in Chapter \@ref(R-intro). The two samples could be fixed values and could differ, but here we assume they are equal in size, based on the capture probability\index{capture probability} (p.true). The number of recaptures in the second sample equals the expected number based on the fraction of the population that is marked (p.true). Because m~2~ takes on its expected value based on p.true, the estimate (N~hat~) equals the true value (see Environment window).

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Two-sample population estimate
N.true <- 400  # Arbitrary assumed population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- n2 * p.true # Marked fish in second sample
N.hat <- n1*n2/m2 # Estimated population size
```

Now, let's extend the model by introducing random variation (stochasticity) in the number of recaptures. We assume that recaptures are the successes in a binomial trial\index{binomial distribution} and replace the deterministic calculation for m~2~ with the following line of code, using the <code>rbinom()</code> function:

```{r eval=FALSE}
m2 <- rbinom(n=1, prob=n1/N.true, size=n2) # Marked fish, second sample
```

The size of the binomial trial is n~2~. The probability of success equals p.true here but we calculate it as n~1~/N.true in case n~1~ and n~2~ were given other fixed values. Running the block of code simulates a single mark-recapture\index{mark-recapture} experiment, with N.hat varying randomly according to the binomially-distributed sampling process.

An easier way of judging the reliability of this mark-recapture\index{mark-recapture} study is to view results from a large number of replicate experiments. Using n=Reps in the <code>rbinom()</code> function produces a vector of recaptures, simulating a mark-recapture\index{mark-recapture} study repeated Reps times. Using that vector in the equation for Nhat.vec now produces a vector of population estimates.  Histogram plots (Figure \@ref(fig:Bootstrap)) shows that estimates are generally close to the true value but tend to have a long tail to the right (overestimating N.true when a low value is drawn for the number of recaptures). 

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Extended model to include stochasticity 
N.true <- 400  # Arbitrary assumed population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample

Reps <- 1000
m2.vec <- rbinom(n=Reps, prob=n1/N.true, size=n2)
Nhat.vec <- n1 * n2 / m2.vec

hist(m2.vec, xlab="Number of recaptures", main="")
hist(Nhat.vec, xlab="Population estimate", main="")

mean(Nhat.vec)
quantile(Nhat.vec, probs=c(0.025, 0.5, 0.975), na.rm=TRUE)
# quantile function provides empirical confidence bounds and median
```


```{r Bootstrap, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Frequency distributions for number of recaptured fish (top) and estimated population size (bottom), based on a simulated mark-recapture study using a binomial distribution for the number of recaptures.', fig.show="hold", fig.height=3, out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result

par(mar = c(4, 4, 1, 2))
# Two-sample population estimate
N.true <- 400  # Arbitrary assumed population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Number caught and marked in first sample
n2 <- N.true * p.true # Number caught and examined for marks in second sample
Reps <- 1000
m2.vec <- rbinom(n=Reps, prob=n1/N.true, size=n2)
Nhat.vec <- n1 * n2 / m2.vec

hist(m2.vec, xlab="Number of recaptures", main="")
hist(Nhat.vec, xlab="Population estimate", main="")
```

Results printed to the Console show that the mean is higher than the true value (due to the tail of extreme values) but the median is very close to N.true. The estimated median and 95% bounds are obtained using the <code>quantile()</code> function. We provide a vector of probabilities and the function returns the estimates at those points. The 0.025 and 0.975 points provide the empirical 95% bounds, and the 0.5 point is the median.

## Using Bayesian methods {#JAGS-model-fit}

In Section \@ref(Model-fit-sim), we used a simulation-based or numerical approach to estimate confidence limits for the mark-recapture\index{mark-recapture} study design. This is an example of parametric bootstrapping [@efron.tibshirani_1993], where replicate samples (number of recaptured fish in this case) are drawn from a parametric distribution, in this case binomial\index{binomial distribution} (Section \@ref(BinomialDist)).  We can compare this simulation approach to a formal statistical method, using a Bayesian framework. We use the same binomial distribution to describe the process of obtaining recaptures.  Our results will be similar, and hopefully the comparison will provide some intuition about how a Bayesian analysis works. Our use of simulation in the remainder of the book will be limited to generating "sample" data. Model fitting will be done using Bayesian methods that work well for simple or complex models.

The two main paradigms for statistical methods are frequentist\index{frequentist paradigm} and Bayesian\index{Bayesian paradigm}.  Frequentist methods are the classical approach for estimating model parameters and making inferences  [@link.etal_2002; @royle.dorazio_2008]. In a frequentist framework, model parameters are assumed to be fixed but unknown values. For example, we could conduct a two-sample mark-recapture study to estimate population size in a lake. If we repeated the mark-recapture study many times, we would expect the 95% confidence limit for each estimate to contain the true population size 95 percent of the time (hence the term frequentist).

A Bayesian\index{Bayesian paradigm} analysis is based on the data in hand and does not rely on the idea of hypothetical replicate studies [@mccarthy_2007; @royle.dorazio_2008]. The results are exact for any sample size [@kéry.schaub_2012], which is an important advantage for fisheries studies because sample sizes are often small.  Bayesian statistics also differs from frequentist statistics in that it takes into account what is known before the study is conducted [@mccarthy_2007]. The requirement to specify prior knowledge can be viewed as either an advantage or disadvantage, depending on the availability of prior data and one's statistical philosophy [@ellison_2004; @mccarthy_2007;  @kéry_2010; @kéry.schaub_2012; @dorazio_2016; @doll.jacquemin_2018; @banner.etal_2020]. The prior information takes the form of a statistical distribution that defines the range and relative likelihood of possible values, prior to collecting the new data. The prior distribution\index{prior distribution}, or simply prior, can range from uninformative\index{uninformative prior distribution} to informative\index{informative prior distribution}. For example, a uniform 0-1 distribution\index{uniform distribution} would be a uninformative prior\index{uninformative prior distribution} for a tag-reporting rate\index{tag-reporting rate}, because probabilities are by definition between 0 and 1. Results from a pilot study might allow for an informative prior\index{informative prior distribution}, for example, a uniform distribution\index{uniform distribution} between 0.2 and 0.4.

Bayes' rule provides the formal structure for combining the prior information and new data in order to estimate the posterior distribution\index{posterior distribution} [@link.etal_2002; @kéry.schaub_2012]. For our purposes, it is sufficient to know that the posterior distribution is proportional to the product of the likelihood\index{likelihood} (based on the new data; Section \@ref(Likelihood)) and the prior\index{prior distribution}:

$posterior \propto likelihood \cdot prior$

[@lunn.etal_2012]. The details in applying Bayes' rule are handled automatically by the software used in this book. Nevertheless, knowing this relationship helps in understanding how the two components work together. Being required to specify the prior distribution\index{prior distribution} makes a Bayesian analysis very transparent [@kéry.schaub_2012]. Anyone reviewing the results can decide about the appropriateness of the prior(s). This would be particularly important in cases where a prior was based on belief (e.g., expert opinion) rather than prior studies.

For most analyses in this book, I adopt the common approach of using flat or vague priors that are intended to be uninformative\index{uninformative prior distribution} [@royle.dorazio_2008; @kéry.schaub_2012]. This approach is sometimes referred to as an objective Bayes analysis [@link.etal_2002]. The simplest example is using a uniform(0,1) distribution\index{uniform distribution} for probabilities. In cases where choosing an uninformative prior distribution\index{uninformative prior distribution} is more subjective (e.g., average maximum size for a growth curve), the analysis can be repeated using narrower and wider priors to ensure that results are insensitive to the bounds [@kéry.schaub_2012]. The ideal situation is for the likelihood\index{likelihood} term to dominate, which is often referred to as the "data overwhelming the prior." This would mean that the sample size for new data was sufficiently large that the prior had little or no influence on results. Typically the results obtained using uninformative priors\index{uninformative prior distribution} will be very similar to results of a frequentist\index{frequentist paradigm} analysis [@link.etal_2002; @mccarthy_2007; @kéry_2010; @kéry.schaub_2012].

@banner.etal_2020 encourage investigators to think carefully about what is known and to take advantage of the Bayesian capability of using prior information, rather than automatically using "default" priors\index{prior distribution} such as a uniform\index{uniform distribution} 0-1 distribution for a probability. Informative priors\index{informative prior distribution} can improve the precision (and sometimes accuracy) of study results [e.g., @doll.jacquemin_2018]. Considerable judgement may be required when selecting relevant data for a prior [@lunn.etal_2012]. A fisheries example could be whether to use growth information from a prior year when population size was higher. Another case could be whether to use a tag-reporting rate\index{tag-reporting rate} from a prior year, if over time there was increased angler dissatisfaction with the current management approach. Sections \@ref(InformativePriors) and \@ref(Otolith) provide additional information about choosing a prior distribution and the use of an informative prior\index{informative prior distribution}.

### Example: mark-recapture {#FirstExampleBayesian}

The following code provides for a Bayesian model fit to the mark-recapture\index{mark-recapture} data, using R packages for JAGS [@plummer2003]. The code should transfer with little or no modification to other BUGS-family software versions.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- n2 * p.true # Marked fish in second sample

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code
sink("TwoSampleCR.txt")
cat("
    model {

  # Prior
    N.hat ~ dlnorm(0, 1E-6) # uninformative prior (N.hat>0)
    MarkedFraction <- n1/N.hat

  # Likelihood
    # Binomial distribution for observed recaptures
    m2 ~ dbin(MarkedFraction, n2)
    }

    ",fill = TRUE)
sink()

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(N.hat= rlnorm(n=1, meanlog=10, sdlog=1))}

model.file <- 'TwoSampleCR.txt'

# Parameters monitored
jags.params <- c("N.hat", "MarkedFraction")

# Call JAGS from R
jagsfit <- jags(data=jags.data, inits=jags.inits, jags.params,
                n.chains = 3, n.thin = 1, n.iter = 2000,
                n.burnin = 1000, model.file)
print(jagsfit, digits=3)
plot(jagsfit)
```

The first section of code sets up the experiment, with arbitrary values for population size and capture probability\index{capture probability} (p.true). The "observed" values for n~1~, n~2~ and m~2~ could be replaced by real data from a field study; in that case, N would be unknown.

Before the first time of running JAGS code, you will need to download and install the current version of [JAGS](https://sourceforge.net/projects/mcmc-jags/files/). Next, use RStudio to install the packages for running JAGS. There are several options; this book uses **rjags** [@R-rjags] and **R2jags** [@R-R2jags].  Both of these steps are done only once. Then each time that you want to access JAGS from R, you load the two packages using the <code>library()</code> function.

The <code>cat()</code> function concatenates (merges together) the lines of JAGS code, which are written out as a text file using the <code>sink()</code> function. The name of the external file is arbitrary but just needs to match the model.file code further below. It can be helpful to use an informative name for the text file rather than something generic that might get overwritten or be hard to locate at a later date. The JAGS code can be thought of as consisting of two main parts: prior information and analysis of the new data. These two parts can come in any order but it seems logical to put the prior information first. In this case, our model parameter, N.hat, can only take on positive values so we use an uninformative lognormal\index{lognormal distribution} [@mccarthy_2007] as the prior distribution\index{prior distribution}. The analysis of new data uses the likelihood\index{likelihood} (see Section \@ref(Likelihood)) which is calculated in the final line of JAGS code. The observed number of recaptures is assumed to be binomally-distributed\index{binomial distribution}. The parameter N.hat is used to estimate the fraction of the population that is marked, which is the probability for the binomial function <code>dbin()</code>.

The next few lines of R code provide settings for the JAGS analysis (data to pass in, initial values for parameters, file name for the JAGS code, parameters to return). The log-scale mean for the initial value of N.hat was set at a high value to reduce the risk of a run-time JAGS error due to a logical inconsistency (e.g., population size smaller than an observed sample catch). Parameters (true model parameters and functions of those parameters) for which we want JAGS to monitor and return results are listed in jags.params. The function call to <code>jags()</code> links to the data, initial values, etc., and provides some settings for the estimation process.

JAGS uses simulation techniques referred to as Markov Chain Monte Carlo\index{Markov Chain Monte Carlo (MCMC)} or MCMC\index{MCMC|see{Markov Chain Monte Carlo}}\index{Markov Chain Monte Carlo (MCMC)} methods to draw samples of each parameter [@mccarthy_2007; @kéry.schaub_2012]. Each sample in a Markov Chain depends on the previous sample so successive values are autocorrelated\index{autocorrelation}. Given a sufficient number of draws, the sampled distribution approximates the posterior distribution\index{posterior distribution} for that parameter. To judge convergence\index{convergence} to the posterior distribution\index{posterior distribution}, the usual practice is to use multiple chains from separate streams of pseudo-random numbers and starting from different initial values. Three chains is a good practical choice. Thinning\index{thinning} (e.g., n.thin=10 to retain only every tenth MCMC result) is sometimes done to reduce the autocorrelation\index{autocorrelation} or memory requirements for long runs [@gelman.hill_2007], but is not generally necessary [@link.eaton2012]. 

The number of MCMC\index{Markov Chain Monte Carlo (MCMC)} iterations is arbitrary but it is better to err on the side of having too many (and can provide a valuable opportunity to stretch legs or get coffee). R returns a measure of convergence\index{convergence} (Rhat\index{Rhat}) that can be useful in deciding if a larger number of iterations is needed. The estimate of Rhat is somewhat like an analysis of variance, in that it is a ratio comparing between- and within-chain variability [@kéry.schaub_2012]. The two types of variability should be similar if chains have converged and are providing samples from the posterior distribution\index{posterior distribution}. Rhat values less than 1.05 [@lunn.etal_2012] or 1.1 [@gelman.hill_2007] can indicate acceptable convergence, with values closer to 1 being preferred. More details about judging convergence\index{convergence} are in Section \@ref(Convergence). The final MCMC\index{Markov Chain Monte Carlo (MCMC)} setting is for the "burn-in" phase, where initial estimates are discarded so that the retained estimates are considered to be unaffected by the initial values. The default if n.burnin is not specified is to discard half the total number of updates. The best way to develop experience and intuition about MCMC\index{Markov Chain Monte Carlo (MCMC)} settings (and Bayesian analysis in general) is to run and re-run code using different settings. For example, are the results different if n.iter is set to 10000?

Results for this case are contained in the arbitrarily named 'jagsfit' object, which can be printed to the Console and viewed as plots. Summary statistics are produced for the two monitored parameters and deviance\index{deviance} (Section \@ref(Deviance)), which is a measure of fit. Convergence\index{convergence} was very good based on estimated Rhat values less than 1.01. The printed output includes n.eff, which is an estimate of the effective sample size. The effective sample size\index{effective sample size} is an estimate of what the sample size would be if the simulation draws were independent (rather than autocorrelated\index{autocorrelation} MCMC\index{Markov Chain Monte Carlo (MCMC)} values). @gelman.hill_2007 recommend an effective sample size\index{effective sample size} of at least 100. For this simple model, the effective sample size\index{effective sample size} for N.hat was the same as the total number of saved iterations.

Summary statistics include the 2.5% and 97.5% percentiles from the estimated posterior distribution\index{posterior distribution}, which I use as a 95% Bayesian credible interval\index{credible interval}. This range has a 0.95 probability of containing the true value [@mccarthy_2007]. Bayesian credible intervals and frequentist confidence intervals are typically very similar if Bayesian priors are uninformative\index{uninformative prior distribution} [@mccarthy_2007; @kéry.schaub_2012]. The credible interval\index{credible interval} in one simulation of our mark-recapture\index{mark-recapture} study was 277 to 670, with a median of 410 and mean of 425. Your results will differ slightly because of the pseudo-randomness of the MCMC\index{Markov Chain Monte Carlo (MCMC)} process. The credible interval was not too different from the estimate obtained through simulation (three simulation runs produced 278-711, 278-640, and 278-640). (The upper bound estimates of 640 and 711 occur with ten and nine recaptures, respectively.)

The estimated posterior distribution\index{posterior distribution} for N.hat (Figure \@ref(fig:BayesMR)) can be obtained from a histogram plot of values from the jagsfit object. The <code>abline()</code> function adds a dotted vertical line for the true value.

```{r eval=FALSE}
hist(jagsfit$BUGSoutput$sims.list$N.hat, main="",
     xlab="Population estimates")
abline(v=N.true, lty=3, lwd=3)
```

```{r BayesMR, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Estimated posterior distribution for population size from a two-sample mark-recapture study with a capture probability of 0.2; dotted vertical line indicates true value.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(11111) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

MCR_model <- function() {
  # Prior
  N.hat ~ dlnorm(0, 1E-6) # Uninformative prior (N.hat>0)
  MarkedFraction <- n1/N.hat
  
  # Likelihood
  # Binomial distribution for observed recaptures
  m2 ~ dbin(MarkedFraction, n2)
}

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Number caught and marked in first sample
n2 <- N.true * p.true # Number caught and examined for marks in second sample
m2 <- n2 * p.true # Number of marked fish in second sample

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(N.hat= rlnorm(n=1, meanlog=10, sdlog=1))}

# Fit the model
MCR_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 2000, n.burnin = 1000,
    model.file = MCR_model,
    parameters.to.save = c("N.hat")
  )

hist(MCR_jags$BUGSoutput$sims.list$N.hat, main="", xlab="Population estimates")
abline(v=N.true, lty=3, lwd=3)

```

The jagsfit object can be inspected in the Environment window. Note that sims.list arrays contain 3000 elements, which is the combined length of 1000 retained values (2000-n.burnin) from each of three chains. (When entering a jagsfit object in the Source window or Console, typing $ after each level of the object will bring up an RStudio prompt for the next choice.)

The model results include pD\index{pD}\index{pD|seealso{effective parameters}}, which is an estimate of the number of effective parameters\index{effective parameters} in the model. In simple models, we can often count the number of nominal parameters, but the number of effective parameters\index{effective parameters} may be less; for example, parameters may be correlated or constrained by an informative prior distribution\index{informative prior distribution} [@mccarthy_2007]. JAGS uses numerical methods to estimate pD\index{pD}. For this model, I would expect pD\index{pD} to be close to 1 for the estimate of N.hat; MarkedFraction is a calculated value and not a model parameter.

JAGS also provides an estimate of the Deviance Information Criterion\index{Deviance Information Criterion (DIC)} (DIC\index{DIC|see {Deviance Information Criterion}}\index{Deviance Information Criterion (DIC)}), which is a relative measure of how well a model fits the data [@mccarthy_2007; @lunn.etal2009]. It is the Bayesian analog of Akaike's Information Criterion\index{Akaike's Information Criterion, (AIC)} (AIC\index{AIC|see{Akaike's Information Criterion}}) which is widely used in a frequentist setting to compare alternative models. Like the AIC, the DIC\index{Deviance Information Criterion (DIC)} results in a trade-off between model fit (likelihood\index{likelihood}) and complexity (number of parameters) [@mccarthy_2007]. The DIC\index{Deviance Information Criterion (DIC)} score is calculated using a measure of fit (deviance\index{deviance} at the mean of the posterior distribution) and a measure of model complexity (pD\index{pD}); lower values are better when comparing alternative models. DIC\index{Deviance Information Criterion (DIC)} scores should be interpreted with caution because of the difficulty in estimating pD\index{pD} for some models [@lunn.etal2009].

This model may also be fitted by using MarkedFraction as the model parameter. It is easy to specify the uninformative prior\index{uninformative prior distribution} (and initial values) in this case, as MarkedFraction is a proportion between 0 and 1. Now N.hat is a calculated value. One major advantage of Bayesian software is that we automatically get full information about parameters that are calculated functions of model parameters. In this case, the calculated value (N.hat) is of primary interest, rather than the actual model parameter MarkedFraction. Results for N.hat characterize its posterior distribution\index{posterior distribution}, which takes into account the prior information (none here) and the new data.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- n2 * p.true # Marked fish in second sample

# JAGS code
sink("TwoSampleCR.txt")
cat("
    model {

    # Priors
    MarkedFraction ~ dunif(0, 1)

    # Calculated value
    N.hat <- n1 /MarkedFraction

    # Likelihood
    # Binomial distribution for observed recaptures
    m2 ~ dbin(MarkedFraction, n2)
    }

    ",fill = TRUE)
sink()

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(MarkedFraction=runif(1, min=0, max=1))}

model.file <- 'TwoSampleCR.txt'

# Parameters monitored
jags.params <- c("N.hat", "MarkedFraction")

# Call JAGS from R
jagsfit <- jags(data=jags.data, inits=jags.inits, jags.params,
                n.chains = 3, n.thin = 1, n.iter = 2000, 
                n.burnin = 1000, model.file)
print(jagsfit, digits=3)
plot(jagsfit)
```

Try multiple runs using different values for p.true. In planning a mark-recapture study for a pond or small lake, how might you decide on a realistic goal for p.true?

### Debugging code {#Debug}

Anyone who develops new code for fisheries data analysis will wrestle with the inevitable coding errors, not only in JAGS but also the R code that surrounds it. Let's begin by looking at a few examples of R coding errors:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

#1
x <- 2
lg(x) # Incorrect function name

#2
y <- 4
z <- x+y # Typo (should have been x*y)

#3
RandLength <- rnorm(1, 10, 2) # Correct but poorly documented
RandLength <- rnorm(10,2) # Sample size omitted
RandLength <- rnorm(1,10) # sd omitted so default used (incorrectly)
RandLength <- rnorm(n=1, mean=10, sd=2) # Correct and fully documented

#4
x2 <- rnorm(n=1000, mean=10, sd=5)
x3 <- rnorm(n=1000, mean=20, sd=10)
x4 <- x2+x3
mean(x4)
var(x4)

```

The first example is easy to resolve, as we get a helpful error message in the Console. Rechecking the code, or doing an internet search for the correct function name, are approaches for addressing this type of error. The second example, if our intent was to assign x*y to z, is much more dangerous. The code will run but will give the wrong answer because of a typing error. It is always essential to review new code, line by line. Having a colleague (fresh set of eyes) check code can also be productive.  R code can often be executed line by line, and checking calculated values in the Environment window can help in finding this sort of logic error, at least for simpler analyses. 

In the third example, we are simulating a random length observation, drawn from a normal distribution with a mean of 10 and standard deviation of 2. The first version of the code works correctly but is poorly documented. The second example runs but does not produce the intended result. We omitted the first argument and did not include parameter names, so R assumes incorrectly that the first argument is the sample size and the second one is the mean (see *vector* RandLength in the Environment window). The third argument is missing so R uses the default value (sd=1), which in this case is incorrect. Even if we wanted to use an sd of 1, it is better to specify it for clarity. RStudio is helpful in working with functions in that it displays the function arguments and any defaults, once the function name and "(" have been entered. The fourth version works as intended, and makes clear the values that are being used.

The fourth example is coded correctly but illustrates another way of checking code. Simulation makes it easy to use a large sample size, to check sample statistics for x4. We verify that the mean is close to the expected value (10+20). The variance of a sum (of independent variables) equals the sum of the variances so we confirm that the sample variance is close to the expected value (25+100).

How many errors do you find in the following section of code? Review line by line and correct errors before executing the code. Once the code is working correctly, confirm that the sample median is very close to the expected population size:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Two-sample population estimate
N.true <- 400  # Arbitrary assumed population size
p.true <-- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true X p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample

Reps <- 1000
m2.vec <- rbinom(n=Rep, prob=n1/N.true, size=n2)
N.hat <- n1 * n2 / m2.vec
hist(N.hat, xlab="Population estimate", main="")
mean(N.hat)
quantile(N.hat, probs=c(0.5), na.rm=TRUE))  # Sample median
```

Debugging JAGS code can be more challenging. There is no option for executing single lines of code as is possible with R. It is usually a matter of fixing one error at a time until the code runs, then checking and testing the code to be confident that it is working correctly. Consider the following (correctly coded) example for a tagging study to estimate the exploitation rate\index{exploitation rate} (fraction of the population harvested). Our simulation generates the single observation for the number of tags returned (e.g., over a year). This observation is drawn from a binomial distribution\index{binomial distribution} and we use the same JAGS function (<code>dbin()</code>) as in the population estimate examples:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Arbitrary 'observed' values for analysis
N.tagged <- 100
Exp.rate <- 0.4
Tags.returned <- rbinom(n=1, prob=Exp.rate, size=N.tagged)

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code
sink("ExpRate.txt")
cat("
    model {

    # Priors
    Exp.rate.est ~ dunif(0,1)

    # Likelihood
    Tags.returned ~ dbin(Exp.rate.est, N.tagged)
}
    ",fill = TRUE)
sink()

# Bundle data
jags.data <- list("N.tagged", "Tags.returned")

# Initial values.
jags.inits <- function(){ list(Exp.rate.est=runif(n=1, min=0, max=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("Exp.rate.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, inits=jags.inits, jags.params,
                n.chains = 3, n.thin = 1, n.iter = 2000,
                n.burnin = 1000, model.file)
print(jagsfit, digits=3)
plot(jagsfit)
```

Begin by running the code as is, then try substituting each of the following (incorrect) lines, one at a time.

  - `ExpRate.est ~ dunif(0,1)`
  - `Exp.rate.est ~ unif(0,1)`
  - `Tags.returned ~ dbin(Exp.rate.est)`
  
For example, the incorrect line of code could be:

    ExpRate.est ~ dunif(0,1)  #Exp.rate.est ~ dunif(0,1)

Rerunning the code (click on RStudio icon to Re-run the previous code region) produces an error message pointing to line 8 (counting from <code>cat("</code>). JAGS correctly reports that the variable Exp.rate.est has not been previously defined. This is correct, but the error is actually on line 5, where the intended parameter name (Exp.rate.est) was mistyped. The second error is easy to diagnose because it specifies that the "unif" distribution on line 5 is unknown. The third error indicates that a discrete-valued parameter for function <code>dbin()</code> is missing. This makes sense because the size of the experiment (N.tagged) was omitted.

As with R code, the first step is always to review the code line by line. Fitting a model to simulated data is always useful (even if there are field observations) because the expected results are known. Another strategy is to start with the simplest possible analysis, then add complexity one piece at a time. For example, @kéry.schaub_2012 discuss Cormack-Jolly-Seber [@royle_2008] models\index{Cormack-Jolly-Seber model} for estimating survival rate\index{survival rate}, beginning with constant parameters then gradually adding more complexity (e.g., models that allow for time and individual variation).

Skill in debugging comes with experience. The above strategies should be useful in resolving coding errors.

### Judging convergence {#Convergence}
The MCMC\index{Markov Chain Monte Carlo (MCMC)} process is iterative so it is important to ensure that the number of updates (iterations) is sufficient to achieve convergence\index{convergence}. One practical approach is to fit the model with some arbitrary (but reasonably large) number of updates, then refit the model using larger numbers of updates. Results that are stable suggest that convergence\index{convergence} has been achieved. The convergence\index{convergence} statistic Rhat\index{Rhat} [@gelman.rubin_1992; @brooks.gelman_1998; @lunn.etal_2012] is also generally reliable as a measure of convergence.  Especially for models for which convergence appears to be slow, it can also be helpful to view "trace" plots that show how estimates change as updating occurs [@kéry_2010]. Add the following lines of code to the two-sample mark-recapture model used in Section \@ref(JAGS-model-fit):

```{r eval=FALSE}
par(mfrow=c(3,1)) # Multi-frame plot, 3 rows, 1 col
matplot(jagsfit$BUGSoutput$sims.array[,,1], type = "l", 
        ylab="Marked fraction")
matplot(jagsfit$BUGSoutput$sims.array[,,2], type = "l", ylab="N hat")
matplot(jagsfit$BUGSoutput$sims.array[,,3], type = "l", ylab="Deviance")

jagsfit.mcmc <- as.mcmc(jagsfit) # Creates an MCMC object for plotting
plot(jagsfit.mcmc) # Trace and density plots
```

The first approach is based on an example from @kéry_2010. We use the <code>par()</code> graphical function to set up a multi-frame plot (by row), with three rows and one column. This sets up a single RStudio plot window to contain three plots. The next three lines use the built-in <code>matplot()</code> function to plot columns of a matrix as overlaid lines. The Environment window can be viewed to see the structure of the sims.array, a matrix of type num[1:1000, 1:3, 1:3]. The first dimension is for the 1000 retained updates, the second is for chains 1-3, and the third is for the variable (MarkedFraction, N.hat, Deviance\index{deviance}). Omitting the ranges for updates and chains results in a plot containing all retained updates, with a different line color for each chain. We could also specify a subset of the range to see the initial pattern (e.g., ...sims.array[1:100,,1], ...). For this simple model, the estimates converge almost immediately so there is no obvious transitory pattern. Note that there is a lot of wasted space and it may be necessary to increase the size of the Plots window to see the plots adequately.

The final two lines provide a more automated way of plotting the sequence of values (trace) and density (Figure \@ref(fig:TracePlots)), using methods contained in the **coda** [@R-coda] package (loaded automatically by rjags). The <code>as.mcmc()</code> function converts the original output object (jagsfit) into an MCMC object. The plot function here produces a nicely formatted set of trace and density plots that are very helpful in confirming convergence. If convergence\index{convergence} has been achieved, the trace plots should appear to be a "grassy lawn" or "fuzzy caterpillar" without any trend, with overlapping (fully mixed) chains. Density plots show the estimated posterior distributions\index{posterior distribution}, and given convergence, should have an essentially identical pattern for the three chains. Convergence\index{convergence} occurs very quickly here but we will see in later chapters with more complex models that convergence can sometimes be more difficult to achieve (see especially Section \@ref(LnWgt)).

```{r TracePlots, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Trace and density plots for the two-sample mark-recapture model, using methods contained in the coda package.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

MCR_model <- function() {
  # Prior
  N.hat ~ dlnorm(0, 1E-6) # Uninformative prior (N.hat>0)
  MarkedFraction <- n1/N.hat
  
  # Likelihood
  # Binomial distribution for observed recaptures
  m2 ~ dbin(MarkedFraction, n2)
}

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Number caught and marked in first sample
n2 <- N.true * p.true # Number caught and examined for marks in second sample
m2 <- n2 * p.true # Number of marked fish in second sample

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(N.hat=runif(n=1, min=max(n1,n2), max=20000))}

# Fit the model
MCR_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 2000, n.burnin = 1000,
    model.file = MCR_model,
    parameters.to.save = c("MarkedFraction","N.hat")
  )

jagsfit.mcmc <- as.mcmc(MCR_jags) # Creates an MCMC object for plotting
plot(jagsfit.mcmc, col=gray.colors(n=3, start=0, end=0.7)) # Trace and density plots
```

### What is a likelihood?  {#Likelihood}
As mentioned above, Bayesian software uses the likelihood\index{likelihood} and prior information to estimate the posterior distribution\index{posterior distribution}. But what exactly is a likelihood\index{likelihood}? It is an expression or function that is used to estimate the *likelihood* of obtaining the sample observation(s), given some specific value for each model parameter. As an example, let's return to the second version of the mark-recapture\index{mark-recapture} analysis, with the single model parameter MarkedFracton (the fraction of the population that is marked). We use the binomial distribution\index{binomial distribution} to model the number of recaptures. The likelihood function for the binomial is the same as the probability distribution: $L(p | n, k) = {n \choose k} p^{k} (1-p)^{n-k}$, except that here, we estimate the likelihood of a certain value for p, given k successes in a trial of size n. We can look at likelihoods using the same parameter values as in the simulation:


```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- n2 * p.true # Marked fish in second sample

# Calculate likelihood for specific value of p
p <- 0.2
choose(n2,m2) * p^m2 * (1-p)^(n2-m2) # Likelihood for single value of p

dbinom(x=m2, size=n2, prob=p) 
# x successes, built-in function to obtain same result

# Create vector of possible p values, determine likelihood for each
p.vec <- seq(from=0.02, to=1, by=0.02)
l.vec <- choose(n2,m2) * p.vec^m2 * (1-p.vec)^(n2-m2)
par(mfrow=c(1,1)) # Reset plot frame
plot(p.vec, l.vec, ylab="Likelihood", col="red")

# Examine change in likelihood at higher number of recaptures
new.m2 <- 18
new.l.vec <- choose(n2,new.m2) * p.vec^new.m2 * (1-p.vec)^(n2-new.m2)
points(p.vec, new.l.vec, col="blue")
```

The likelihood\index{likelihood} calculation at p=0.2 uses the <code>choose()</code> function, to determine how many ways m2=16 recaptures can be drawn from a sample of size 80. Type `choose(n2,m2)` in the Console to see that it is a very large number! The remainder of the expression determines the probability of 16 successes and 80-16 failures. We can obtain the same result from the built-in function <code>dbinom()</code>, although it seems more instructive to write out the full expression.

The calculated likelihood\index{likelihood} (0.11) is really only of interest as a relative value, compared to other potential values for p. We use the <code>seq()</code> function to create a vector of potential p values, and obtain a vector of likelihoods from the p vector. Plotting the points shows that the likelihood peaks sharply at 0.2 (not surprisingly since that is the value that generated the 16 recaptures) and decreases to near 0 at about 0.08 and 0.32.

We have assumed thus far that we obtained the expected number of recaptures (80*p), but in reality, the number of recaptures would be a binomially-distributed\index{binomial distribution} random variate. A different observed value for m~2~ would shift the likelihood\index{likelihood} function. The next lines of code calculate the likelihood\index{likelihood} for a slightly higher number of recaptures. The <code>points()</code> function adds the new likelihood\index{likelihood} values to the plot, using the col="blue" plot option. The new values are shifted to the right because the number of recaptured fish was higher.

If multiple independent samples are taken, the combined likelihood\index{likelihood} is a product. For our two assumed samples of 80 fish, with 16 recaptures in one and 18 in the other, the combined likelihood\index{likelihood} at each level of p would be the product l.vec * new.l.vec.

We wrote out the expression for the likelihood\index{likelihood} for this example, but that is not needed when using the BUGS language. We need only specify the function name for the distribution describing our sample data. JAGS does the behind-the-scenes work to provide the code for the likelihood\index{likelihood} function. Working at a higher level allows us to focus on the study design and assumptions, which guide us to our choice of sample distribution.

### Deviance as a measure of model fit  {#Deviance}
Likelihood\index{likelihood} calculations were introduced in Section \@ref(Likelihood). Here we show two related measures of model fit (log-likelihood and deviance\index{deviance}) using the same mark-recapture\index{mark-recapture} example. There can be computational advantages to working with the log-likelihood compared to the likelihood [@mccarthy_2007]. 

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Arbitrary 'observed' values for analysis
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- n2 * p.true # Marked fish in second sample

# Create vector of possible p values, determine likelihood for each
p.vec <- seq(from=0.02, to=1, by=0.02)
l.vec <- choose(n2,m2) * p.vec^m2 * (1-p.vec)^(n2-m2)
plot(p.vec, l.vec, ylab="Likelihood", col="red")

log_l.vec <- log(l.vec) # Log-likelihood
plot(p.vec, log_l.vec, ylab="log-Likelihood", col="red")
dev.vec <- -2 * log_l.vec # Deviance (-2 * ln_L)
plot(p.vec, dev.vec, ylab="Deviance", col="red")
```

Compared to the likelihood\index{likelihood}, the log-likelihood curve has a different shape but the same peak at p=0.2 (the most likely value given 16 successes in a trial size of 80). The deviance\index{deviance} is obtained by multiplying the log-likelihood by -2. The plotted pattern is now inverted so that now the curve has a *minimum* at p=0.2, i.e., a larger value for deviance\index{deviance} indicates a poorer fit [@mccarthy_2007]. JAGS uses the deviance\index{deviance} to obtain parameter estimates, but it is nice to know that we would obtain the same estimates from either the maximum likelihood or log-likelihood or the minimum deviance\index{deviance} [@mccarthy_2007].

### Model checking {#Model-checking}

It is easy to fit the correct model to simulated data, but that is not the case for real (field) data. Any model fit to field data should be viewed as an approximation of the underlying ecological processes. We cannot prove that the model is correct, but we can examine whether it is a useful approximation. One method for doing this is the posterior predictive check\index{posterior predictive check} [@gelman.hill_2007; @kéry_2010; @kéry.schaub_2012; @conn.etal_2018]. This method compares a measure of fit for replicated data generated using the fitted model compared to the observed data. If the measure of fit (e.g., sum of squares or chi-square) is markedly different (usually higher) for the observed data, that suggests that the observed data contain some features not captured by the fitted model.

Our first example uses simulated catch data from a Poisson distribution\index{Poisson distribution}, as might be obtained by standardized electrofishing or trawling. Keep in mind that these are our "observed" catches, to be compared with replicate data obtained by simulation. I arbitrarily set the mean for the Poisson distribution\index{Poisson distribution} at 7 and the sample size at 30. Before viewing the extended JAGS code that includes the posterior predictive check, let's examine a version that simply fits the model: 

```{r eval=FALSE}
# Fit model to catch data from Poisson distribution

rm(list=ls()) # Clear Environment

N <- 30
mu <- 7
Catch <- rpois(n=N, lambda=mu) # Drawn from Poisson distribution
Freq <- table(Catch)  # Distribution of simulated catches
barplot(Freq, main="", xlab="Catch", ylab="Frequency")

# Load necessary library packages
library(rjags)
library(R2jags)

sink("PoissonFit.txt")
cat("
model {

# Prior
 lambda.est ~ dunif(0, 100)

# Likelihood
    for(i in 1:N) {
      Catch[i] ~ dpois(lambda.est)
      } #y
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("N", "Catch")

# Initial values
jags.inits <- function(){ list(lambda.est=runif(n=1, min=0, max=100))}

model.file <- 'PoissonFit.txt'

# Parameters monitored
jags.params <- c("lambda.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 10000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

There are only a few lines of JAGS code. We use an uninformative uniform prior distribution\index{uninformative prior distribution} for the single Poisson parameter lambda.est. The catches are assumed (correctly) to be drawn from a Poisson distribution\index{Poisson distribution}, and the likelihood\index{likelihood} calculations use the Poisson distribution function <code>dpois()</code>.  The initial value is obtained from the same uniform distribution\index{uniform distribution} as the prior. JAGS returns the estimated mean, which varies around the true value due to the modest sample size. Try rerunning the code using a large sample size to see the improvement in the sample catch plot and the estimated mean.

Next we add a few lines of code to carry out the posterior predictive check\index{posterior predictive check}:

```{r eval=FALSE}
# Fit model to catch data from Poisson distribution
# Includes code for posterior predictive check

rm(list=ls()) # Clear Environment

N <- 30
mu <- 7
Catch <- rpois(n=N, lambda=mu) # Drawn from Poisson distribution
Freq <- table(Catch)  # Distribution of simulated catches
barplot(Freq, main="", xlab="Catch", ylab="Frequency")

# Load necessary library packages
library(rjags)
library(R2jags)

sink("PPC_Example.txt")
cat("
model {

# Prior
 lambda.est ~ dunif(0, 100)

# Likelihood
    for(i in 1:N) {
      Catch[i] ~ dpois(lambda.est)
      Catch.new[i] ~ dpois(lambda.est) # Replicate new data for PPC
      # Chi-square discrepancy values for observed and replicate data
      chi.obs[i] <- pow(Catch[i]-lambda.est,2)/lambda.est
      chi.new[i] <- pow(Catch.new[i]-lambda.est,2)/lambda.est
      } #y
  Total.obs <- sum(chi.obs[])
  Total.new <- sum(chi.new[])
  Bayesian.p <- step(Total.new - Total.obs)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("N", "Catch")

# Initial values
jags.inits <- function(){ list(lambda.est=runif(n=1, min=0, max=100))}

model.file <- 'PPC_Example.txt'

# Parameters monitored
jags.params <- c("lambda.est", "Bayesian.p")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 10000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

Within the likelihood\index{likelihood} looping section, we draw new replicate observations from the posterior distribution\index{posterior distribution} (Catch.new), and calculate chi-square values for observed and replicate data. The Bayesian p-value\index{Bayesian p-value} is obtained from the <code>step()</code> function, which returns a 0 if its argument (difference in total chi-square values) is negative and a 1 if 0 or greater. Thus the variable Bayesian.p has a sequence of 0 and 1 values and the mean provides the proportion of updates for which Total.new was the larger of the two values. The Bayesian p-values\index{Bayesian p-value} tend to be quite variable; ten trials produced values of 0.34, 0.42, 0.18, 0.16, 0.32, 0.30, 0.32, 0.67, 0.69, and 0.18. We would expect the total chi-square for observed and replicate data to be the same on average as the fitted model is known to be correct (i.e., our "observed" catches and the replicate data sets are drawn from Poisson distributions\index{Poisson distribution}). Keep in mind that, when analyzing a real data set, there will be a single Bayesian p-value\index{Bayesian p-value} rather than the replicated values shown above. Simulations mimicing the field study are very helpful in interpreting that single value.

Next consider a case where the "observed" data are obtained from something other than a Poisson distribution. The negative binomial distribution\index{negative binomial distribution} (Section \@ref(NegBinom)) is a good choice because it can approximate a Poisson\index{Poisson distribution} or have a more skewed distribution, depending on the value for the overdispersion parameter k. Here we use the same mean as in the Poisson example and set k=1 (variance much larger than mean). The JAGS code for model fitting remains unchanged but we replace the Poisson simulation code with the following:

```{r eval=FALSE}
# Fit model to negative binomial data
# Includes code for posterior predictive check

rm(list=ls()) # Clear Environment

N <- 30
mu <- 7
k=1
variance <- mu+(mu^2)/k
Catch <- rnbinom(n=N, mu=mu, size=k) # Catches drawn from negative binomial
Freq <- table(Catch)  # Distribution of simulated counts
barplot(Freq, main="", xlab="Count", ylab="Frequency")
```

Now Bayesian p-values\index{Bayesian p-value} are consistently extreme (0.00 for five replicate trials). The total chi-square for the replicate data sets (which *are* Poisson distributed) is always less than for the observed data (which are not). The fitted Poisson model lacks the flexibility to mimic the skew of the observed catch distribution. More intermediate Bayesian p-values\index{Bayesian p-value} are only obtained if k takes on larger values, such that the negative binomial approximates a Poisson distribution (Figure \@ref(fig:PPC)).

```{r PPC, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Bayesian p-value for fitting a Poisson model to simulated data from a negative binomial distribution with different values for overdispersion parameter k but fixed mean=7. Twenty replicate simulations were done for each value of k.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

PPC_model <- function() {
# Prior
 lambda.est ~ dunif(0, 100)

# Likelihood
    for(i in 1:N) {
      Catch[i] ~ dpois(lambda.est)
      Catch.new[i] ~ dpois(lambda.est) # Replicate new data for PPC
      # Chi-square discrepancy values for observed and replicate data
      chi.obs[i] <- pow(Catch[i]-lambda.est,2)/lambda.est
      chi.new[i] <- pow(Catch.new[i]-lambda.est,2)/lambda.est
      } #y
  Total.obs <- sum(chi.obs[])
  Total.new <- sum(chi.new[])
  Bayesian.p <- step(Total.new - Total.obs)
}

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# Negative binomial parameters
N <- 30
mu <- 7
k.values=c(1, 2.5, 5, 10, 20, 30, 40, 50, 60, 70)
k=rep(k.values, times=20)
Reps <- length(k)
Bayesian.p <- array(data=NA, dim=Reps)
for (i in 1:Reps){
Catch <- rnbinom(n=N, mu=mu, size=k[i]) # Catches drawn from negative binomial

# JAGS code
jags.data <- list("N", "Catch")

# Initial values
jags.inits <- function(){ list(lambda.est=runif(n=1, min=0, max=100))}

# Fit the model
PPC_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 2000, n.burnin = 1000,
    model.file = PPC_model,
    parameters.to.save = c("lambda.est", "Bayesian.p")
  )
Bayesian.p[i] <- PPC_jags$BUGSoutput$mean$Bayesian.p
}

plot(k,Bayesian.p, xlab="k", ylab="Bayesian p-value", pch=16)

```

@kéry.schaub_2012 list some concerns about the posterior predictive check\index{posterior predictive check}. One is that the observed data are used twice: in fitting the model and in producing the Bayesian p-value\index{Bayesian p-value}. Another is that the method is qualitative, in that there is not an objective p-value range that indicates an acceptable fit. Despite those concerns, lack of fit can be established by extreme p-values. @kéry_2010 suggested poor fit was indicated by values close to 0 or 1; @conn.etal_2018 suggested p-values less than 0.05 or greater than 0.95. These recommmendations appear to be consistent with the simulation results for k<10 in Figure \@ref(fig:PPC). To better understand this pattern in Bayesian p-values\index{Bayesian p-value}, try the following code as a simple graphical way to compare a Poisson\index{Poisson distribution} distribution with negative binomial\index{negative binomial distribution} distributions generated using a range of values for k. The <code>range()</code> function provides a common x-axis for the two plots to facilitate comparisons.

```{r eval=FALSE}
x.p <- rpois(n=1000, lambda=7)
k <- 1 # Overdispersion parameter
x.np <- rnbinom(n=1000, size=k, mu=7)
x.bound <- range(x.p, x.np)
par(mfrow=c(1,2))
hist(x.p, main="", xlim=x.bound, xlab="Poisson")
hist(x.np, main="", xlim=x.bound, xlab="Negative binomial")
```

My experience has been that posterior predictive checks\index{posterior predictive check} can be a useful way of detecting lack of fit. Simulation is helpful in gaining experience with the approach, because it is straightforward to obtain Bayesian p-values\index{Bayesian p-value} for correct and incorrect models. Also, simulation of a specific study design and model can be used to determine the range of p-values that would be considered extreme [@conn.etal_2018].

### Model selection {#Model-selection}

It is often the case that more than one model may be considered as plausible. For example, survey catches could be modeled using either a Poisson or negative binomial distribution. A model for a tag-return study could include separate parameters for tag-reporting rate\index{tag-reporting rate} from commercial and recreational sectors, or just a single parameter if the reporting rates were similar. A third example is a model for survey catch rate, which might include covariates such as location, year, depth, or water temperature. Determining which covariates affected catch rate would make it possible to produce an improved survey index that was adjusted for environmental variation. For example, were low survey catches in the current year due to anomalously low water temperatures on sampling dates or to a real decline in abundance?  The suite of candidate models should be chosen before data analysis begins [@burnham.anderson_1998] to avoid the tendency to examine the study results and to choose candidate models that match observed patterns. For example, observing low recruitment in a recent year might spur us to search for environmental covariates with a similar temporal pattern.

Choosing a preferred model among the suite of candidates (model selection\index{model selection}) is a vast and complex topic [@link.barker_2010], and there is no consensus among statisticians as to the best approach [@kéry.schaub_2012]. @kéry.schaub_2012 suggest that one practical approach is to decide on a model that is biologically plausible and stick to that. They also sometimes eliminate candidate parameters that have credible intervals that include zero, similar to a backward stepwise regression. @hooten.hobbs2015 recommend model selection\index{model selection} based on predictive ability, either in-sample (data used in fitting the model) or out-of-sample (new data). Out-of-sample validation is considered the gold standard [@hooten.hobbs2015]. The main disadvantage of that approach is the requirement for additional data not used in fitting the model. @doll.jacquemin_2019 demonstrate and provide Stan [@gelman.etal_2015] code for additional methods of model selection\index{model selection} including WAIC\index{WAIC|see {Watanabe-Akaike information criterion}}\index{Watanabe-Akaike information criterion} (Watanabe-Akaike or widely applicable information criterion: @watanabe_2010). WAIC is not calculated automatically by JAGS but, unlike DIC\index{Deviance Information Criterion (DIC)}, it can be used in hierarchical\index{hierarchical model} and mixture\index{binomial-mixture model} models [@gelman.etal_2014; @hooten.hobbs2015].

We illustrate here a simpler approach of separately fitting candidate models and comparing DIC\index{Deviance Information Criterion (DIC)} scores. Our "observed" data are simulated counts from a negative binomial distribution\index{negative binomial distribution}. Next we fit Poisson\index{Poisson distribution} and negative binomial candidate models to the same observed data and compare DIC\index{Deviance Information Criterion (DIC)} scores. The negative binomial distribution in JAGS uses the probability of success (p) and size (our overdispersion parameter k). We return the estimated mean for the negative binomial distribution, which is calculated internally using p.est and k.est.

```{r eval=FALSE}
# DIC comparison for model selection, using "observed" data
# from a negative binomial distribution

rm(list=ls()) # Clear Environment

N <- 30
mu <- 7
k=1
variance <- mu+(mu^2)/k
Count <- rnbinom(n=N, mu=mu, size=k) # Drawn from negative binomial
Freq <- table(Count)  # Distribution of simulated counts
barplot(Freq, main="", xlab="Count", ylab="Frequency")

# Load necessary library packages
library(rjags)
library(R2jags)

sink("PoissonFit.txt")
cat("
model {

# Prior
 lambda.est ~ dunif(0, 100)

# Likelihood
    for(i in 1:N) {
      Count[i] ~ dpois(lambda.est)
      } #y
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("N", "Count")

# Initial values
jags.inits <- function(){ list(lambda.est=runif(n=1, min=0, max=100))}

model.file <- 'PoissonFit.txt'

# Parameters monitored
jags.params <- c("lambda.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 10000,
                model.file)
print(jagsfit)
#plot(jagsfit)

Poisson.DIC <- jagsfit$BUGSoutput$DIC

sink("NBFit.txt")
cat("
model {

# Prior
 p.est ~ dunif(0, 1)  
   # JAGS uses p (probability of success) for negative binomial
 k.est ~ dunif(0, 1000)
 mu.est <- k.est*(1-p.est)/p.est
 
# Likelihood
    for(i in 1:N) {
      Count[i] ~ dnbinom(p.est, k.est)
      } #y
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("N", "Count")

# Initial values
jags.inits <- function(){ list(p.est=runif(n=1, min=1E-6, max=1),
                               k.est=runif(n=1, min=0, max=1000))}

model.file <- 'NBFit.txt'

# Parameters monitored
jags.params <- c("mu.est", "k.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 10000,
                model.file)
print(jagsfit)
#plot(jagsfit)
NB.DIC <- jagsfit$BUGSoutput$DIC
Poisson.DIC - NB.DIC
```

The difference in DIC\index{Deviance Information Criterion (DIC)} scores (printed to the Console for convenience) ranged from 50.4 to 90.4 in five trials using the default simulation settings. The substantially higher DIC score for the (incorrect) Poisson model is clear evidence of a poorer fit. @burnham.anderson_1998 suggested that an alternative model with an AIC score within 1-2 of the best model merits consideration whereas a difference of 3-7 suggests much less support. @spiegelhalter.etal_2002 noted that the @burnham.anderson_1998 criteria appear to work well for DIC\index{Deviance Information Criterion (DIC)} scores.

Comparing DIC\index{Deviance Information Criterion (DIC)} scores worked well for this simple (non-hierarchical, non-mixture) case. Also, using simulated counts from a negative binomial distribution\index{negative binomial distribution} is convenient because we can adjust the overdispersion\index{overdispersion} parameter (k) to obtain a more "Poisson-like" distribution. At what value for k do you obtain similar DIC scores for the two models? Observe the change in shape of the count frequency distribution as you vary k.

## Exercises

1. Estimate uncertainty around a two-sample mark-recapture estimate N.hat using the simulation and Bayesian approaches for a capture probability\index{capture probability} of 0.4. How similar are the results and how do they compare to results using the original capture probability\index{capture probability} of 0.2?

2. @robson.regier_1964 provided practical advice on acceptable levels of accuracy and precision. They suggested that estimates within 10% of the true value be acceptable for research studies, versus 25% for management and 50% for preliminary studies. Bayesian credible intervals could be used as a proxy for their accuracy and precision targets. What capture probabilities (approximate) would correspond to those targets for the mark-recapture example?

3. How many errors can you locate before running the following code? Fix all errors that you spotted then run the code to fix any remaining errors.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# Arbitrary 'observed' values for analysis
Reps <- 20
AveC <- 3
Count <- rpois(n=Reps lambda=AveC)

# JAGS code
sink("PoissonSim.txt")
cat("
    model {

    # Priors
    AveC.est ~ dunif(0, MaxCount)
   
    # Likelihood
    for (i in 1:Rep){
    Count[i] ~ dpois(AveCest)
    } #i
}
    ",fill = TRUE)
sink()

# Bundle data
jags.data <- list("Count")

# Initial values.
jags.inits <- function(){ list(AveC.est=runif(n=1, min=0, max=100))}

model.file <- 'PoisonSim.txt'

# Parameters monitored
jags.params <- c("AveC.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, inits=jags.inits, jags.params,
                n.chains = 3, n.thin = 1, n.iter = 2000,
                n.burnin = 1000, model.file)
print(jagsfit, digits=3)
plot(jagsfit)
```

4. Consider a mark-recapture study with n~1~ = 60 and two independent samples for recaptures (n~2~=90, m~2~=40; n~3~=55, m~3~=30). Use a vector for p at 0.01 intervals and produce a vector and plot of joint likelihoods (vector given the name "both"). Assume that only the initial sample (n~1~) is marked; samples n~2~ and n~3~ provide two independent snapshots of the marked fraction. Explain in detail how p[which.max(both)] works and what it produces.

5. Use the approach from Section \@ref(Model-selection) to compare model fits for a more "Poisson-like" negative binomial distribution (e.g., k=10).
