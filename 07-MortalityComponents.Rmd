# Mortality Components {#Mortality}

Reliable estimates of the survival rate (Chapter \@ref(Survival)) are essential in understanding fish population dynamics, but for exploited species, there is generally interest in partitioning mortality (1-survival) into its component rates. Knowing the relative importance of fishing\index{fishing mortality} versus natural mortality\index{natural mortality} clarifies the extent to which fishery managers can use regulations to affect population change. The sections that follow illustrate different approaches for examining the impact of fishing and for estimating fishing\index{fishing mortality} and natural mortality\index{natural mortality} rates.

## Exploitation rate {#ExpRate}

A tagging study can provide a real-time estimate of the exploitation rate\index{exploitation rate}, or the fraction of the population being harvested. Similar to the tag-return\index{tag-return} method of estimating survival (Section \@ref(BrownieSurvival)), the only field work required is the initial tagging effort. Here there is a single release of fish with external tags at the start of the study, and the number of returned tags is recorded over a specified interval. The study duration could be an entire year, or a fishing season for a population managed by seasonal harvest. A high exploitation rate might indicate that harvest regulations such as a size limit or creel limit could be worthwhile.  Alternatively a low estimate might indicate that other factors such as natural mortality\index{natural mortality} or recruitment\index{recruitment} may be more important in regulating population size. With only a single release, we cannot estimate survival\index{survival rate} (or its complement, mortality) but it is nevertheless an important first step in judging the relative impact of fishing.

There are several practical issues to consider in planning an exploitation rate study. We want to ensure that essentially all tags of harvested fish are reported, so high-reward tags\index{high-reward tags} (e.g., $100) could be used [@pollock.etal_2001]. Tag loss\index{tag loss} is another potential source of bias. It can be estimated by double-tagging (giving some or all fish two tags, and recording returns of two versus only one tag), but it greatly complicates the analysis. For this example, we will keep things simple and assume that tag loss\index{tag loss} is negligible. We also assume that there is no immediate tagging mortality\index{tagging mortality} (i.e., due to capture, handling and tagging). This can be challenging to investigate. It is sometimes estimated by holding a sample of fish in a cage after tagging but this can itself be a source of mortality [@pollock.pine2007].  For now, we assume that there is no short- or long-term mortality associated with tagging. We assume that all caught fish are harvested. If catch-and-release was occurring, our model could be extended to estimate rates of harvest and catch-and-release. The decision of how many fish to tag will depend on the required precision of the study, and as usual can be examined through simulation. Finally, it is important that tagged fish be representative of the entire population. This refers not only to size or age, but also the spatial pattern. Tagging could be done at randomly selected locations, or when the entire study population is concentrated within a single area (e.g., winter holding area or spawning location). The key is for all fish, including the tagged subset, to have the same probability of being harvested.

Our simulation code uses an assumed exploitation rate\index{exploitation rate} (u) of 0.4. The value could be based on a literature review or prior work from the study area or similar areas regionally. The number of returned tags has a binomial distribution\index{binomial distribution} (Section \@ref(BinomialDist)):

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)
```

We need only a single observation for the returned number of tags. What lines of code could you add to look at the distribution of returned tags (using a new variable for the vector of tag returns)?

The JAGS code is also extremely simple. We have one line of code for the uninformative prior distribution\index{uninformative prior distribution} for the exploitation rate\index{exploitation rate}, and one for the likelihood\index{likelihood}.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dunif(0,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values
jags.inits <- function(){ list(u.est=runif(n=1, min=0, max=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

A single run with a release of 100 tagged fish provided a credible interval of 0.27-0.46 (from a random return of 36 tags). Experiment with different sample sizes and consider what level of precision would be adequate for management purposes. As always, this is an optimistic scenario, with the number of returned tags perfectly conforming to a binomial distribution\index{binomial distribution}.

### Informative prior distribution {#InformativePriors}
@banner.etal_2020 discourage the (at least uncritical) use of "default" prior distributions\index{prior distribution} (which are used throughout this book). It is often true that previous studies contain relevant information, and Bayesian methods provide an objective framework for building on that previous information [@mccarthy_2007]. The simple example of estimating the exploitation rate\index{exploitation rate} is a perfect opportunity to look at how an informative prior\index{informative prior distribution} would be implemented and to explore its effect on the posterior distribution\index{posterior distribution}.

To compare posterior distributions obtained using uninformative and informative priors\index{informative prior distribution}, we use two copies of the above code. In the uninformative prior\index{uninformative prior distribution} version, we simply replace <code>dunif(0,1)</code> with <code>dbeta(1,1)</code> in the JAGS code, and replace the uniform initial value with the corresponding beta\index{beta distribution} (<code>rbeta(n=1, shape1=1, shape2=1)</code>). Neither change affects the calculations because the two distributions are equivalent, but it sets up the model for the informative prior\index{informative prior distribution} version.

For the informative\index{informative prior distribution} case, the exploitation rate\index{exploitation rate} simulation is unchanged. We simply add two lines of code for the pilot study that will be used for the prior\index{prior distribution}. The informative beta distribution\index{beta distribution} in the JAGS code has two parameters: successes+1 and failures+1 [@bolker2008]. We augment jags.data with the pilot study data, and use the same initial values as in the uninformative prior\index{uninformative prior distribution} version. The JAGS code is followed by R code for producing two plots, showing the prior\index{prior distribution} and posterior distributions\index{posterior distribution} for the uninformative (upper plot) and informative (lower) cases. We specify margins for each plot to reduce blank space. The <code>density()</code> function provides a smoothed probability density for the exploitation rate estimates. The <code>lines()</code> function adds the prior distribution\index{prior distribution} to each plot, generated using the <code>dbeta()</code> function.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dbeta(1,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values
jags.inits <- function(){ list(u.est=rbeta(n=1, shape1=1, shape2=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)

# Informative prior
n.pilot <- 20
n.returned.pilot <- rbinom(n=1, prob=u.true, size=n.pilot)

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dbeta((n.returned.pilot+1), 
         (n.pilot-n.returned.pilot+1)) # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged", "n.pilot", 
                  "n.returned.pilot")

# Initial values
jags.inits <- function(){ list(u.est=rbeta(n=1, shape1=1, shape2=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
informative <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(informative)
plot(informative)

# Vague prior and posterior
par(mfrow=c(2,1))
par(mar=c(2,4,2,1)) # bottom, left, right, top margins
dens1 <- density(jagsfit$BUGSoutput$sims.list$u.est)
dens2 <- density(informative$BUGSoutput$sims.list$u.est)
y.bounds <- range(dens1$y, dens2$y)
x.bounds <- range(jagsfit$BUGSoutput$sims.list$u.est, 
              informative$BUGSoutput$sims.list$u.est)
plot(density(jagsfit$BUGSoutput$sims.list$u.est), xlab="", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true, col="red")
x.vec <- seq(x.bounds[1], x.bounds[2], length.out=101)
lines(x.vec,
      dbeta(seq(x.bounds[1], x.bounds[2], length.out=101), shape1=1, 
            shape2=1), lty=2)
legend(x = "topleft", lty = c(2,1), text.font = 1,  
       legend=c("Prior", "Posterior")) 

# Informative prior and posterior
par(mar=c(4,4,1,1))
plot(dens2,xlab="Estimated exploitation rate", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true, col="red")
lines(x.vec, dbeta(seq(x.bounds[1], x.bounds[2], 
                    length.out=101), shape1=(n.returned.pilot+1), 
                    shape2=(n.pilot-n.returned.pilot+1)), lty=2)

```

A pilot study provides a valuable opportunity to work out field logistics, as well as allowing for construction of an informative prior\index{informative prior distribution}. Our simulation suggests a modest gain in precision from a pilot release of 20 tagged fish, given a full study release of 100 (Figure \@ref(fig:InfPriorPlot)). The benefit of an informative prior\index{informative prior distribution} depends on the information content of the prior\index{prior distribution} versus the new data [@mccarthy.masters_2005]. Also, our simulated pilot study has a random aspect (number returned), so a small pilot release will occasionally produce a prior\index{prior distribution} that is unhelpful for estimating the true exploitation rate\index{exploitation rate}. Planning this type of study should be done using multiple simulation runs with varying sample sizes for the pilot and full studies. For example, try starting with a pilot study sample size of 1, which approximates the uninformative prior\index{uninformative prior distribution} case. As you increase the sample size for the pilot study, when is there a noticeable change in the posterior distribution\index{posterior distribution}, compared to the uninformative prior\index{uninformative prior distribution} case? This can also be evaluated by comparing the width of credible intervals\index{credible interval} [@mccarthy.masters_2005]. How does an increase in the sample size for the full study change the effect of an informative prior\index{informative prior distribution}?

The beta distribution\index{beta distribution} works well for characterizing a pilot tagging study, because the two parameters are based on the number of successes (returned tag) and failures (not returned). In general, informative priors\index{informative prior distribution} can be based on the mean and variance of previous estimates, or can include the additional variation within individual studies [@mccarthy.masters_2005; @regehr.etal_2018]. Another approach is to base the prior on ecological theory; for example, a prior for survival rate\index{survival rate} based on its relationship to body mass [[@mccarthy.masters_2005]. @lemoine_2019 recommends carrying out analyses of real data using multiple prior distributions\index{prior distribution} of varying strength, to better understand the information content of the new data and the effect of priors\index{prior distribution}. 

```{r InfPriorPlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Prior and posterior distributions for estimated exploitation rate, using uninformative (upper) and informative (lower) priors. The full study sample size is 100 tagged fish; the pilot study (lower panel) has a sample size of 20. The true exploitation rate is shown as a vertical red line.', out.width="90%"}

rm(list=ls()) # Clear Environment 
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

UninfPrior <- function() {
    # Priors
    u.est ~ dbeta(1,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}

InfPrior <- function() {
    # Priors
    u.est ~ dbeta((n.returned.pilot+1), 
         (n.pilot-n.returned.pilot+1)) # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values.
jags.inits <- function(){list(u.est=rbeta(n=1, shape1=1, shape2=1))}

# Fit the model
UninfPrior_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = UninfPrior,
    parameters.to.save = c("u.est")
  )

# Informative prior - add pilot study
n.pilot <- 20
n.returned.pilot <- rbinom(n=1, prob=u.true, size=n.pilot)

# JAGS code

# Bundle data
jags.data <- list("n.returned", "n.tagged", "n.pilot", "n.returned.pilot")

# Initial values.
jags.inits <- function(){list(u.est=rbeta(n=1, shape1=1, shape2=1))}

# Fit the model
InfPrior_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = InfPrior,
    parameters.to.save = c("u.est")
  )

# Vague prior and posterior
par(mfrow=c(2,1))
par(mar=c(2,4,2,1)) # bottom, left, right, top margins
dens1 <- density(UninfPrior_jags$BUGSoutput$sims.list$u.est)
dens2 <- density(InfPrior_jags$BUGSoutput$sims.list$u.est)
y.bounds <- range(dens1$y, dens2$y)
x.bounds <- range(UninfPrior_jags$BUGSoutput$sims.list$u.est, 
              InfPrior_jags$BUGSoutput$sims.list$u.est)
plot(density(UninfPrior_jags$BUGSoutput$sims.list$u.est), xlab="", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true, col="red")
x.vec <- seq(x.bounds[1], x.bounds[2], length.out=101)
lines(x.vec,
      dbeta(seq(x.bounds[1], x.bounds[2], length.out=101), shape1=1, shape2=1),
      lty=2)
legend(x = "topleft", lty = c(2,1), text.font = 1,  
       legend=c("Prior", "Posterior")) 

# Informative prior and posterior
par(mar=c(4,4,1,1))
plot(dens2,xlab="Estimated exploitation rate", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true, col="red")
lines(x.vec, dbeta(seq(x.bounds[1], x.bounds[2], 
                    length.out=101), shape1=(n.returned.pilot+1), 
                    shape2=(n.pilot-n.returned.pilot+1)), lty=2)

```


## Completed tagging study {#Hearn}

A single release of tags (Section \@ref(ExpRate)) can also be used to estimate the rate of natural mortality\index{natural mortality}. @hearn.etal1987 provide a method that can be used in completed tagging studies\index{completed tagging study}; that is, a study of sufficient duration that tag returns have ceased (assumed to indicate that no live tagged fish remain). Their method requires only that fishing mortality\index{fishing mortality} in each period be greater than zero, in order to establish the study's endpoint. It is assumed that tags of harvested fish are always reported\index{tag-reporting rate}, there is no mortality associated with tagging\index{tagging mortality}, and there is no tag loss\index{tag loss}. The model assumes a constant rate of natural mortality\index{natural mortality}, so the estimated rate would in practice represent the average over the study period.

The following Bayesian version is a modification of the @hearn.etal1987 method. We allow for zero harvest but assume that tag returns are monitored for a sufficiently long time to confirm the true study length (i.e., to allow for the possibility of an interspersed 0). The @hearn.etal1987 model uses instantaneous rates, which we have to this point avoided. The relationship between probabilities (survival, death, exploitation, natural death) and instantaneous rates is as follows. The instantaneous rate of population decline is $dN/dt = -Zt$, where N is population size and Z is the instantaneous total mortality rate\index{instantaneous total mortality rate} (rate of decline due to fishing and natural sources). An advantage of instantaneous rates is that they are additive. We can partition Z into whatever categories are useful and estimable; for example, fishing\index{fishing mortality} versus natural mortality\index{natural mortality}, commmercial versus recreational fishing, longline versus trawl, etc.  The most common partitioning is for fishing (F) versus natural (M) sources: $dN/dt = -(M+F)t$. Integrating this equation provides us with an expression for population size at any time t:

$N_t = N_0 * exp(-(M+F)*t)$

where N~0~ is initial abundance and $S=exp(-(M+F)*t)$ is the survival rate\index{survival rate}. The probability of dying from all causes is A=1-S. The exploitation rate\index{exploitation rate} is obtained as u=FA/Z, which makes sense as the fraction of total deaths due to fishing (F/Z). Similarly the probability of natural death is v=MA/Z. These expressions make it possible to go back and forth between probabilities and instantaneous rates, depending on which is more useful or traditional for a particular model. These expressions apply for the most common situation where fishing\index{fishing mortality} and natural mortality\index{natural mortality} occur simultaneously (referred to by @ricker1975 as a Type 2 fishery). @ricker1975 provides alternate expressions for the less common case (a Type 1 fishery) where natural mortality\index{natural mortality} occurs after a short fishing season.

Unlike the probabilities for survival, harvest, etc., instantaneous rates are unbounded. In a Bayesian context, this makes them more challenging to estimate compared to probabilities, which are conveniently bounded by 0 and 1. Instantaneous rates are also less intuitive in terms of their magnitude compared to probabilities. There is typically more uncertainty about M than F because natural deaths are almost never observed [@quinn.deriso_1999]. The joke about M is that it is often assumed to be 0.2, because .2 looks like a distorted version of a question mark. In a review of published natural mortality estimates, @vetter_1988 found that a majority were less than 0.5 but values exceeding 2.0 were occasionally reported.

We begin with simulation code:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

n.tagged <- 100
Periods <- 20 
  # Set to high value to ensure that end of study is observed
F.true <- 0.4
M.true <- 0.2
S.true <- exp(-F.true - M.true)
u.true <- F.true * (1-S.true) / (F.true + M.true)

TagsAtRisk <- array(data=NA, dim=Periods)
TagsAtRisk[1] <- n.tagged
TagFates <- array(data=NA, dim=(Periods-1)) # Returns by period

for (j in 2:Periods){
  TagsAtRisk[j] <- rbinom(n=1, size=TagsAtRisk[j-1], prob=S.true)
  TagFates[j-1] <- rbinom(n=1, size=(TagsAtRisk[j-1]
                        -TagsAtRisk[j]),(F.true/(F.true+M.true))) 
    # Random realization: fishing deaths
} #j

t.Completed <- max(which(TagFates[]>0))
# Locate period when study completed (last return)
NatDeaths <- n.tagged - sum(TagFates)
TagFates <- c(TagFates[1:t.Completed],NatDeaths) 
# Returns + all natural deaths
```

The vectors for tags-at-risk and fates are binomially-distributed\index{binomial distribution} random realizations. Tags at risk are obtained using the number at risk in the previous period and the survival rate\index{survival rate}. Tag returns (stored in tag fates vector) are obtained from the number of deaths between periods and the fraction of deaths due to fishing (F/Z). The <code>which()</code> function provides a vector of periods when tag returns are greater than 0, and the <code>max()</code> function chooses the final period with at least one tag return. Try <code>which(TagFates[]>0)</code> in the Console to make clear how this expression works. The final version of the tag fates vector contains tag returns for the completed study length plus a final value for natural deaths (any tagged fish not seen in the completed tagging study).

The JAGS code for fitting the model is similar to that for the tag-based method of estimating survival (Section \@ref(BrownieSurvival)), except that our parameters now are instantaneous rates. We use an arbitrary uniform prior distribution\index{prior distribution}, with an upper bound (2) set high enough to ensure that the interval contains the true value. The likelihood\index{likelihood} uses a multinomial distribution\index{multinomial distribution} (Section \@ref(Multinomial)), where cell probabilities represent the fraction returned in each period plus a final value for the fraction not seen again (natural deaths in our completed tagging study\index{completed tagging study}). The last three lines of code show the estimated posterior distribution\index{posterior distribution} compared to the true value. 

```{r eval=FALSE}

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("Hearn.txt")
cat("
  model {
  # Priors
  M.est ~ dunif(0, 2)  # Instantaneous natural mortality rate
  for (i in 1:t.Completed) {
     F.est[i] ~ dunif(0,2) # Instantaneous fishing mortality rate
     S.est[i] <- exp(-M.est - F.est[i])
     u.est[i] <- F.est[i] * (1-S.est[i])/(F.est[i]+M.est) # FA/Z
  } #i

# Cell probabilities
  p.est[1] <- u.est[1]
  for (i in 2:t.Completed) {
    p.est[i] <- prod(S.est[1:(i-1)])*u.est[i]
      } #i
    p.est[t.Completed+1] <- 1 - sum(p.est[1:t.Completed])
      # Prob of not being seen again
  TagFates[1:(t.Completed+1)] ~ dmulti(p.est[1:(t.Completed+1)],
                                n.tagged)
 }
  ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("TagFates", "t.Completed", "n.tagged")

# Initial values
jags.inits <- function(){list(M.est=runif(1, 0, 2), 
                              F.est=runif(t.Completed, 0, 2))}

model.file <- 'Hearn.txt'

# Parameters monitored
jags.params <- c("M.est", "F.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)

par(mfrow=c(1,1)) # Default plot settings
mar = c(5.1, 4.1, 4.1, 2.1)
# Look at posterior distribution versus true value
hist(jagsfit$BUGSoutput$sims.list$M.est, 
     xlab="Estimated natural mortality rate", main="")
abline(v=M.true, col="red")

```

This approach works because there are only two fates for tagged fish: caught and reported or natural death. Using a completed tagging study\index{completed tagging study} means that (when all assumptions hold) the number of natural deaths is known exactly (number of tag releases - total returns). The model adjusts the estimate of M in order to account for all the natural deaths by the end of the completed study. The length of the completed tagging study\index{completed tagging study} depends on the assumed values for F and M. The estimated natural mortality rate\index{instantaneous natural mortality rate} will be higher when the study length is short and when many tagged fish are not seen again.

For the default settings, estimates of M tend to be reliable (Figure \@ref(fig:HearnPlot)), which is unsurprising given that it is modeled as a constant rate and estimated from multiple years of data. Run the full code (simulation and analysis) multiple times, using different assumed values for the natural mortality rate\index{instantaneous natural mortality rate}, to build intuition about the reliability of this method. (Make sure that the number of periods is sufficiently large to contain the end of the completed study if a low natural mortality rate is used.) How would uncertainty change with a different number of tags released?

```{r HearnPlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Example of estimated posterior distribution and true natural mortality rate (vertical red line) using the completed tagging study (Hearn et al. 1987) approach.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

Hearn <- function() {
  # Priors
  M.est ~ dunif(0, 2)  # Instantaneous natural mortality rate
  for (i in 1:t.Completed) {
     F.est[i] ~ dunif(0,2) # Instantaneous fishing mortality rate
     S.est[i] <- exp(-M.est - F.est[i])
     u.est[i] <- F.est[i] * (1-S.est[i])/(F.est[i]+M.est) # FA/Z
  } #i

# Cell probabilities
  p.est[1] <- u.est[1]
  for (i in 2:t.Completed) {
    p.est[i] <- prod(S.est[1:(i-1)])*u.est[i]
      } #i
    p.est[t.Completed+1] <- 1 - sum(p.est[1:t.Completed])   # Prob of not being seen again
  TagFates[1:(t.Completed+1)] ~ dmulti(p.est[1:(t.Completed+1)], n.tagged)
}

# Simulation code for Hearn completed tagging study
n.tagged <- 100
Periods <- 20 # Set to high value to ensure that end of study is observed
F.true <- 0.4
M.true <- 0.2
S.true <- exp(-F.true - M.true)
u.true <- F.true * (1-S.true) / (F.true + M.true)

TagsAtRisk <- array(data=NA, dim=Periods)
TagsAtRisk[1] <- n.tagged
TagFates <- array(data=NA, dim=Periods) # Returns by period

for (j in 2:Periods){
  TagsAtRisk[j] <- rbinom(n=1, size=TagsAtRisk[j-1], prob=S.true)
  TagFates[j-1] <- rbinom(n=1, size=(TagsAtRisk[j-1]
                                     -TagsAtRisk[j]),(F.true/(F.true+M.true))) 
  # Random realization: fishing deaths
} #j
TagFates[Periods] <- rbinom(n=1, size=(TagsAtRisk[Periods-1]
                                       -TagsAtRisk[Periods]),(F.true/(F.true+M.true)))
t.Completed <- max(which(TagFates[]>0))
# Locate period when study completed (last return)
NatDeaths <- n.tagged - sum(TagFates)
TagFates <- c(TagFates[1:t.Completed],NatDeaths) 
# Returns + all natural deaths

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("TagFates", "t.Completed", "n.tagged")

# Initial values.
jags.inits <- function(){list(M.est=runif(1, 0, 2), F.est=runif(t.Completed, 0, 2))}

# Fit the model
Hearn_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = Hearn,
    parameters.to.save = c("M.est", "F.est")
  )

# Look at posterior distribution versus true value
hist(Hearn_jags$BUGSoutput$sims.list$M.est, 
     xlab="Estimated natural mortality rate", main="")
abline(v=M.true, col="red")
```

Estimates of F are reliable in the first few years but uncertainty increases dramatically towards the end of the completed study. This makes sense because the estimates of tags at risk are further and further from the known initial tagged population. There would likely be some improvement from simplifying the model to estimate only an average F rather than period-specific estimates, given that the true simulated F values vary stochastically around a fixed value.

## Multi-year tagging study {#BrownieFandM}

We return to the multi-year tagging study (Section \@ref(BrownieSurvival)) but now assume that we have information about the tag-reporting rate\index{tag-reporting rate}. This information allows us to partition mortality into fishing\index{fishing mortality} and natural\index{natural mortality} sources. Tag-reporting rate\index{tag-reporting rate} is fixed at 1.0 in the following code, as might be appropriate when using high-reward tags\index{high-reward tags}. In a later section of code, we estimate it internally, using planted tags [@hearn.etal_2003] and an additional likelihood\index{likelihood} component. Information about the tag-reporting rate\index{tag-reporting rate} is very powerful. For example, a 100% tag-reporting rate means that (assuming no assumptions are violated) you ultimately know the number of natural deaths because all other tags will be reported (as in Section \@ref(Hearn)). There is uncertainty in the short term because tags not returned could belong to fish still alive and at risk, so information about natural deaths is clear only after sufficient time has passed.

Our simulation code is similar to that of Section \@ref(BrownieSurvival) except that we now specify the probabilities of harvest (u.true) and natural death (v.true). We use vectors so that the rates can vary by period. We have arbitrarily set the probabilities for harvest higher than for natural death, to investigate whether we can reliably detect the predominant threat in our simulated study design.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Parameter values for three-yr experiment
Periods <- 3  # Equal number of release and return periods assumed
NTags <- 1000 # Number released each period
NumTagged <- rep(NTags, Periods) # Release NTags fish each period

u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
lam.true <- 1  # Reporting rate

# Calculated value(s), array creation
S.true <- 1 - u.true - v.true
TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again

for (i in 1:Periods){ 
# Expected values for tags-at-risk, returned/not-seen-again
  TagsAtRisk[i,i] <- NumTagged[i]
  TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
   # Returns in release period
  j <- i+1
  while(j <= Periods) {
    TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
    TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
    j <- j+1
   } #while
  TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) 
    # Tags not seen again
  } #i

RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
#Random trial
  for (i in 1:Periods) # Upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                        TagFate[i,i:(Periods+1)]))
  } #i

```

The analysis is again similar to Section \@ref(BrownieSurvival) except that now we now have a multinomial distribution\index{multinomial distribution} (Section \@ref(Multinomial)) with three possible fates (survival, harvest, natural death). Following Section 9.6 of @kéry.schaub_2011, we use three gamma-distributed\index{gamma distribution} (Section \@ref(GammaDist)) parameters that are scaled by their sum, thus ensuring that they sum to 1. The parameters of interest (u, v, S) are calculated functions of the "a" parameters, which are used internally but not themselves of interest. 

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

  # JAGS code
  sink("TagReturn.txt")
  cat("
  model {
  # Priors
  lam.est <- 1 #  100% reporting of tags
  for (i in 1:Periods) {
     for (j in 1:3) # Rows are return periods, columns are fates
       {
         a[i,j] ~ dgamma(1,1)
       } #j
     S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
     u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest
     v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
     # v.est[3] could also be obtained as 1-S.est[3]-u.est[3]
  } #i

# Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
      } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])
    # p.est[i,Periods+1] = Prob of not being seen again
    } #i
  for (i in 1:Periods) {
  RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], 
                                    NumTagged[i])
  }#i
 }
  ",fill=TRUE)
  sink()

# Bundle data
  jags.data <- list("RandRet", "Periods", "NumTagged")

# Initial values
  a.init <- array(data=NA, dim=c(Periods, 3))
  for (i in 1:3){
    a.init[i,] <- runif(n=3, min=0, max=1)
    a.init[i,] <- a.init[i,]/sum(a.init[i,]) # Rescale to sum to 1
  }
  jags.inits <- function(){ list(a=a.init)}

  model.file <- 'TagReturn.txt'

  # Parameters monitored
  jags.params <- c(#"lam.est",
                   "u.est", "v.est")
  #, "S.est")

   # Call JAGS from R
  jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                  n.chains = 3, n.thin = 1, n.iter = 4000, 
                  n.burnin=2000, model.file)
  print(jagsfit)
  plot(jagsfit)

```

We provide arbitrary starting values for the "a" parameters by rescaling uniform (0,1) random variates. Using a large release on each occasion (1000), our estimates are generally close to the true values except that v.est[3] is imprecise and typically overestimated. It is the most difficult time-dependent parameter to estimate. We get direct information (from returned tags) about the final exploitation rate\index{exploitation rate} but there is much uncertainty about whether fish not seen were still at risk or were natural deaths. This is especially true for the final release because most fish are still at large (examine the TagFate and RandRet matrices). Note that this differs from a completed tagging study (Section \@ref(Hearn)), where the study continues until no live fish remain.

The analysis can be extended to include estimation of the tag-reporting rate. We use the simplest approach of a binomial\index{binomial distribution} experiment using planted tags\index{planted tags} [@hearn.etal_2003]. The fraction of planted tags\index{planted tags} that is returned is a direct estimate of the tag-reporting rate\index{tag-reporting rate}. The only change required in the simulation code is to replace the assignment lam.true <- 1.0 with the following three lines:

```{r eval=FALSE}
lam.true <- 0.9  # Reporting rate
PlantedTags <- 30
PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
```

The assumed tag-reporting rate\index{tag-reporting rate} and number of planted tags\index{planted tags} are arbitrary and can be varied to see the effect on parameter uncertainty.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
  sink("TagReturn.txt")
  cat("
  model {
  # Priors
  lam.est ~ dunif(0,1) # Reporting rate
  for (i in 1:Periods) {
     for (j in 1:3) # Rows are return periods, columns are fates
       {
         a[i,j] ~ dgamma(1,1)
       } #j
     S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
     u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest
     v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
     # v.est[3] could also be obtained as 1-S.est[i]-u.est[i]
  } #i

# Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
      } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])   
      # Prob of not being seen again
    } #i
  for (i in 1:Periods) {
  RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], 
                                    NumTagged[i])
  }#i
  PlantReturns ~ dbin(lam.est, PlantedTags) 
    # Additional likelihood component
 }
  ",fill=TRUE)
  sink()

# Bundle data
  jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags",
                    "PlantReturns")

# Initial values
  a.init <- array(data=NA, dim=c(Periods, 3))
  for (i in 1:3){
    a.init[i,] <- runif(n=3, min=0, max=1)
    a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
  }
  jags.inits <- function(){ list(a=a.init)}

  model.file <- 'TagReturn.txt'

  # Parameters monitored
  jags.params <- c("lam.est", "u.est", "v.est")
  #, "S.est")

   # Call JAGS from R
  jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                  n.chains = 3, n.thin = 1, n.iter = 10000, 
                  n.burnin=5000, model.file)
  print(jagsfit)
  plot(jagsfit)
```

The auxiliary planted tag experiment requires only a few changes in the JAGS code. We provide an uninformative prior distribution\index{uninformative prior distribution} for the now estimated parameter lam.est, and pass in the number of planted tags and returns. We let JAGS generate an initial value for lam.est. The additional likelihood\index{likelihood} component describes the binomial\index{binomial distribution} experiment. Convergence\index{convergence} may be slower for this version of the code, so the number of MCMC\index{Markov Chain Monte Carlo (MCMC)} iterations was increased to 10,000.

```{r BrowniePlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Estimated posterior distributions for exploitation rate in period 2 at tag reporting rates of 0.9 (left) and 0.5 (right), using a multi-year tagging study with planted tags. The true exploitation rate for period 2 is shown as a vertical red line.', fig.show="hold", out.width="50%"}

rm(list=ls()) # Clear Environment
par(mar = c(4, 4, 1, .1))
set.seed(12345) # Ensure that book will have a reasonable result

BrownieFit <- function() {
  # Priors
  lam.est ~ dunif(0,1) # Reporting rate
  for (i in 1:Periods) {
    for (j in 1:3) # Rows are return periods, columns are fates
    {
      a[i,j] ~ dgamma(1,1)
    } #j
    S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
    u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest (exploitation rate)
    v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
    # v.est[3] could also be obtained as 1-S.est[i]-u.est[i]
  } #i
  
  # Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
    } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])   # Prob of not being seen again
  } #i
  for (i in 1:Periods) {
    RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], NumTagged[i])
  }#i
  PlantReturns ~ dbin(lam.est, PlantedTags) # Additional likelihood component
}

# lambda=0.9 -------------------------------
  # Parameter values for three-yr experiment
  Periods <- 3  # Equal number of release and return periods assumed
  NTags <- 1000 # Number released each period
  NumTagged <- rep(NTags, Periods) # Release NTags fish each period
  
  u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
  v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
  lam.true <- 0.9  # Reporting rate
  PlantedTags <- 30
  PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
  
  # Calculated value(s), array creation
  S.true <- 1 - u.true - v.true
  TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
  TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again
  
  for (i in 1:Periods){ 
    # Expected values for tags-at-risk, returned/not-seen-again
    TagsAtRisk[i,i] <- NumTagged[i]
    TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
    # Returns in release period
    j <- i+1
    while(j <= Periods) {
      TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
      TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
      j <- j+1
    } #while
    TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) # Tags not seen again
  } #i
  
  RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
  #Random trial
  for (i in 1:Periods) # Create upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                            TagFate[i,i:(Periods+1)]))
  } #i
# Simulation code-------------------------

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags", "PlantReturns")

# Initial values
a.init <- array(data=NA, dim=c(Periods, 3))
for (i in 1:3){
  a.init[i,] <- runif(n=3, min=0, max=1)
  a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
}
jags.inits <- function(){ list(a=a.init)}

# Fit the model
Brownie_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = BrownieFit,
    parameters.to.save = c("lam.est", "u.est", "v.est")
  )

# lambda=0.5 -------------------------------
  # Parameter values for three-yr experiment
  Periods <- 3  # Equal number of release and return periods assumed
  NTags <- 1000 # Number released each period
  NumTagged <- rep(NTags, Periods) # Release NTags fish each period
  
  u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
  v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
  lam.true <- 0.5  # Reporting rate
  PlantedTags <- 30
  PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
  
  # Calculated value(s), array creation
  S.true <- 1 - u.true - v.true
  TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
  TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again
  
  for (i in 1:Periods){ 
    # Expected values for tags-at-risk, returned/not-seen-again
    TagsAtRisk[i,i] <- NumTagged[i]
    TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
    # Returns in release period
    j <- i+1
    while(j <= Periods) {
      TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
      TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
      j <- j+1
    } #while
    TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) # Tags not seen again
  } #i
  
  RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
  #Random trial
  for (i in 1:Periods) # Create upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                            TagFate[i,i:(Periods+1)]))
  } #i
# Simulation code-------------------------

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags", "PlantReturns")

# Initial values
a.init <- array(data=NA, dim=c(Periods, 3))
for (i in 1:3){
  a.init[i,] <- runif(n=3, min=0, max=1)
  a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
}
jags.inits <- function(){ list(a=a.init)}

# Fit the model
Brownie2_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = BrownieFit,
    parameters.to.save = c("lam.est", "u.est", "v.est")
  )

# Posterior distribution, 0.9 reporting rate
x.bounds <- range(Brownie_jags$BUGSoutput$sims.list$u.est[,2],
                  Brownie2_jags$BUGSoutput$sims.list$u.est[,2])
hist(Brownie_jags$BUGSoutput$sims.list$u.est[,2], xlim=x.bounds,
     xlab="Estimated exploitation rate, period 2", main="")
abline(v=u.true[2], col="red")

# Posterior distribution, 0.5 reporting rate
hist(Brownie2_jags$BUGSoutput$sims.list$u.est[,2], xlim=x.bounds,
     xlab="Estimated exploitation rate, period 2", main="")
abline(v=u.true[2], col="red")
```

Estimates have similar precision to the original example if the reporting rate is high (Figure \@ref(fig:BrowniePlot)). Precision is reduced at lower reporting rates because of the reduced sample size of returned tags and uncertainty about lambda.

## Telemetry-based {#TelemetryFandM}

In Section \@ref(TelemetryBased), we used fixed receiving stations and telemetry\index{telemetry} tags to estimate the survival rate\index{survival rate}. Here we extend that approach to partition total mortality into fishing and natural components by giving telemetered fish an external high-reward tag\index{high-reward tags} [Design C of @hightower.harris_2017]. Live fish "detect themselves" and provide information on survival by passing within range of a receiver. Harvest information comes from the reported high-reward tags\index{high-reward tags}. Thus we have observations on two of the three possible states, with natural mortality\index{natural mortality} being the only unobserved true state. An important practical advantage of this approach is that field surveys are not needed after tagging is done (other than maintaining the receiver array and downloading detection data).

We begin with a detour to introduce dcat, the JAGS categorical distribution\index{categorical distribution}, used to estimate probabilities for observations from different categories.  

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("dcat_example.txt")
cat("
model {

# Priors
    for (j in 1:3) {
         a[j] ~ dgamma(1,1)
         } #j
     p[1] <- a[1]/sum(a[])
     p[2] <- a[2]/sum(a[])
     p[3] <- a[3]/sum(a[]) # Could also be obtained as 1-p[1]-p[2]

# Likelihood
  for (i in 1:N){
    y[i] ~ dcat(p[])
  } #i
}
    ",fill = TRUE)
sink()

# Bundle data
y <- c(1, 1, 3, 3, 2) # Each observation drawn from category 1, 2, or 3
N <- length(y)
jags.data <- list("N", "y")

model.file <- 'dcat_example.txt'

# Parameters monitored
jags.params <- c("p")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=NULL,
                n.chains = 3, n.thin=1, n.iter = 2000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

Our dcat example has a y vector with five observations in the range of 1 to 3. The categories can be of any sort; for example, age or species code for individual fish. The model parameters of interest are calculated variables representing the probabilities of the three categories. The probabilities sum to 1, so as in Section \@ref(BrownieFandM), we calculate them using an "a" vector with an uninformative gamma prior distribution\index{gamma distribution}. (A Dirichlet distribution\index{Dirichlet distribution} can also be used as an uninformative prior distribution\index{uninformative prior distribution} for probabilities that must sum to 1 [@mccarthy_2007].) For the purposes of this section, the p vector could represent probabilities of a live detection, harvest, or not being detected. The MCMC\index{Markov Chain Monte Carlo (MCMC)} process adjusts the p estimates to achieve the best possible match between model predictions and the observations (y vector). For simplicity, we let JAGS provide the initial values for the a vector. The estimates have wide credible intervals because of the small sample size.

Returning to the telemetry\index{telemetry} example, we begin with simulation code, using arbitrary values for the number of occasions and sample size. Varying the patterns for rates of exploitation and natural death is helpful in judging the model's ability to detect moderate time variation. This is a multistate model\index{multistate model} [@kéry.schaub_2011], with a matrix (PSI.STATE) describing the various true states and the probabilities for transitioning between states over time. The three rows represent true states alive, harvest, and natural death. A fish that is alive (row 1) can remain alive (probability S.true), be harvested (probability u.true), or become a natural mortality (probability v.true). A harvested fish (row 2) remains in that state with probability 1, as does a natural death (row 3). Note that the third matrix dimension for PSI.STATE is time, as the rates are time-dependent.

The observation matrix (PSI.OBS) has a similar structure. Rows are true states, columns are observed states, and the third dimension is time. There are three possible observed states: detected alive, harvested, and not detected. A live fish (row 1) can either be detected or not, with probabilities p[t] and 1-p[t]. A harvested fish (row 2) is assumed to be always detected (100% tag-reporting rate\index{tag-reporting rate}), and a natural death (row 3) is never detected.

The function for generating the simulated capture-history\index{capture history} matrix (simul.ms) is complex, but key features are as follows. All fish are assumed to be tagged (transmitters and external tags) at time 1. Next we loop over occasions and use a multinomial\index{multinomial distribution} trial to determine the true state transition between time t-1 and t (i.e., a live fish at time t-1 can remain alive, be harvested, or transition to natural death). To better understand the multinomial trial, try a simpler example in the Console. For example, <code>rmultinom(n=1, size=1, prob=c(0.7, 0.25, 0.05))</code> is a single trial (n=1) for one individual (size=1), with three states at the specified probabilities. The function returns a matrix with a 1 in a randomly determined row. The <code>which()</code> function locates the "1" and returns the row index (= fish's state). This example could represent probabilities that a live fish at time t-1 survives (0.7), is harvested (0.25), or is a natural death (0.05). Returning to the code, a second multinomial\index{multinomial distribution} trial based on the fish's true state draws a random observed outcome (e.g., a live fish is either detected or missed).

```{r eval=FALSE}
# Code modified from Kery and Schaub 2012, Section 9.5

rm(list=ls()) # Clear Environment

# Generation of simulated data
# Define probabilities as well as number of occasions, 
# states, observations and released individuals
n.occasions <- 5
n.tagged <- 100  # Simplify from Kery's version, 
                 # only an initial release, and no spatial states

u.true <- c(0.2, 0.1, 0.3, 0.2)
v.true <- c(0.1, 0.15, 0.15, 0.1)
S.true <- 1-u.true-v.true
p <- c(1, 0.8, 0.8, 0.6, 0.7)
#  Detection probability fixed at 1 for first period, 
# n.occasions-1 searches (e.g start of period 2, 3, ..)

n.states <- 3
n.obs <- 3

# Define matrices with survival, transition and detection probabilities

# 1. State process matrix, dimensions 1=state[t], 2=state[t+1], 3=time
PSI.STATE <- array(NA, dim=c(n.states, n.states, n.occasions-1))
   for (t in 1:(n.occasions-1)){
      PSI.STATE[,,t] <- matrix(c(
      S.true[t], u.true[t], v.true[t],
      0, 1, 0,
      0, 0, 1), nrow = n.states, byrow = TRUE)
      } #t

# 2.Observation process matrix.  1=true state, 2=observed state, 3=time
PSI.OBS <- array(NA, dim=c(n.states, n.obs, n.occasions))
   for (t in 1:n.occasions){
      PSI.OBS[,,t] <- matrix(c(
      p[t], 0, 1-p[t],
      0,  1, 0,  # Caught w/ high-reward tag, assume 100% reporting
      0, 0, 1), nrow = n.states, byrow = TRUE)  
        # Natural deaths never detected
      } #t

# Define function to simulate multistate capture-recapture data
simul.ms <- function(PSI.STATE, PSI.OBS, n.tagged, n.occasions, 
                     unobservable = NA){
   # Unobservable: number of state that is unobservable
   CH <- CH.TRUE <- matrix(NA, ncol = n.occasions, nrow = n.tagged)
   CH[,1] <- CH.TRUE[,1] <- 1 # All releases at t=1
   for (i in 1:n.tagged){
     for (t in 2:n.occasions){
         # Multinomial trials for state transitions
         CH.TRUE[i,t] <- which(rmultinom(n=1, size=1,
                            prob=PSI.STATE[CH.TRUE[i,t-1],,t-1])==1)
         # which vector element=1; i.e., which state gets random draw
         # at time t given true state at t-1. True state determines
         # which row of PSI.STATE provides probabilities

         # Multinomial trials for observation process
         CH[i,t] <- which(rmultinom(1, 1, PSI.OBS[CH.TRUE[i,t],,t])==1)
         # which observation gets the 1 random draw, given 
         # true time t state.
         } #t
      } #i

    return(list(CH=CH, CH.TRUE=CH.TRUE)) 
      # True (CH.TRUE) and observed (CH)
   } # simul.ms
```

The JAGS code is also complex but does generally mirror the simulation code.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code for estimating model parameters
sink("Mort_Tel_HRTag.txt")
cat("
model {

# Priors
for (t in 1:(n.occasions-1)) {
     for (j in 1:3) { # Rows are return periods, columns are fates
         a[t,j] ~ dgamma(1,1)
         } #j
     S.est[t] <- a[t,1]/sum(a[t,]) # Probability of survival
     u.est[t] <- a[t,2]/sum(a[t,]) # Probability of harvest
     v.est[t] <- a[t,3]/sum(a[t,]) # Probability of natural death
     # v.est[t] could also be obtained as 1-S.est[t]-u.est[t]
    } #t

  p[1] ~ dbern(1)
  for (t in 2:n.occasions){
    p[t] ~ dunif(0,1)
    }#t

# Define state-transition and observation matrices
# Probabilities of true state at t+1 given state at time t
    for (t in 1:(n.occasions-1)){
    ps[1,t,1] <- S.est[t]
    ps[1,t,2] <- u.est[t]
    ps[1,t,3] <- v.est[t]
    ps[2,t,1] <- 0
    ps[2,t,2] <- 1
    ps[2,t,3] <- 0
    ps[3,t,1] <- 0
    ps[3,t,2] <- 0
    ps[3,t,3] <- 1
    } #t

# Probabilities of observed states given true state
    for (t in 2:n.occasions){
    po[1,t,1] <- p[t]  # Row 1 true state = alive
    po[1,t,2] <- 0
    po[1,t,3] <- 1-p[t]
    po[2,t,1] <- 0     # Row 2 true state = harvested
    po[2,t,2] <- 1
    po[2,t,3] <- 0
    po[3,t,1] <- 0     # Row 3 true state= natural death
    po[3,t,2] <- 0
    po[3,t,3] <- 1
    } #t

    # Likelihood
    for (i in 1:n.tagged){
    z[i,1] <- y[i,1] # Latent state known at time of tagging (t=1)
    for (t in 2:n.occasions){
    # State process: draw state at time t given state at time t-1
    z[i,t] ~ dcat(ps[z[i,(t-1)], (t-1),])
    # Observation process: draw obs state given true state at time t
    y[i,t] ~ dcat(po[z[i,t], t,])
    } #t
    } #i
}
    ",fill = TRUE)
sink()

# Generate initial values for z. Modified from Kery code for
# JAGS inits, age-specific example 9.5.3
z.init <- function(ch) {
  # State 1=Obs 1 (alive) State 2=Obs 2 (fishing death). 
  # Start w/ known states from obs capture-history,
  # replace "3" with possible state(s)
  ch[ch==3] <- -1  # Not observed so temporarily replace w/ neg value
  ch[,1] <- NA  # Initial value not needed for release period
  for (i in 1:nrow(ch)){
    if(max(ch[i,], na.rm=TRUE)<2){
      ch[i, 2:ncol(ch)] <- 1 
      # Not detected dead so initialize as alive (after release period)
      } else {
      m <- min(which(ch[i,]==2))  
        # Period when fishing death first detected
      if(m>2) ch[i, 2:(m-1)] <- 1  
      # Initialize as alive up to period prior to harvest
      ch[i, m:ncol(ch)] <- 2  
        # Initialize as dead after detected fishing death
      } # if/else
} # i
  return(ch)
} # z.init

# Call function to get simulated true (CH.TRUE) and obs (CH) states
sim <- simul.ms(PSI.STATE, PSI.OBS, n.tagged, n.occasions)
y <- sim$CH

# Bundle data
jags.data <- list("n.occasions", "n.tagged", "y")

# Initial values.
jags.inits <- function(){ list(z = z.init(y))}

model.file <- 'Mort_Tel_HRTag.txt'

# Parameters monitored
jags.params <- c("u.est", "v.est", "p")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 5000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

The parameters for rates of survival\index{survival rate}, exploitation\index{exploitation rate} and natural death\index{natural death rate} use the same approach as in the above dcat example. The "a" matrix is estimated internally using a vague gamma prior distribution\index{gamma distribution}, then the three fractions estimate the probabilities of the three possible true states (survival\index{survival rate}, harvest\index{exploitation rate}, natural death\index{natural death rate}), which sum to 1. The latent true states (z matrix) are estimated in the likelihood\index{likelihood} section of the code. The dcat (categorical distribution\index{categorical distribution}) is used to draw the true state at time t given state at time t-1, as well as the observation at each time given the true state.

One difficult part of fitting this model is obtaining initial values for the latent true states. JAGS will refuse to update model parameters if initial values are inconsistent with the observations [(link)](https://www.vogelwarte.ch/de/projekte/publikationen/bpa/code-for-running-bpa-using-jags). The function used here <code>z.init()</code> initializes all unknown true states to be "alive" unless a harvest occurs. This works because fish not detected could either be alive or a natural death, so "alive" is a valid initial value. Individuals that are ultimately harvested are known to be alive up to the period prior to harvest. It can be instructive to compare the matrices containing true and observed states with the initial values. For example, initial values can be saved by entering z.test <- z.init(y) in the Console. Entering sim$CH.TRUE[1:5,] and CH[1:5,] in the Console will display true and observed states for the first five individuals for comparison to z.test.

Results for the chosen simulation settings are moderately reliable. There are many parameters, as the model estimates true latent states as well as the probabilities for survival\index{survival rate}, harvest\index{harvest rate} and natural death\index{natural death rate}. It seems to consistently detect the increase in harvest rate\index{harvest rate} in period 3. Uncertainty increases toward the end of the study, especially for probabilities of detection and natural death. This makes sense because fish not detected on the final occasion could either be alive or a natural death.

Perhaps the most obvious simulation settings to vary are the fairly robust sample size of tagged fish (100) and the detection probabilities (0.6 and above). Uncertainty increases markedly as detection probabilities decrease, so these simulations can be very helpful in planning the field study (e.g., number of receiver stations). The number of occasions can also be varied but it requires commensurate changes in the vectors for u, v, and p.

## Exercises

1. For the exploitation rate study design (Section \@ref(ExpRate)), modify the JAGS code to plot the posterior distribution for the exploitation rate. Include a vertical line showing the underlying true exploitation rate.

2. For the completed tagging study design (Section \@ref(Hearn)), modify the code to use a single F parameter (representing an average over periods).

3. For the multi-year tagging study (Section \@ref(BrownieFandM)), run three replicate simulations of the version using planted tags, with tag releases of 1000 (current code), 100, and 30. How does uncertainty (e.g. credible interval width) vary? What sample size would you recommend if planning this study? Also, save credible intervals for natural death rate for period 3 (v3) for the 1000 fish releases (for exercise 4 below).

4. Compare results for the natural death rate in period 3 (v3) (from exercise 3) with a modified design using five release and return periods. Extend the exploitation (u) and natural death (v) vectors by using the values from periods 1-2 for the final two periods. How do credible intervals for v3 change when the study is extended?

5. For the telemetry study (Section \@ref(TelemetryFandM)), compare results (particularly uncertainty) for the natural death rate for period 4 (v4) when the detection probability in period 5 (p5) takes on the following values: 0.2, 0.4, 0.7, 0.9. How might this affect planning for conducting the field work?
