# Mortality Components {#Mortality}

Reliable estimates of the survival rate (Chapter \@ref(Survival)) are essential in understanding fish population dynamics, but for exploited species, there is generally interest in partitioning mortality (1-survival) into its component rates. Knowing the relative importance of fishing\index{fishing mortality} versus natural mortality\index{natural mortality} clarifies the extent to which fishery managers can use regulations to affect population change. The sections that follow illustrate different approaches for examining the impact of fishing and for estimating fishing\index{fishing mortality} and natural mortality\index{natural mortality} rates.

## Exploitation rate {#ExpRate}

A tagging study can provide a real-time estimate of the exploitation rate\index{exploitation rate}, or the fraction of the population being harvested. Similar to the tag-return\index{tag-return} method of estimating survival (Section \@ref(BrownieSurvival)), the only field work required is the initial tagging effort. Here there is a single release of fish with external tags at the start of the study, and the number of returned tags is recorded over a specified interval. The study duration could be an entire year, or a fishing season for a population managed by seasonal harvest. A high exploitation rate might indicate that harvest regulations such as a size limit or creel limit could be worthwhile.  Alternatively, a low estimate might indicate that other factors such as natural mortality\index{natural mortality} or recruitment\index{recruitment} may be more important in regulating population size. With only a single release, we cannot estimate survival\index{survival rate} (or its complement, mortality), but it is nevertheless an important first step in judging the relative impact of fishing.

There are several practical issues to consider in planning an exploitation rate study. We want to ensure that essentially all tags of harvested fish are reported, so high-reward tags\index{high-reward tags} (e.g., $100) could be used [@pollock.etal_2001]. Tag loss\index{tag loss} is another potential source of bias. It can be estimated by double-tagging (giving some or all fish two tags, and recording returns of two versus only one tag), but it greatly complicates the analysis. For this example, we will keep things simple and assume that tag loss\index{tag loss} is negligible. We also assume that there is no immediate tagging mortality\index{tagging mortality} (i.e., due to capture, handling and tagging). This can be challenging to investigate. It is sometimes estimated by holding a sample of fish in a cage after tagging but this can itself be a source of mortality [@pollock.pine2007].  For now, we assume that there is no short- or long-term mortality associated with tagging. We assume that all caught fish are harvested. If catch-and-release was occurring, our model could be extended to estimate rates of harvest and catch-and-release. The decision of how many fish to tag will depend on the required precision of the study, and as usual it can be examined through simulation. Finally, it is important that tagged fish be representative of the entire population. This refers not only to size or age, but also spatial distribution. Tagging could be done at randomly selected locations, or when the entire study population is concentrated within a single area (e.g., winter holding area or spawning location). The key is for all fish, including the tagged subset, to have the same probability of being harvested.

Our simulation code uses an assumed exploitation rate\index{exploitation rate} (u) of 0.4. The value could be based on a literature review or prior work from the study area or similar areas regionally. The number of returned tags has a binomial distribution\index{binomial distribution} (Section \@ref(BinomialDist)):

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)
```

The simulation provides only a single observation for the number of returned tags. What lines of code could you add to look at the distribution of returned tags (using a new variable for the vector of tag returns)?

The JAGS code is also extremely simple. We have one line of code for the uninformative prior distribution\index{uninformative prior distribution} for the exploitation rate\index{exploitation rate}, and one for the likelihood\index{likelihood}.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dunif(0,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values
jags.inits <- function(){ list(u.est=runif(n=1, min=0, max=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

A single run with a release of 100 tagged fish provided a credible interval of 0.27-0.46 (from a random return of 36 tags). Experiment with different sample sizes and consider what level of precision would be adequate for management purposes. As always, this is an optimistic scenario, with the number of returned tags perfectly conforming to a binomial distribution\index{binomial distribution}.

### Informative prior distribution {#InformativePriors}
@banner.etal_2020 discourage the (at least uncritical) use of "default" prior distributions\index{prior distribution} (which are used throughout this book). It is often true that previous studies contain relevant information, and Bayesian methods provide an objective framework for building on that previous information [@mccarthy_2007]. The simple example of estimating the exploitation rate\index{exploitation rate} is a perfect opportunity to look at how an informative prior\index{informative prior distribution} would be implemented and to explore its effect on the posterior distribution\index{posterior distribution}.

\newpage
To compare posterior distributions obtained using uninformative and informative priors\index{informative prior distribution}, we use two copies of the above code. In the uninformative prior\index{uninformative prior distribution} version, we simply replace <code>dunif(0,1)</code> with <code>dbeta(1,1)</code> in the JAGS code, and we replace the uniform initial value with the corresponding beta\index{beta distribution} (<code>rbeta(n=1, shape1=1, shape2=1)</code>). Neither change affects the calculations because the two distributions are equivalent, but it sets up the model for the informative prior\index{informative prior distribution} version.

For the informative\index{informative prior distribution} case, the exploitation rate\index{exploitation rate} simulation is unchanged. We simply add two lines of code for the pilot study that will be used for the prior\index{prior distribution}. The informative beta distribution\index{beta distribution} in the JAGS code has two parameters: successes+1 and failures+1 [@bolker2008]. We augment jags.data with the pilot study data, and use initial values from the informative prior\index{informative prior distribution}. The JAGS code is followed by R code for producing two plots, showing the prior\index{prior distribution} and posterior distributions\index{posterior distribution} for the uninformative (upper plot) and informative (lower) cases. We specify margins for each plot to reduce blank space. The <code>density()</code> function provides a smoothed probability density for the exploitation rate estimates. The <code>lines()</code> function adds the prior distribution\index{prior distribution} to each plot, generated using the <code>dbeta()</code> function.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dbeta(1,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values
jags.inits <- function(){ list(u.est=rbeta(n=1, shape1=1, shape2=1))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)

# Informative prior
n.pilot <- 20
n.returned.pilot <- rbinom(n=1, prob=u.true, size=n.pilot)

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("ExpRate.txt")
cat("
model{
    # Priors
    u.est ~ dbeta((n.returned.pilot+1), 
         (n.pilot-n.returned.pilot+1)) # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}
    ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("n.returned", "n.tagged", "n.pilot", 
                  "n.returned.pilot")

# Initial values
jags.inits <- function(){ list(u.est=rbeta(n=1, 
                          shape1=(n.returned.pilot+1), 
                          shape2=(n.pilot-n.returned.pilot+1)))}

model.file <- 'ExpRate.txt'

# Parameters monitored
jags.params <- c("u.est")

# Call JAGS from R
informative <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 4000, n.burnin=2000,
                model.file)
print(informative)
plot(informative)

# Vague prior and posterior
par(mfrow=c(2,1))
par(mar=c(2,4,2,1)) # bottom, left, right, top margins
dens1 <- density(jagsfit$BUGSoutput$sims.list$u.est)
dens2 <- density(informative$BUGSoutput$sims.list$u.est)
y.bounds <- range(dens1$y, dens2$y)
x.bounds <- range(jagsfit$BUGSoutput$sims.list$u.est, 
              informative$BUGSoutput$sims.list$u.est)
plot(density(jagsfit$BUGSoutput$sims.list$u.est), xlab="", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true,  lty=3, lwd=3)
x.vec <- seq(x.bounds[1], x.bounds[2], length.out=101)
lines(x.vec,
      dbeta(seq(x.bounds[1], x.bounds[2], length.out=101), shape1=1, 
            shape2=1), lty=2)
legend(x = "topleft", lty = c(2,1), text.font = 1,  
       legend=c("Prior", "Posterior")) 

# Informative prior and posterior
par(mar=c(4,4,1,1))
plot(dens2,xlab="Estimated exploitation rate", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true,  lty=3, lwd=3)
lines(x.vec, dbeta(seq(x.bounds[1], x.bounds[2], 
                    length.out=101), shape1=(n.returned.pilot+1), 
                    shape2=(n.pilot-n.returned.pilot+1)), lty=2)

```

\newpage
A pilot study provides a valuable opportunity to work out field logistics, and allows for construction of an informative prior\index{informative prior distribution}. Informative priors\index{informative prior distribution} can also be based on the mean and variance of published estimates, and they can include the additional variation within individual studies [@mccarthy.masters_2005; @regehr.etal_2018]. Another approach is to base the prior on ecological theory, for example, a prior for survival rate\index{survival rate} based on its relationship to body mass [[@mccarthy.masters_2005]. For our tagging example, the beta distribution\index{beta distribution} worked well as an informative prior because the two parameters are based on the number of successes (returned tag) and failures (not returned). The plotted distributions suggest a modest gain in precision from a pilot release of 20 tagged fish, given a full study release of 100 (Figure \@ref(fig:InfPriorPlot)). The effect of an informative prior\index{informative prior distribution} can also be evaluated by comparing the width of credible intervals\index{credible interval} [@mccarthy.masters_2005].

The benefit of an informative prior\index{informative prior distribution} depends on the information content of the prior\index{prior distribution} versus the new data [@mccarthy.masters_2005]. @lemoine_2019 recommends carrying out analyses of real data using multiple prior distributions\index{prior distribution} of varying strengths, to better understand the information content of the new data and the effect of priors\index{prior distribution}. One other point to consider if conducting a pilot-tagging study, rather than using published information, is the random aspect of the number returned. Try multiple runs with small pilot releases to see how often the prior\index{prior distribution} is unhelpful for estimating the true exploitation rate\index{exploitation rate}. What factors would you consider when deciding whether to base an informative prior\index{informative prior distribution} on a pilot study versus a summary of published estimates?

```{r InfPriorPlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Prior and posterior distributions for estimated exploitation rate, using uninformative (upper) and informative (lower) priors. The full study sample size is 100 tagged fish; the pilot study (lower panel) has a sample size of 20. The true exploitation rate is shown as a dotted vertical line.', out.width="90%"}

rm(list=ls()) # Clear Environment 
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

UninfPrior <- function() {
    # Priors
    u.est ~ dbeta(1,1)  # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}

InfPrior <- function() {
    # Priors
    u.est ~ dbeta((n.returned.pilot+1), 
         (n.pilot-n.returned.pilot+1)) # Exploitation rate

    # Likelihood
    n.returned ~ dbin(u.est, n.tagged)
}

# Tagging study to estimate exploitation rate
u.true <- 0.4
n.tagged <- 100
n.returned <- rbinom(n=1, prob=u.true, size=n.tagged)

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("n.returned", "n.tagged")

# Initial values.
jags.inits <- function(){list(u.est=rbeta(n=1, shape1=1, shape2=1))}

# Fit the model
UninfPrior_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = UninfPrior,
    parameters.to.save = c("u.est")
  )

# Informative prior - add pilot study
n.pilot <- 20
n.returned.pilot <- rbinom(n=1, prob=u.true, size=n.pilot)

# JAGS code

# Bundle data
jags.data <- list("n.returned", "n.tagged", "n.pilot", "n.returned.pilot")

# Initial values.
jags.inits <- function(){list(u.est=rbeta(n=1, shape1=1, shape2=1))}

# Fit the model
InfPrior_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = InfPrior,
    parameters.to.save = c("u.est")
  )

# Vague prior and posterior
par(mfrow=c(2,1))
par(mar=c(2,4,2,1)) # bottom, left, right, top margins
dens1 <- density(UninfPrior_jags$BUGSoutput$sims.list$u.est)
dens2 <- density(InfPrior_jags$BUGSoutput$sims.list$u.est)
y.bounds <- range(dens1$y, dens2$y)
x.bounds <- range(UninfPrior_jags$BUGSoutput$sims.list$u.est, 
              InfPrior_jags$BUGSoutput$sims.list$u.est)
plot(density(UninfPrior_jags$BUGSoutput$sims.list$u.est), xlab="", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true,  lty=3, lwd=3)
x.vec <- seq(x.bounds[1], x.bounds[2], length.out=101)
lines(x.vec,
      dbeta(seq(x.bounds[1], x.bounds[2], length.out=101), shape1=1, shape2=1),
      lty=2)
legend(x = "topleft", lty = c(2,1), text.font = 1,  
       legend=c("Prior", "Posterior")) 

# Informative prior and posterior
par(mar=c(4,4,1,1))
plot(dens2,xlab="Estimated exploitation rate", main="", 
     xlim=x.bounds, ylim=y.bounds)
abline(v=u.true,  lty=3, lwd=3)
lines(x.vec, dbeta(seq(x.bounds[1], x.bounds[2], 
                    length.out=101), shape1=(n.returned.pilot+1), 
                    shape2=(n.pilot-n.returned.pilot+1)), lty=2)

```


## Completed tagging study {#Hearn}

A single release of tags (Section \@ref(ExpRate)) can also be used to estimate the rate of natural mortality\index{instantaneous natural mortality rate}. @hearn.etal1987 provide a method that can be used in completed tagging studies\index{completed tagging study}, that is, a study of sufficient duration that tag returns have ceased (assumed to indicate that no live tagged fish remain). Their model uses instantaneous rates, which we have up to this point avoided. The relationship between probabilities (survival\index{survival rate}, death\index{mortality rate}, exploitation\index{exploitation rate}, natural death\index{natural death rate}) and instantaneous rates is as follows. The instantaneous rate of population decline is $dN/dt = -Zt$, where N is population size and Z is the instantaneous total mortality rate\index{instantaneous total mortality rate} (rate of decline due to fishing and natural sources). An advantage of instantaneous rates is that they are additive. Stock assessment models routinely partition Z into whatever categories are useful and estimable; for example, fishing\index{fishing mortality} versus natural mortality\index{natural mortality}, commmercial versus recreational fishing, longline versus trawl, etc.  The most common partitioning is for fishing (F) versus natural (M) sources: $dN/dt = -(M+F)t$. Integrating this equation provides us with an expression for population size at any time t:

$N_t = N_0 * exp(-(M+F)*t)$

where N~0~ is initial abundance, and $S=exp(-(M+F)*t)$ is the survival rate\index{survival rate}. The total mortality rate\index{total mortality rate} (probability of dying from all causes) is $A=1-S$. The exploitation rate\index{exploitation rate} is obtained as u=FA/Z, which makes sense as the fraction of total mortality due to fishing (F/Z). Similarly the probability of natural death\index{natural death rate} is v=MA/Z. These expressions make it possible to go back and forth between probabilities and instantaneous rates, depending on which is more useful or traditional for a particular model. These expressions apply for the most common situation where fishing\index{fishing mortality} and natural mortality\index{natural mortality} occur simultaneously (referred to by @ricker_1975 as a Type 2 fishery). @ricker_1975 provides alternate expressions for the less common case (a Type 1 fishery) where natural mortality\index{natural mortality} occurs after a short fishing season.

Unlike the probabilities for survival, harvest, etc., instantaneous rates are unbounded. In a Bayesian context, this makes them more challenging to estimate compared to probabilities, which are conveniently bounded by 0 and 1. Instantaneous rates are also less intuitive in terms of their magnitude compared to probabilities. There is typically more uncertainty about M than F because natural deaths are almost never observed [@hearn.etal1987; @quinn.deriso_1999]. The joke about M is that it is often assumed to be 0.2, because .2 looks like a distorted version of a question mark. In a review of published natural mortality estimates, @vetter_1988 found that a majority were less than 0.5, but values exceeding 2.0 were occasionally reported.

The @hearn.etal1987 method requires only that fishing mortality\index{fishing mortality} in each period be greater than zero, in order to establish the study's endpoint. Their model assumes a constant rate of natural mortality\index{instantaneous natural mortality rate}, so the estimated rate would in practice represent the average over the study. Their base model assumes that tags of harvested fish are always reported\index{tag-reporting rate}, there is no mortality associated with tagging\index{tagging mortality}, and there is no tag loss\index{tag loss}.

The @hearn.etal1987 model used exact times at large for returned tags to estimate the natural mortality rate. The following Bayesian version is a modification of their method, using tag returns by period (assumed here to be annual) to establish the length of the completed study. Simulated fishing mortality rates are greater than 0 but vary randomly within a user-defined range. We begin with simulation code:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

n.tagged <- 100
Periods <- 40 
# Set to high value to ensure that end of study is observed
F.true <- runif(n=(Periods-1), min=0.3, max=0.5)
M.true <- 0.2
S.true <- exp(-F.true - M.true)
u.true <- F.true * (1-S.true) / (F.true + M.true)

TagsAtRisk <- array(data=NA, dim=Periods)
TagsAtRisk[1] <- n.tagged
TagFates <- array(data=NA, dim=(Periods-1)) # Returns by period

for (j in 2:Periods){
  TagsAtRisk[j] <- rbinom(n=1, size=TagsAtRisk[j-1], prob=S.true[j-1])
  TagFates[j-1] <- rbinom(n=1, size=(TagsAtRisk[j-1]
                     -TagsAtRisk[j]),prob=(F.true[j-1]
                      /(F.true[j-1]+M.true))) 
  # Random realization: fishing deaths
} #j

t.Completed <- max(which(TagFates[]>0))
# Locate period when study completed (last return)
NatDeaths <- n.tagged - sum(TagFates)
TagFates <- c(TagFates[1:t.Completed],NatDeaths) 
# Returns + all natural deaths
```

The vectors for tags-at-risk and fates are binomially-distributed\index{binomial distribution} random realizations. Tags at risk are obtained using the number at risk in the previous period and the survival rate\index{survival rate}. Tag returns (stored in tag fates vector) are obtained from the number of deaths between periods and the fraction of deaths due to fishing (F/Z). The <code>which()</code> function provides a vector of periods when tag returns are greater than 0, and the <code>max()</code> function chooses the final period with at least one tag return. Try <code>which(TagFates[]>0)</code> in the Console to make clear how this expression works. The final version of the tag fates vector contains tag returns for the completed study length plus a final value for natural deaths (any tagged fish not seen in the completed tagging study).

The JAGS code for fitting the model is similar to that for the tag-based method of estimating survival (Section \@ref(BrownieSurvival)), except that our parameters now are instantaneous rates. We use an arbitrary uniform prior distribution\index{prior distribution}, with an upper bound (2) set high enough to ensure that the interval contains the true value. The likelihood\index{likelihood} uses a multinomial distribution\index{multinomial distribution} (Section \@ref(Multinomial)), where cell probabilities represent the fraction returned in each period plus a final value for the fraction not seen again (natural deaths in our completed tagging study\index{completed tagging study}). The last three lines of code show the estimated posterior distribution\index{posterior distribution} compared to the true value. 

```{r eval=FALSE}

# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
sink("Hearn.txt")
cat("
  model {
  # Priors
  M.est ~ dunif(0, 2)  # Instantaneous natural mortality rate
  for (i in 1:t.Completed) {
     F.est[i] ~ dunif(0,2) # Instantaneous fishing mortality rate
     S.est[i] <- exp(-M.est - F.est[i])
     u.est[i] <- F.est[i] * (1-S.est[i])/(F.est[i]+M.est) # FA/Z
  } #i

# Cell probabilities
  p.est[1] <- u.est[1]
  for (i in 2:t.Completed) {
    p.est[i] <- prod(S.est[1:(i-1)])*u.est[i]
      } #i
    p.est[t.Completed+1] <- 1 - sum(p.est[1:t.Completed])
      # Prob of not being seen again
  TagFates[1:(t.Completed+1)] ~ dmulti(p.est[1:(t.Completed+1)],
                                n.tagged)
 }
  ",fill=TRUE)
sink()

# Bundle data
jags.data <- list("TagFates", "t.Completed", "n.tagged")

# Initial values
jags.inits <- function(){list(M.est=runif(1, 0, 2), 
                              F.est=runif(t.Completed, 0, 2))}

model.file <- 'Hearn.txt'

# Parameters monitored
jags.params <- c("M.est", "F.est")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.iter = 4000, n.burnin=2000,
                model.file)
print(jagsfit)
plot(jagsfit)

par(mfrow=c(1,1)) # Default plot settings
mar = c(5.1, 4.1, 4.1, 2.1)
# Look at posterior distribution versus true value
hist(jagsfit$BUGSoutput$sims.list$M.est, 
     xlab="Estimated natural mortality rate", main="")
abline(v=M.true,  lty=3, lwd=3)

```

This approach works because there are only two fates for tagged fish: caught and reported or natural death. Using a completed tagging study\index{completed tagging study} means that (when all assumptions hold) the number of natural deaths is known exactly (number of tag releases - total returns). The model adjusts the estimate of M in order to account for all the natural deaths by the end of the completed study. The length of the completed tagging study\index{completed tagging study} depends on the assumed values for F and M. The estimated natural mortality rate\index{instantaneous natural mortality rate} will be higher when the study length is short and when many tagged fish are not seen again.

There are elements of randomness in the length of the completed tagging study. One example is when tagged fish at risk survive for one or more periods beyond the last tag return but are ultimately natural deaths. This can occasionally be seen by comparing the TagsAtRisk and TagFates vectors. There can also be embedded zeros in the vector of tag returns when the number of tags at risk is low. Simulation runs suggest that this approach to determining study length works reasonably well but should be evaluated for the likely range of fishing \index{instantaneous fishing mortality rate} and natural mortality\index{instantaneous natural mortality rate} rates.

For the default settings, estimates of M tend to be reliable (Figure \@ref(fig:HearnPlot)), which is unsurprising given that it is modeled as a constant rate and estimated from multiple years of data. Run the full code (simulation and analysis) multiple times, using different assumed values for the natural mortality rate\index{instantaneous natural mortality rate}, to build intuition about the reliability of this method. (Periods here are assumed to be years, but a larger number of periods might be needed for monthly intervals or if a very low natural mortality rate is used.) How would uncertainty change with a different number of tags released?

```{r HearnPlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Example of estimated posterior distribution and true natural mortality rate (dotted vertical line) using the completed tagging study (Hearn et al. 1987) approach.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, .1))

Hearn <- function() {
  # Priors
  M.est ~ dunif(0, 2)  # Instantaneous natural mortality rate
  for (i in 1:t.Completed) {
     F.est[i] ~ dunif(0,2) # Instantaneous fishing mortality rate
     S.est[i] <- exp(-M.est - F.est[i])
     u.est[i] <- F.est[i] * (1-S.est[i])/(F.est[i]+M.est) # FA/Z
  } #i

# Cell probabilities
  p.est[1] <- u.est[1]
  for (i in 2:t.Completed) {
    p.est[i] <- prod(S.est[1:(i-1)])*u.est[i]
      } #i
    p.est[t.Completed+1] <- 1 - sum(p.est[1:t.Completed])   # Prob of not being seen again
  TagFates[1:(t.Completed+1)] ~ dmulti(p.est[1:(t.Completed+1)], n.tagged)
}

# Simulation code for Hearn completed tagging study
n.tagged <- 100
Periods <- 40 
# Set to high value to ensure that end of study is observed
F.true <- runif(n=(Periods-1), min=0.3, max=0.5)
M.true <- 0.2
S.true <- exp(-F.true - M.true)
u.true <- F.true * (1-S.true) / (F.true + M.true)

TagsAtRisk <- array(data=NA, dim=Periods)
TagsAtRisk[1] <- n.tagged
TagFates <- array(data=NA, dim=(Periods-1)) # Returns by period

for (j in 2:Periods){
  TagsAtRisk[j] <- rbinom(n=1, size=TagsAtRisk[j-1], prob=S.true)
  TagFates[j-1] <- rbinom(n=1, size=(TagsAtRisk[j-1]
                                     -TagsAtRisk[j]),(F.true/(F.true+M.true))) 
  # Random realization: fishing deaths
} #j

t.Completed <- max(which(TagFates[]>0))
# Locate period when study completed (last return)
NatDeaths <- n.tagged - sum(TagFates)
TagFates <- c(TagFates[1:t.Completed],NatDeaths) 
# Returns + all natural deaths

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("TagFates", "t.Completed", "n.tagged")

# Initial values.
jags.inits <- function(){list(M.est=runif(1, 0, 2), F.est=runif(t.Completed, 0, 2))}

# Fit the model
Hearn_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = Hearn,
    parameters.to.save = c("M.est", "F.est")
  )

# Look at posterior distribution versus true value
hist(Hearn_jags$BUGSoutput$sims.list$M.est, 
     xlab="Estimated natural mortality rate", main="")
abline(v=M.true,  lty=3, lwd=3)
```

Estimated fishing mortality rates\index{instantaneous fishing mortality rate} for the first few periods are moderately accurate, especially for larger tag releases. Estimates near the end of the completed study are typically high with wide credible intervals\index{credible interval}. There would likely be some improvement from simplifying the model to estimate only an average annual fishing mortality rate\index{instantaneous fishing mortality rate} rather than period-specific estimates.

## Multi-year tagging study {#BrownieFandM}

We return to the multi-year tagging study (Section \@ref(BrownieSurvival)) but now assume that we have information about the tag-reporting rate\index{tag-reporting rate}. This information allows us to partition mortality into fishing\index{fishing mortality} and natural\index{natural mortality} sources. Tag-reporting rate\index{tag-reporting rate} is fixed at 1.0 in the following code, as might be appropriate when using high-reward tags\index{high-reward tags}. In a later section of code, we estimate it internally, using planted tags [@hearn.etal_2003] and an additional likelihood\index{likelihood} component. Information about the tag-reporting rate\index{tag-reporting rate} is very powerful. For example, a 100% tag-reporting rate means that (assuming no assumptions are violated) you ultimately know the number of natural deaths because all other tags will be reported (as in Section \@ref(Hearn)). There is uncertainty in the short term, because tags not returned could belong to fish still alive and at risk; so information about natural deaths is clear only after sufficient time has passed.

Our simulation code is similar to that of Section \@ref(BrownieSurvival) except that we now specify the probabilities of harvest (u.true) and natural death (v.true). We use vectors so that the rates can vary by period. We have arbitrarily set the probabilities for harvest higher than for natural death, to investigate whether we can reliably detect the predominant threat in our simulated study design.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Parameter values for three-yr experiment
Periods <- 3  # Equal number of release and return periods assumed
NTags <- 1000 # Number released each period
NumTagged <- rep(NTags, Periods) # Release NTags fish each period

u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
lam.true <- 1  # Reporting rate

# Calculated value(s), array creation
S.true <- 1 - u.true - v.true
TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again

for (i in 1:Periods){ 
# Expected values for tags-at-risk, returned/not-seen-again
  TagsAtRisk[i,i] <- NumTagged[i]
  TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
   # Returns in release period
  j <- i+1
  while(j <= Periods) {
    TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
    TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
    j <- j+1
   } #while
  TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) 
    # Tags not seen again
  } #i

RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
#Random trial
  for (i in 1:Periods) # Upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                        TagFate[i,i:(Periods+1)]))
  } #i

```

The analysis is again similar to Section \@ref(BrownieSurvival) except that now we have a multinomial distribution\index{multinomial distribution} (Section \@ref(Multinomial)) with three possible fates (survival, harvest, natural death). Following Section 9.6 of @kéry.schaub_2012, we use three gamma-distributed\index{gamma distribution} (Section \@ref(GammaDist)) parameters that are scaled by their sum, thus ensuring that the rescaled values sum to 1. The parameters of interest (u, v, S) are calculated functions of the 'a' parameters, which are used internally but not themselves of interest. 

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

  # JAGS code
  sink("TagReturn.txt")
  cat("
  model {
  # Priors
  lam.est <- 1 #  100% reporting of tags
  for (i in 1:Periods) {
     for (j in 1:3) # Rows are return periods, columns are fates
       {
         a[i,j] ~ dgamma(1,1)
       } #j
     S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
     u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest
     v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
     # v.est[i] could also be obtained as 1-S.est[i]-u.est[i]
  } #i

# Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
      } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])
    # Probability of not being seen again
    } #i
  for (i in 1:Periods) {
  RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], 
                                    NumTagged[i])
  }#i
 }
  ",fill=TRUE)
  sink()

# Bundle data
  jags.data <- list("RandRet", "Periods", "NumTagged")

# Initial values
  a.init <- array(data=NA, dim=c(Periods, 3))
  for (i in 1:3){
    a.init[i,] <- runif(n=3, min=0, max=1)
    a.init[i,] <- a.init[i,]/sum(a.init[i,]) # Rescale to sum to 1
  }
  jags.inits <- function(){ list(a=a.init)}

  model.file <- 'TagReturn.txt'

  # Parameters monitored
  jags.params <- c("u.est", "v.est")

   # Call JAGS from R
  jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                  n.chains = 3, n.thin = 1, n.iter = 4000, 
                  n.burnin=2000, model.file)
  print(jagsfit)
  plot(jagsfit)

```

We provide arbitrary starting values for the 'a' parameters by rescaling uniform (0,1) random variates. Using a large release on each occasion (1000), our estimates are generally close to the true values except that v.est[3] is imprecise and typically overestimated. It is the most difficult time-dependent parameter to estimate. We get direct information (from returned tags) about the final exploitation rate\index{exploitation rate}, but there is much uncertainty about whether fish not seen are still at risk or are natural deaths. This is especially true for the final release because most fish are still at large (examine the TagFate and RandRet matrices). Note that this differs from a completed tagging study (Section \@ref(Hearn)), where the study continues until no live fish remain.

The analysis can be extended to include estimation of the tag-reporting rate. We use the simplest approach of a binomial\index{binomial distribution} experiment using planted tags\index{planted tags} [@hearn.etal_2003]. The fraction of planted tags\index{planted tags} that is returned is a direct estimate of the tag-reporting rate\index{tag-reporting rate}. The only change required in the simulation code is to replace the assignment lam.true <- 1.0 with the following three lines:

```{r eval=FALSE}
lam.true <- 0.9  # Reporting rate
PlantedTags <- 30
PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
```

The assumed tag-reporting rate\index{tag-reporting rate} and number of planted tags\index{planted tags} are arbitrary and can be varied to see the effect on parameter uncertainty.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code
  sink("TagReturn.txt")
  cat("
  model {
  # Priors
  lam.est ~ dunif(0,1) # Reporting rate
  for (i in 1:Periods) {
     for (j in 1:3) # Rows are return periods, columns are fates
       {
         a[i,j] ~ dgamma(1,1)
       } #j
     S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
     u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest
     v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
     # v.est[i] could also be obtained as 1-S.est[i]-u.est[i]
  } #i

# Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
      } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])   
      # Prob of not being seen again
    } #i
  for (i in 1:Periods) {
  RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], 
                                    NumTagged[i])
  }#i
  PlantReturns ~ dbin(lam.est, PlantedTags) 
    # Additional likelihood component
 }
  ",fill=TRUE)
  sink()

# Bundle data
  jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags",
                    "PlantReturns")

# Initial values
  a.init <- array(data=NA, dim=c(Periods, 3))
  for (i in 1:3){
    a.init[i,] <- runif(n=3, min=0, max=1)
    a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
  }
  jags.inits <- function(){ list(a=a.init)}

  model.file <- 'TagReturn.txt'

  # Parameters monitored
  jags.params <- c("lam.est", "u.est", "v.est")

   # Call JAGS from R
  jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                  n.chains = 3, n.thin = 1, n.iter = 10000, 
                  n.burnin=5000, model.file)
  print(jagsfit)
  plot(jagsfit)
```

The auxiliary planted tag experiment requires only a few changes in the JAGS code. We provide an uninformative prior distribution\index{uninformative prior distribution} for the now estimated parameter lam.est, and pass in the number of planted tags and returns. We let JAGS generate an initial value for lam.est. The additional likelihood\index{likelihood} component describes the binomial\index{binomial distribution} experiment. Convergence\index{convergence} may be slower for this version of the code, so the number of MCMC\index{Markov Chain Monte Carlo (MCMC)} iterations was increased to 10,000.

```{r BrowniePlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Estimated posterior distributions for exploitation rate in period 2 at tag reporting rates of 0.9 (top) and 0.5 (bottom), using a multi-year tagging study with planted tags. The true exploitation rate for period 2 is shown as a dotted vertical line.', fig.show="hold", fig.height=3, out.width="90%"}

rm(list=ls()) # Clear Environment
par(mar = c(4, 4, 1, 2))
set.seed(12345) # Ensure that book will have a reasonable result

BrownieFit <- function() {
  # Priors
  lam.est ~ dunif(0,1) # Reporting rate
  for (i in 1:Periods) {
    for (j in 1:3) # Rows are return periods, columns are fates
    {
      a[i,j] ~ dgamma(1,1)
    } #j
    S.est[i] <- a[i,1]/sum(a[i,]) # Probability of survival
    u.est[i] <- a[i,2]/sum(a[i,]) # Probability of harvest (exploitation rate)
    v.est[i] <- a[i,3]/sum(a[i,]) # Probability of natural death
    # v.est[3] could also be obtained as 1-S.est[i]-u.est[i]
  } #i
  
  # Cell probabilities
  for (i in 1:Periods) {
    p.est[i,i] <- lam.est * u.est[i]
    for (j in (i+1):Periods) {
      p.est[i,j] <- prod(S.est[i:(j-1)])*lam.est*u.est[j]
    } #j
    p.est[i,Periods+1] <- 1 - sum(p.est[i, i:Periods])   # Prob of not being seen again
  } #i
  for (i in 1:Periods) {
    RandRet[i,i:(Periods+1)] ~ dmulti(p.est[i,i:(Periods+1)], NumTagged[i])
  }#i
  PlantReturns ~ dbin(lam.est, PlantedTags) # Additional likelihood component
}

# lambda=0.9 -------------------------------
  # Parameter values for three-yr experiment
  Periods <- 3  # Equal number of release and return periods assumed
  NTags <- 1000 # Number released each period
  NumTagged <- rep(NTags, Periods) # Release NTags fish each period
  
  u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
  v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
  lam.true <- 0.9  # Reporting rate
  PlantedTags <- 30
  PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
  
  # Calculated value(s), array creation
  S.true <- 1 - u.true - v.true
  TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
  TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again
  
  for (i in 1:Periods){ 
    # Expected values for tags-at-risk, returned/not-seen-again
    TagsAtRisk[i,i] <- NumTagged[i]
    TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
    # Returns in release period
    j <- i+1
    while(j <= Periods) {
      TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
      TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
      j <- j+1
    } #while
    TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) # Tags not seen again
  } #i
  
  RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
  #Random trial
  for (i in 1:Periods) # Create upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                            TagFate[i,i:(Periods+1)]))
  } #i
# Simulation code-------------------------

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags", "PlantReturns")

# Initial values
a.init <- array(data=NA, dim=c(Periods, 3))
for (i in 1:3){
  a.init[i,] <- runif(n=3, min=0, max=1)
  a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
}
jags.inits <- function(){ list(a=a.init)}

# Fit the model
Brownie_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = BrownieFit,
    parameters.to.save = c("lam.est", "u.est", "v.est")
  )

# lambda=0.5 -------------------------------
  # Parameter values for three-yr experiment
  Periods <- 3  # Equal number of release and return periods assumed
  NTags <- 1000 # Number released each period
  NumTagged <- rep(NTags, Periods) # Release NTags fish each period
  
  u.true <- c(0.4, 0.25, 0.3) # Probability of fishing death
  v.true <- c(0.1, 0.15, 0.2) # Probability of natural death
  lam.true <- 0.5  # Reporting rate
  PlantedTags <- 30
  PlantReturns <- rbinom(n=1, size=PlantedTags, prob=lam.true)
  
  # Calculated value(s), array creation
  S.true <- 1 - u.true - v.true
  TagsAtRisk <- array(data=NA, dim=c(Periods, Periods))
  TagFate <- array(data=NA, dim=c(Periods, Periods+1))
  # Extra column for tags not seen again
  
  for (i in 1:Periods){ 
    # Expected values for tags-at-risk, returned/not-seen-again
    TagsAtRisk[i,i] <- NumTagged[i]
    TagFate[i,i] <-  TagsAtRisk[i,i] * lam.true * u.true[i] 
    # Returns in release period
    j <- i+1
    while(j <= Periods) {
      TagsAtRisk[i,j] <- TagsAtRisk[i,(j-1)]*S.true[j-1]
      TagFate[i,j] <- TagsAtRisk[i,j] * lam.true * u.true[j]
      j <- j+1
    } #while
    TagFate[i, Periods+1] <- NTags-sum(TagFate[i,i:Periods]) # Tags not seen again
  } #i
  
  RandRet <- array(data=NA, dim=c(Periods, Periods+1))  
  # Extra column for fish not seen again
  #Random trial
  for (i in 1:Periods) # Create upper diagonal matrix of random tag returns
  {
    RandRet[i,i:(Periods+1)] <- t(rmultinom(1, NumTagged[i], 
                                            TagFate[i,i:(Periods+1)]))
  } #i
# Simulation code-------------------------

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("RandRet", "Periods", "NumTagged", "PlantedTags", "PlantReturns")

# Initial values
a.init <- array(data=NA, dim=c(Periods, 3))
for (i in 1:3){
  a.init[i,] <- runif(n=3, min=0, max=1)
  a.init[i,] <- a.init[i,]/sum(a.init[i,])  # Rescale to sum to 1
}
jags.inits <- function(){ list(a=a.init)}

# Fit the model
Brownie2_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 10000,
    model.file = BrownieFit,
    parameters.to.save = c("lam.est", "u.est", "v.est")
  )

# Posterior distribution, 0.9 reporting rate
x.bounds <- range(Brownie_jags$BUGSoutput$sims.list$u.est[,2],
                  Brownie2_jags$BUGSoutput$sims.list$u.est[,2])
hist(Brownie_jags$BUGSoutput$sims.list$u.est[,2], xlim=x.bounds,
     xlab="Estimated exploitation rate, period 2", main="")
abline(v=u.true[2],  lty=3, lwd=3)

# Posterior distribution, 0.5 reporting rate
hist(Brownie2_jags$BUGSoutput$sims.list$u.est[,2], xlim=x.bounds,
     xlab="Estimated exploitation rate, period 2", main="")
abline(v=u.true[2],  lty=3, lwd=3)
```

Estimates have similar precision to the original example if the reporting rate is high (Figure \@ref(fig:BrowniePlot)). Precision is reduced at lower reporting rates because of the reduced sample size of returned tags and uncertainty about lambda.

## Telemetry-based {#TelemetryFandM}

In Section \@ref(TelemetryBased), we used fixed receiving stations and telemetry\index{telemetry} tags to estimate the survival rate\index{survival rate}. Here we extend that approach to partition total mortality into fishing and natural components by giving telemetered fish an external high-reward tag\index{high-reward tags} [Design C of @hightower.harris_2017]. Live fish "detect themselves" and provide information on survival by passing within range of a receiver. Harvest information comes from the reported high-reward tags\index{high-reward tags}. Thus we have observations on two of the three possible states, with natural mortality\index{natural mortality} being the only unobserved true state. An important practical advantage of this approach is that field surveys are not needed after tagging is done (other than maintaining the receiver array and downloading detection data).

We begin with a detour to introduce dcat, the JAGS categorical distribution\index{categorical distribution}, used to estimate probabilities for observations from different categories.  

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Load necessary library packages)
library(rjags)
library(R2jags)

# JAGS code
sink("dcat_example.txt")
cat("
model {

# Priors
    for (j in 1:3) {
         a[j] ~ dgamma(1,1)
         } #j
     p[1] <- a[1]/sum(a[])
     p[2] <- a[2]/sum(a[])
     p[3] <- a[3]/sum(a[]) # Could also be obtained as 1-p[1]-p[2]

# Likelihood
  for (i in 1:N){
    y[i] ~ dcat(p[])
  } #i
}
    ",fill = TRUE)
sink()

# Bundle data
y <- c(1, 1, 3, 3, 2) # Each observation drawn from category 1, 2, or 3
N <- length(y)
jags.data <- list("N", "y")

model.file <- 'dcat_example.txt'

# Parameters monitored
jags.params <- c("p")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=NULL,
                n.chains = 3, n.thin=1, n.iter = 2000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

Our dcat example has a 'y' vector with five observations in the range of 1 to 3. The categories can be of any sort; for example, age or species code for individual fish. The model parameters of interest are calculated variables representing the probabilities of the three categories. The probabilities sum to 1, so, as in Section \@ref(BrownieFandM), we calculate them using an 'a' vector with an uninformative gamma prior distribution\index{gamma distribution}. (A Dirichlet distribution\index{Dirichlet distribution} can also be used as an uninformative prior distribution\index{uninformative prior distribution} for probabilities that must sum to 1 [@mccarthy_2007].) For the purposes of this section, the 'p' vector could represent probabilities of survival\index{survival rate}, harvest\index{harvest rate}, or natural death\index{natural death rate}. The MCMC\index{Markov Chain Monte Carlo (MCMC)} process adjusts the 'p' estimates to achieve the best possible match between model predictions and the observations ('y' vector). For simplicity, we let JAGS provide the initial values for the 'a' vector. The estimates have wide credible intervals because of the small sample size.

Returning to the telemetry\index{telemetry} example, we begin with simulation code, using arbitrary values for the number of occasions and sample size. Varying the patterns for rates of exploitation and natural death is helpful in judging the model's ability to detect moderate time variation. This is a multi-state model\index{multistate model} [@kéry.schaub_2012], with a matrix (PSI.STATE) describing the various true states and the probabilities for transitioning between states over time. The three rows represent true states alive, harvest, and natural death. A fish that is alive (row 1) can remain alive (probability S.true), be harvested (probability u.true), or become a natural mortality (probability v.true). A harvested fish (row 2) remains in that state with probability 1, as does a natural death (row 3). Note that the third matrix dimension for PSI.STATE is time, as the rates are time-dependent.

The observation matrix (PSI.OBS) has a similar structure. Rows are true states, columns are observed states, and the third dimension is time. There are three possible observed states: detected alive, harvested, and not detected. A live fish (row 1) can either be detected or not, with probabilities p[t] and 1-p[t]. A harvested fish (row 2) is assumed to be always detected (100% tag-reporting rate\index{tag-reporting rate}), and a natural death (row 3) is never detected.

The function for generating the simulated capture-history\index{capture history} matrix (simul.ms) is complex, but key features are as follows. All fish are assumed to be tagged (transmitters and external tags) at time 1. Next we loop over occasions and use a multinomial\index{multinomial distribution} trial to determine the true state transition between time t-1 and t (i.e., a live fish at time t-1 can remain alive, be harvested, or transition to natural death). To better understand the multinomial trial, try a simpler example in the Console. For example, <code>rmultinom(n=1, size=1, prob=c(0.7, 0.25, 0.05))</code> is a single trial (n=1) for one individual (size=1), with three states at the specified probabilities. The function returns a matrix with a 1 in a randomly determined row. The <code>which()</code> function locates the '1' and returns the row index (= fish's state). This example could represent probabilities that a live fish at time t-1 survives (0.7), is harvested (0.25), or is a natural death (0.05). Returning to the code, a second multinomial\index{multinomial distribution} trial based on the fish's true state draws a random observed outcome (e.g., a live fish is either detected or missed).

```{r eval=FALSE}
# Code modified from Kéry and Schaub 2012, section 9.5

rm(list=ls()) # Clear Environment

# Generation of simulated data
# Define probabilities as well as number of occasions, 
# states, observations and released individuals
n.occasions <- 5
n.tagged <- 100  # Simplify from Kéry's version, 
                 # only an initial release, and no spatial states

u.true <- c(0.2, 0.1, 0.3, 0.2)
v.true <- c(0.1, 0.15, 0.15, 0.1)
S.true <- 1-u.true-v.true
p <- c(1, 0.8, 0.8, 0.6, 0.7)
#  Detection probability fixed at 1 for first period, 
# n.occasions-1 searches (e.g start of period 2, 3, ..)

n.states <- 3
n.obs <- 3

# Define matrices with survival, transition and detection probabilities

# 1. State process matrix, dimensions 1=state[t], 2=state[t+1], 3=time
PSI.STATE <- array(NA, dim=c(n.states, n.states, n.occasions-1))
   for (t in 1:(n.occasions-1)){
      PSI.STATE[,,t] <- matrix(c(
      S.true[t], u.true[t], v.true[t],
      0, 1, 0,
      0, 0, 1), nrow = n.states, byrow = TRUE)
      } #t

# 2.Observation process matrix.  1=true state, 2=observed state, 3=time
PSI.OBS <- array(NA, dim=c(n.states, n.obs, n.occasions))
   for (t in 1:n.occasions){
      PSI.OBS[,,t] <- matrix(c(
      p[t], 0, 1-p[t],
      0,  1, 0,  # Caught w/ high-reward tag, assume 100% reporting
      0, 0, 1), nrow = n.states, byrow = TRUE)  
        # Natural deaths never detected
      } #t

# Define function to simulate multistate capture-recapture data
simul.ms <- function(PSI.STATE, PSI.OBS, n.tagged, n.occasions){
   CH <- CH.TRUE <- matrix(NA, ncol = n.occasions, nrow = n.tagged)
   CH[,1] <- CH.TRUE[,1] <- 1 # All releases at t=1
   for (i in 1:n.tagged){
     for (t in 2:n.occasions){
         # Multinomial trials for state transitions
         CH.TRUE[i,t] <- which(rmultinom(n=1, size=1,
                            prob=PSI.STATE[CH.TRUE[i,t-1],,t-1])==1)
         # which vector element=1; i.e., which state gets random draw
         # at time t given true state at t-1. True state determines
         # which row of PSI.STATE provides probabilities

         # Multinomial trials for observation process
         CH[i,t] <- which(rmultinom(1, 1, PSI.OBS[CH.TRUE[i,t],,t])==1)
         # which observation gets the 1 random draw, given 
         # true time t state.
         } #t
      } #i

    return(list(CH=CH, CH.TRUE=CH.TRUE)) 
      # True (CH.TRUE) and observed (CH)
   } # simul.ms
```

The JAGS code is also complex but does generally mirror the simulation code.

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# JAGS code for estimating model parameters
sink("Mort_Tel_HRTag.txt")
cat("
model {

# Priors
for (t in 1:(n.occasions-1)) {
     for (j in 1:3) { # Rows are return periods, columns are fates
         a[t,j] ~ dgamma(1,1)
         } #j
     S.est[t] <- a[t,1]/sum(a[t,]) # Probability of survival
     u.est[t] <- a[t,2]/sum(a[t,]) # Probability of harvest
     v.est[t] <- a[t,3]/sum(a[t,]) # Probability of natural death
     # v.est[t] could also be obtained as 1-S.est[t]-u.est[t]
    } #t

  p[1] ~ dbern(1)
  for (t in 2:n.occasions){
    p[t] ~ dunif(0,1)
    }#t

# Define state-transition and observation matrices
# Probabilities of true state at t+1 given state at time t
    for (t in 1:(n.occasions-1)){
    ps[1,t,1] <- S.est[t]
    ps[1,t,2] <- u.est[t]
    ps[1,t,3] <- v.est[t]
    ps[2,t,1] <- 0
    ps[2,t,2] <- 1
    ps[2,t,3] <- 0
    ps[3,t,1] <- 0
    ps[3,t,2] <- 0
    ps[3,t,3] <- 1
    } #t

# Probabilities of observed states given true state
    for (t in 2:n.occasions){
    po[1,t,1] <- p[t]  # Row 1 true state = alive
    po[1,t,2] <- 0
    po[1,t,3] <- 1-p[t]
    po[2,t,1] <- 0     # Row 2 true state = harvested
    po[2,t,2] <- 1
    po[2,t,3] <- 0
    po[3,t,1] <- 0     # Row 3 true state= natural death
    po[3,t,2] <- 0
    po[3,t,3] <- 1
    } #t

    # Likelihood
    for (i in 1:n.tagged){
    z[i,1] <- y[i,1] # Latent state known at time of tagging (t=1)
    for (t in 2:n.occasions){
    # State process: draw state at time t given state at time t-1
    z[i,t] ~ dcat(ps[z[i,(t-1)], (t-1),])
    # Observation process: obs state given true state at time t
    y[i,t] ~ dcat(po[z[i,t], t,])
    } #t
    } #i
}
    ",fill = TRUE)
sink()

# Generate initial values for z. Modified from Kéry code for
# JAGS inits, age-specific example 9.5.3
z.init <- function(ch) {
  # State 1=Obs 1 (alive) State 2=Obs 2 (fishing death). 
  # Start w/ known states from obs capture-history,
  # replace "3" with possible state(s)
  ch[ch==3] <- -1  # Not observed so temporarily replace w/ neg value
  ch[,1] <- NA  # Initial value not needed for release period
  for (i in 1:nrow(ch)){
    if(max(ch[i,], na.rm=TRUE)<2){
      ch[i, 2:ncol(ch)] <- 1 
      # Not detected dead so initialize as alive (after release period)
      } else {
      m <- min(which(ch[i,]==2))  
        # Period when fishing death first detected
      if(m>2) ch[i, 2:(m-1)] <- 1  
      # Initialize as alive up to period prior to harvest
      ch[i, m:ncol(ch)] <- 2  
        # Initialize as dead after detected fishing death
      } # if/else
} # i
  return(ch)
} # z.init

# Call function to get simulated true (CH.TRUE) and obs (CH) states
sim <- simul.ms(PSI.STATE, PSI.OBS, n.tagged, n.occasions)
y <- sim$CH

# Bundle data
jags.data <- list("n.occasions", "n.tagged", "y")

# Initial values.
jags.inits <- function(){ list(z = z.init(y))}

model.file <- 'Mort_Tel_HRTag.txt'

# Parameters monitored
jags.params <- c("u.est", "v.est", "p")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 5000,
                model.file)
print(jagsfit)
plot(jagsfit)
```

The parameters for rates of survival\index{survival rate}, exploitation\index{exploitation rate} and natural death\index{natural death rate} use the same approach as in the above dcat example. The 'a' matrix is estimated internally using a vague gamma prior distribution\index{gamma distribution}, then the three fractions estimate the probabilities of the three possible true states (survival\index{survival rate}, harvest\index{exploitation rate}, natural death\index{natural death rate}), which sum to 1. The latent true states ('z' matrix) are estimated in the likelihood\index{likelihood} section of the code. The dcat (categorical) distribution\index{categorical distribution} is used to draw the true state at time t given state at time t-1, as well as the observation at each time given the inferred true state.

One difficult part of fitting this model is obtaining initial values for the latent true states. JAGS will refuse to update model parameters if initial values are inconsistent with the [observations](https://www.vogelwarte.ch/en/research/population-biology/book-bpa/#accordion=code-for-running-bpa-using-jags). The function used here <code>z.init()</code> initializes all unknown true states to be "alive" unless a harvest occurs. This works because fish not detected could either be alive or be a natural death, so "alive" is a valid initial value. Individuals that are ultimately harvested are known to be alive up to the period prior to harvest. It can be instructive to compare the matrices containing true and observed states with the initial values. For example, initial values can be saved by entering z.test <- z.init(y) in the Console. Entering sim$CH.TRUE[1:5,] and CH[1:5,] in the Console will display true and observed states for the first five individuals for comparison to z.test.

Results for the chosen simulation settings are moderately reliable. There are many parameters, as the model estimates true latent states as well as the probabilities for survival\index{survival rate}, harvest\index{harvest rate} and natural death\index{natural death rate}. It seems to consistently detect the increase in harvest rate\index{harvest rate} in period 3. Uncertainty increases toward the end of the study, especially for probabilities of detection and natural death. This makes sense, because fish not detected on the final occasion could either be alive or be a natural death.

Perhaps the most obvious simulation settings to vary are the fairly robust sample size of tagged fish (100) and the detection probabilities (0.6 and above). Uncertainty increases markedly as detection probabilities decrease, so these simulations can be very helpful in planning the field study (e.g., number of receiver stations). The number of occasions can also be varied, but it requires commensurate changes in the vectors for u, v, and p.

## Exercises

1. For the exploitation rate study design (Section \@ref(ExpRate)), modify the JAGS code to plot the posterior distribution for the exploitation rate. Include a vertical line showing the underlying true exploitation rate.

2. For the completed tagging study design (Section \@ref(Hearn)), modify the code to use a single F parameter (representing an average over periods).

3. For the multi-year tagging study (Section \@ref(BrownieFandM)), run three replicate simulations of the version using planted tags, with tag releases of 1000 (current code), 100, and 30. How does uncertainty (e.g., credible interval width) vary? What sample size would you recommend if planning this study? Also, save credible intervals for natural death rate for period 3 (v3) for the 1000 fish releases (for Exercise 4 below).

4. Compare results for the natural death rate in period 3 (v3) (from Exercise 3) with a modified design using five release and return periods. Extend the exploitation (u) and natural death (v) vectors by using the values from periods 1-2 for the final two periods. How do credible intervals for v3 change when the study is extended?

5. For the telemetry study (Section \@ref(TelemetryFandM)), compare results (particularly uncertainty) for the natural death rate for period 4 (v4) when the detection probability in period 5 (p5) takes on the following values: 0.2, 0.4, 0.7, 0.9. How might this affect planning for conducting the field work?
