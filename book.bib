@article{allen_1997,
  title = {Effects of {{Variable Recruitment}} on {{Catch-Curve Analysis}} for {{Crappie Populations}}},
  author = {Allen, M. S.},
  year = {1997},
  journal = {North American Journal of Fisheries Management},
  volume = {17},
  number = {1},
  pages = {202--205},
  issn = {1548-8675},
  doi = {10.1577/1548-8675(1997)017<0202:EOVROC>2.3.CO;2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1577/1548-8675%281997%29017%3C0202%3AEOVROC%3E2.3.CO%3B2},
  urldate = {2023-11-05},
  abstract = {Catch-curve analysis is frequently used to estimate total annual mortality (A) of exploited fishes, but the method assumes constant recruitment. Because populations of crappie Pomoxis spp. have exhibited large fluctuations in recruitment, I conducted simulations to assess the amount of variability in recruitment that precludes the use of a catch curve and compared results to recruitment dynamics in six crappie populations. Coefficients of variation (CV = 100.SD/mean) in recruitment to age 1 ranged from 55\% to 84\% among the six crappie populations. Simulations suggested that recruitment variability in these ranges would likely allow estimation of A within {\textpm} 10\%, but the probability of obtaining estimates of A that were {\textpm}5\% or more of the true A would exceed 0.15. 1 suggest that catch curves may be used to approximate A in crappie populations but that managers should consider the effects on management recommendations if A were {\textpm} 10\% of the estimated A.},
  copyright = {{\copyright} 1997 American Fisheries Society},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\U3BPYTBL\\Allen - 1997 - Effects of Variable Recruitment on Catch-Curve Ana.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\2VF9RGPB\\1548-8675(1997)0170202EOVROC2.3.html}
}

@article{auger-méthé.etal_2021,
  title = {A Guide to State--Space Modeling of Ecological Time Series},
  author = {{Auger-M{\'e}th{\'e}}, Marie and Newman, Ken and Cole, Diana and Empacher, Fanny and Gryba, Rowenna and King, Aaron A. and {Leos-Barajas}, Vianey and Mills Flemming, Joanna and Nielsen, Anders and Petris, Giovanni and Thomas, Len},
  year = {2021},
  journal = {Ecological Monographs},
  volume = {91},
  number = {4},
  pages = {e01470},
  issn = {1557-7015},
  doi = {10.1002/ecm.1470},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1470},
  urldate = {2024-11-11},
  abstract = {State--space models (SSMs) are an important modeling framework for analyzing ecological time series. These hierarchical models are commonly used to model population dynamics, animal movement, and capture--recapture data, and are now increasingly being used to model other ecological processes. SSMs are popular because they are flexible and they model the natural variation in ecological processes separately from observation error. Their flexibility allows ecologists to model continuous, count, binary, and categorical data with linear or nonlinear processes that evolve in discrete or continuous time. Modeling the two sources of stochasticity separately allows researchers to differentiate between biological variation and imprecision in the sampling methodology, and generally provides better estimates of the ecological quantities of interest than if only one source of stochasticity is directly modeled. Since the introduction of SSMs, a broad range of fitting procedures have been proposed. However, the variety and complexity of these procedures can limit the ability of ecologists to formulate and fit their own SSMs. We provide the knowledge for ecologists to create SSMs that are robust to common, and often hidden, estimation problems, and the model selection and validation tools that can help them assess how well their models fit their data. We present a review of SSMs that will provide a strong foundation to ecologists interested in learning about SSMs, introduce new tools to veteran SSM users, and highlight promising research directions for statisticians interested in ecological applications. The review is accompanied by an in-depth tutorial that demonstrates how SSMs can be fitted and validated in R. Together, the review and tutorial present an introduction to SSMs that will help ecologists to formulate, fit, and validate their models.},
  copyright = {{\copyright} 2021 The Authors. Ecological Monographs published by Wiley Periodicals LLC on behalf of Ecological Society of America},
  langid = {english},
  keywords = {Bayesian,diagnostic,fitting procedure,frequentist,model selection,state-space model,time series},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\HZYZMW82\\Auger-Méthé et al. - 2021 - A guide to state–space modeling of ecological time.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\QHXZZGNU\\ecm.html}
}

@article{banner.etal_2020,
  title = {The Use of {{Bayesian}} Priors in {{Ecology}}: {{The}} Good, the Bad and the Not Great},
  shorttitle = {The Use of {{Bayesian}} Priors in {{Ecology}}},
  author = {Banner, Katharine M. and Irvine, Kathryn M. and Rodhouse, Thomas J.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {8},
  pages = {882--889},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13407},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13407},
  urldate = {2024-10-28},
  abstract = {Bayesian data analysis (BDA) is a powerful tool for making inference from ecological data, but its full potential has yet to be realized. Despite a generally positive trajectory in research surrounding model development and assessment, far too little attention has been given to prior specification. Default priors, a sub-class of non-informative prior distributions that are often chosen without critical thought or evaluation, are commonly used in practice. We believe the fear of being too `subjective' has prevented many researchers from using any prior information in their analyses despite the fact that defending prior choice (informative or not) promotes good statistical practice. In this commentary, we provide an overview of how BDA is currently being used in a random sample of articles, discuss implications for inference if current bad practices continue, and highlight sub-fields where knowledge about the system has improved inference and promoted good statistical practices through the careful and justified use of informative priors. We hope to inspire a renewed discussion about the use of Bayesian priors in Ecology with particular attention paid to specification and justification. We also emphasize that all priors are the result of a subjective choice, and should be discussed in that way.},
  langid = {english},
  keywords = {Bayesian hierarchical models,good statistical practice,sensitivity analysis,subjective priors},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\P45HCST7\\Banner et al. - 2020 - The use of Bayesian priors in Ecology The good, t.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\UL679B3T\\2041-210X.html}
}

@book{bolker2008,
  title = {Ecological {{Models}} and {{Data}} in {{R}}},
  author = {Bolker, Benjamin M.},
  year = {2008},
  edition = {1st},
  publisher = {Princeton University Press},
  address = {Princeton, New Jersey}
}

@article{brooks.gelman1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  year = {1998},
  month = dec,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  number = {4},
  pages = {434--455},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
  urldate = {2022-01-31},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  keywords = {Convergence diagnosis,Inference,Markov chain Monte Carlo},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\ZK3P5J8X\\Brooks and Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\ZLRS57HH\\10618600.1998.html}
}

@techreport{brownie.etal1985,
  type = {Federal {{Government Series}}},
  title = {Statistical Inference from Band Recovery Data: A Handbook},
  shorttitle = {Statistical Inference from Band Recovery Data},
  author = {Brownie, Cavell and Anderson, D.R. and Burnham, K.P. and Robson, D.S.},
  year = {1985},
  journal = {Statistical inference from band recovery data: a handbook},
  series = {Resource {{Publication}}},
  number = {156},
  address = {Washington, D.C.},
  institution = {{U.S. Fish and Wildlife}},
  url = {http://pubs.er.usgs.gov/publication/rp156},
  urldate = {2022-04-11},
  abstract = {No abstract available.}
}

@article{bryant2000,
  title = {Estimating {{Fish Populations}} by {{Removal Methods}} with {{Minnow Traps}} in {{Southeast Alaska Streams}}},
  author = {Bryant, Mason D.},
  year = {2000},
  month = nov,
  journal = {North American Journal of Fisheries Management},
  volume = {20},
  number = {4},
  pages = {923--930},
  issn = {0275-5947, 1548-8675},
  doi = {10.1577/1548-8675(2000)020<0923:EFPBRM>2.0.CO;2},
  url = {http://doi.wiley.com/10.1577/1548-8675(2000)020<0923:EFPBRM>2.0.CO;2},
  urldate = {2022-02-10},
  abstract = {Passive capture methods, such as minnow traps, are commonly used to capture fish for mark-recapture population estimates; however, they have not been used for removal methods. Minnow traps set for 90-min periods during three or four sequential capture occasions during the summer of 1996 were used to capture coho salmon Oncorhynchus kisutch fry and parr, Dolly Varden Salvelinus malma, cutthroat trout O. clarki, and juvenile steelhead O. mykiss to estimate population size with the Zippin or generalized removal method. More than 45\% of the total catch was obtained during the first capture occasion, and in most cases, the catch during the fourth occasion was less than 15\% of the total catch. In most pools, the probability of capture was greater than 0.4 but was lower for coho salmon fry than for coho salmon parr and other species. Mean population estimates for coho salmon parr made with concurrent mark-recapture and removal methods differed significantly in small streams. Estimates from mark-recapture and removal methods were not significantly different for coho salmon fry and Dolly Varden, but mark-recapture estimates were higher than removal estimates in most cases. My results show that removal estimates can be obtained with minnow traps if sampling procedures conform to the assumptions required for the method.},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\EL5DAMDI\Bryant - 2000 - Estimating Fish Populations by Removal Methods wit.pdf}
}

@book{burnham.anderson_2004,
  title = {Model {{Selection}} and {{Multimodel Inference}}},
  editor = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/b97636},
  url = {http://link.springer.com/10.1007/b97636},
  urldate = {2023-12-06},
  isbn = {978-0-387-95364-9},
  langid = {english},
  keywords = {data analysis,Estimator,Inference,information theory,Likelihood,Model Selection},
  file = {C:\Users\jhncsu\Zotero\storage\PZSJUIRD\Burnham and Anderson - 2004 - Model Selection and Multimodel Inference.pdf}
}

@article{campana2001,
  title = {Accuracy, Precision and Quality Control in Age Determination, Including a Review of the Use and Abuse of Age Validation Methods},
  author = {Campana, S. E.},
  year = {2001},
  journal = {Journal of Fish Biology},
  volume = {59},
  number = {2},
  pages = {197--242},
  issn = {1095-8649},
  doi = {10.1111/j.1095-8649.2001.tb00127.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1095-8649.2001.tb00127.x},
  urldate = {2023-02-23},
  abstract = {Many calcified structures produce periodic growth increments useful for age determination at the annual or daily scale. However, age determination is invariably accompanied by various sources of error, some of which can have a serious effect on age-structured calculations. This review highlights the best available methods for insuring ageing accuracy and quantifying ageing precision, whether in support of large-scale production ageing or a small-scale research project. Included in this review is a critical overview of methods used to initiate and pursue an accurate and controlled ageing program, including (but not limited to) validation of an ageing method. The distinction between validation of absolute age and increment periodicity is emphasized, as is the importance of determining the age of first increment formation. Based on an analysis of 372 papers reporting age validation since 1983, considerable progress has been made in age validation efforts in recent years. Nevertheless, several of the age validation methods which have been used routinely are of dubious value, particularly marginal increment analysis. The two major measures of precision, average percent error and coefficient of variation, are shown to be functionally equivalent, and a conversion factor relating the two is presented. Through use of quality control monitoring, ageing errors are readily detected and quantified; reference collections are the key to both quality control and reduction of costs. Although some level of random ageing error is unavoidable, such error can often be corrected after the fact using statistical (`digital sharpening)' methods.},
  langid = {english},
  keywords = {accuracy,age determination,otolith,precision,quality,validation},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\P5XG854R\\Campana - 2001 - Accuracy, precision and quality control in age det.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\7ZILP8N8\\j.1095-8649.2001.tb00127.html}
}

@article{conn.etal_2008,
  title = {Bayesian {{Analysis}} of {{Wildlife Age-at-Harvest Data}}},
  author = {Conn, Paul B. and Diefenbach, Duane R. and Laake, Jeffrey L. and Ternent, Mark A. and White, Gary C.},
  year = {2008},
  journal = {Biometrics},
  volume = {64},
  number = {4},
  pages = {1170--1177},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2008.00987.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2008.00987.x},
  urldate = {2024-02-06},
  abstract = {State and federal natural resource management agencies often collect age-structured harvest data. These data represent finite realizations of stochastic demographic and sampling processes and have long been used by biologists to infer population trends. However, different sources of data have been combined in ad hoc ways and these methods usually failed to incorporate sampling error. In this article, we propose a ``hidden process'' (or state-space) model for estimating abundance, survival, recovery rate, and recruitment from age-at-harvest data that incorporate both demographic and sampling stochasticity. To this end, a likelihood for age-at-harvest data is developed by embedding a population dynamics model within a model for the sampling process. Under this framework, the identification of abundance parameters can be achieved by conducting a joint analysis with an auxiliary data set. We illustrate this approach by conducting a Bayesian analysis of age-at-harvest and mark-recovery data from black bears (Ursus americanus) in Pennsylvania. Using a set of reasonable prior distributions, we demonstrate a substantial increase in precision when posterior summaries of abundance are compared to a bias-corrected Lincoln--Petersen estimator. Because demographic processes link consecutive abundance estimates, we also obtain a more realistic biological picture of annual changes in abundance. Because age-at-harvest data are often readily obtained, we argue that this type of analysis provides a valuable strategy for wildlife population monitoring.},
  langid = {english},
  keywords = {Abundance,Age-at-harvest,Black bear,Cohort model,Mark-recovery model,Recruitment,State-space model,Survival},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\W8GK2BTP\\Conn et al. - 2008 - Bayesian Analysis of Wildlife Age-at-Harvest Data.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\I3R5E49F\\j.1541-0420.2008.00987.html}
}

@article{conn.etal_2018,
  title = {A Guide to {{Bayesian}} Model Checking for Ecologists},
  author = {Conn, Paul B. and Johnson, Devin S. and Williams, Perry J. and Melin, Sharon R. and Hooten, Mevin B.},
  year = {2018},
  journal = {Ecological Monographs},
  volume = {88},
  number = {4},
  pages = {526--542},
  issn = {1557-7015},
  doi = {10.1002/ecm.1314},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1314},
  urldate = {2023-12-15},
  abstract = {Checking that models adequately represent data is an essential component of applied statistical inference. Ecologists increasingly use hierarchical Bayesian statistical models in their research. The appeal of this modeling paradigm is undeniable, as researchers can build and fit models that embody complex ecological processes while simultaneously accounting for observation error. However, ecologists tend to be less focused on checking model assumptions and assessing potential lack of fit when applying Bayesian methods than when applying more traditional modes of inference such as maximum likelihood. There are also multiple ways of assessing the fit of Bayesian models, each of which has strengths and weaknesses. For instance, Bayesian P values are relatively easy to compute, but are well known to be conservative, producing P values biased toward 0.5. Alternatively, lesser known approaches to model checking, such as prior predictive checks, cross-validation probability integral transforms, and pivot discrepancy measures may produce more accurate characterizations of goodness-of-fit but are not as well known to ecologists. In addition, a suite of visual and targeted diagnostics can be used to examine violations of different model assumptions and lack of fit at different levels of the modeling hierarchy, and to check for residual temporal or spatial autocorrelation. In this review, we synthesize existing literature to guide ecologists through the many available options for Bayesian model checking. We illustrate methods and procedures with several ecological case studies including (1) analysis of simulated spatiotemporal count data, (2) N-mixture models for estimating abundance of sea otters from an aircraft, and (3) hidden Markov modeling to describe attendance patterns of California sea lion mothers on a rookery. We find that commonly used procedures based on posterior predictive P values detect extreme model inadequacy, but often do not detect more subtle cases of lack of fit. Tests based on cross-validation and pivot discrepancy measures (including the ``sampled predictive P value'') appear to be better suited to model checking and to have better overall statistical performance. We conclude that model checking is necessary to ensure that scientific inference is well founded. As an essential component of scientific discovery, it should accompany most Bayesian analyses presented in the literature.},
  copyright = {Published 2018. This article is a U.S. Government work and is in the public domain in the USA.},
  langid = {english},
  keywords = {Bayesian model checking,Bayesian P value,goodness-of-fit,hierarchical model,model diagnostics,posterior checks},
  file = {C:\Users\jhncsu\Zotero\storage\7PZV2J6X\ecm.html}
}

@article{deriso.etal_1985,
  title = {Catch-{{Age Analysis}} with {{Auxiliary Information}}},
  author = {Deriso, R. B. and Quinn II, T. J. and Neal, P. R.},
  year = {1985},
  month = apr,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {42},
  number = {4},
  pages = {815--824},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f85-104},
  url = {https://cdnsciencepub.com/doi/abs/10.1139/f85-104},
  urldate = {2024-09-10},
  abstract = {We examined the use of catch-at-age data for estimating population abundance, productivity, and year-class abundance. A review section is included where various published models and our new models are shown to form a cohesive theory of catch-at-age analysis linked by level of model complexity. We developed three new models with different error structures: a log-normal measurement error model, a multinomial measurement error model, and a log-normal process error model. By application to data on Pacific halibut (Hippoglossus stenolepis), we show that moderate amounts of auxiliary information, such as fishing effort data or the assumption of a spawner--recruit relationship, are needed to stabilize estimates. The models performed very similarly with moderate amounts of auxiliary information, suggesting a degree of robustness to the underlying error structure. We also developed an extension to classic catch-curve analysis that estimates relative year-class strength reasonably well.},
  file = {C:\Users\jhncsu\Zotero\storage\AJGQCR68\Deriso et al. - 1985 - Catch-Age Analysis with Auxiliary Information.pdf}
}

@article{devalpine.etal_2017,
  title = {Programming {{With Models}}: {{Writing Statistical Algorithms}} for {{General Model Structures With NIMBLE}}},
  shorttitle = {Programming {{With Models}}},
  author = {{de Valpine}, Perry and Turek, Daniel and Paciorek, Christopher J. and {Anderson-Bergman}, Clifford and Lang, Duncan Temple and Bodik, Rastislav},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {403--413},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2016.1172487},
  url = {https://doi.org/10.1080/10618600.2016.1172487},
  urldate = {2024-04-08},
  abstract = {We describe NIMBLE, a system for programming statistical algorithms for general model structures within R. NIMBLE is designed to meet three challenges: flexible model specification, a language for programming algorithms that can use different models, and a balance between high-level programmability and execution efficiency. For model specification, NIMBLE extends the BUGS language and creates model objects, which can manipulate variables, calculate log probability values, generate simulations, and query the relationships among variables. For algorithm programming, NIMBLE provides functions that operate with model objects using two stages of evaluation. The first stage allows specialization of a function to a particular model and/or nodes, such as creating a Metropolis-Hastings sampler for a particular block of nodes. The second stage allows repeated execution of computations using the results of the first stage. To achieve efficient second-stage computation, NIMBLE compiles models and functions via C++, using the Eigen library for linear algebra, and provides the user with an interface to compiled objects. The NIMBLE language represents a compilable domain-specific language (DSL) embedded within R. This article provides an overview of the design and rationale for NIMBLE along with illustrative examples including importance sampling, Markov chain Monte Carlo (MCMC) and Monte Carlo expectation maximization (MCEM). Supplementary materials for this article are available online.},
  keywords = {Domain-specific language,Hierarchical models,MCEM,MCMC,Probabilistic programming,R},
  file = {C:\Users\jhncsu\Zotero\storage\D75HKLPA\de Valpine et al. - 2017 - Programming With Models Writing Statistical Algor.pdf}
}

@article{doll.jacquemin_2018,
  title = {Introduction to {{Bayesian Modeling}} and {{Inference}} for {{Fisheries Scientists}}},
  author = {Doll, Jason C. and Jacquemin, Stephen J.},
  year = {2018},
  journal = {Fisheries},
  volume = {43},
  number = {3},
  pages = {152--161},
  issn = {1548-8446},
  doi = {10.1002/fsh.10038},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fsh.10038},
  urldate = {2024-02-15},
  abstract = {Bayesian inference is everywhere, from one of the most recent journal articles in Transactions of the American Fisheries Society to the decision-making process you undergo when selecting a new fishing spot. Bayesian inference is the only statistical paradigm that synthesizes prior knowledge with newly collected data to facilitate a more informed decision---and it is being used at an increasing rate in almost every area of our profession. Thus, the goal of this article is to provide fisheries managers, educators, and students with a conceptual introduction to Bayesian inference. We do not assume that the reader is familiar with Bayesian inference; however, we do assume that the reader has completed an introductory biostatistics course. To this end, we review the conceptual foundation of Bayesian inference without the use of complex equations; present one example of using Bayesian inference to compare relative weight between two time periods; present one example of using prior information about von Bertalanffy growth parameters to improve parameter estimation; and, finally, suggest literature that can help to develop the skills needed to use Bayesian inference in your own management or research program.},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\H8JJQM2Q\\Doll and Jacquemin - 2018 - Introduction to Bayesian Modeling and Inference fo.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\SP7SBPER\\fsh.html}
}

@article{doll.jacquemin_2019,
  title = {Bayesian {{Model Selection}} in {{Fisheries Management}} and {{Ecology}}},
  author = {Doll, Jason C. and Jacquemin, Stephen J.},
  year = {2019},
  month = sep,
  journal = {Journal of Fish and Wildlife Management},
  volume = {10},
  number = {2},
  pages = {691--707},
  issn = {1944-687X},
  doi = {10.3996/042019-JFWM-024},
  url = {https://doi.org/10.3996/042019-JFWM-024},
  urldate = {2024-11-05},
  abstract = {Researchers often test ecological hypotheses relating to a myriad of questions ranging from assemblage structure, population dynamics, demography, abundance, growth rate, and more using mathematical models that explain trends in data. To aid in the evaluation process when faced with competing hypotheses, we employ statistical methods to evaluate the validity of these multiple hypotheses with the goal of deriving the most robust conclusions possible. In fisheries management and ecology, frequentist methodologies have largely dominated this approach. However, in recent years, researchers have increasingly used Bayesian inference methods to estimate model parameters. Our aim with this perspective is to provide the practicing fisheries ecologist with an accessible introduction to Bayesian model selection. Here we discuss Bayesian inference methods for model selection in the context of fisheries management and ecology with empirical examples to guide researchers in the use of these methods. In this perspective we discuss three methods for selecting among competing models. For comparing two models we discuss Bayes factor and for more complex models we discuss Watanabe--Akaike information criterion and leave-one-out cross-validation. We also describe what kinds of information to report when conducting Bayesian inference. We conclude this review with a discussion of final thoughts about these model selection techniques.},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\PR69SFIC\\Doll and Jacquemin - 2019 - Bayesian Model Selection in Fisheries Management a.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\7QPAUJ3E\\Bayesian-Model-Selection-in-Fisheries-Management.html}
}

@article{dorazio_2016,
  title = {Bayesian Data Analysis in Population Ecology: Motivations, Methods, and Benefits},
  shorttitle = {Bayesian Data Analysis in Population Ecology},
  author = {Dorazio, Robert M.},
  year = {2016},
  journal = {Population Ecology},
  volume = {58},
  number = {1},
  pages = {31--44},
  issn = {1438-390X},
  doi = {10.1007/s10144-015-0503-4},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1007/s10144-015-0503-4},
  urldate = {2023-10-15},
  abstract = {During the 20th century ecologists largely relied on the frequentist system of inference for the analysis of their data. However, in the past few decades ecologists have become increasingly interested in the use of Bayesian methods of data analysis. In this article I provide guidance to ecologists who would like to decide whether Bayesian methods can be used to improve their conclusions and predictions. I begin by providing a concise summary of Bayesian methods of analysis, including a comparison of differences between Bayesian and frequentist approaches to inference when using hierarchical models. Next I provide a list of problems where Bayesian methods of analysis may arguably be preferred over frequentist methods. These problems are usually encountered in analyses based on hierarchical models of data. I describe the essentials required for applying modern methods of Bayesian computation, and I use real-world examples to illustrate these methods. I conclude by summarizing what I perceive to be the main strengths and weaknesses of using Bayesian methods to solve ecological inference problems.},
  copyright = {{\copyright} 2016 The Society of Population Ecology},
  langid = {english},
  keywords = {Frequentist inference,Hierarchical modeling,Missing data,Occupancy model,Spatial analysis,State-space modeling},
  file = {C:\Users\jhncsu\Zotero\storage\KWESD48P\s10144-015-0503-4.html}
}

@book{efron.tibshirani_1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1993},
  month = jan,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Fla.},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  isbn = {978-0-412-04231-7},
  langid = {english}
}

@article{ellison_2004,
  title = {Bayesian Inference in Ecology},
  author = {Ellison, Aaron M.},
  year = {2004},
  journal = {Ecology Letters},
  volume = {7},
  number = {6},
  pages = {509--520},
  issn = {1461-0248},
  doi = {10.1111/j.1461-0248.2004.00603.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1461-0248.2004.00603.x},
  urldate = {2023-10-17},
  abstract = {Bayesian inference is an important statistical tool that is increasingly being used by ecologists. In a Bayesian analysis, information available before a study is conducted is summarized in a quantitative model or hypothesis: the prior probability distribution. Bayes' Theorem uses the prior probability distribution and the likelihood of the data to generate a posterior probability distribution. Posterior probability distributions are an epistemological alternative to P-values and provide a direct measure of the degree of belief that can be placed on models, hypotheses, or parameter estimates. Moreover, Bayesian information-theoretic methods provide robust measures of the probability of alternative models, and multiple models can be averaged into a single model that reflects uncertainty in model construction and selection. These methods are demonstrated through a simple worked example. Ecologists are using Bayesian inference in studies that range from predicting single-species population dynamics to understanding ecosystem processes. Not all ecologists, however, appreciate the philosophical underpinnings of Bayesian inference. In particular, Bayesians and frequentists differ in their definition of probability and in their treatment of model parameters as random variables or estimates of true values. These assumptions must be addressed explicitly before deciding whether or not to use Bayesian methods to analyse ecological data.},
  langid = {english},
  keywords = {Bayes' Theorem,Bayesian inference,epistemology,information criteria,model averaging,model selection},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\AJCLBLXD\\Ellison - 2004 - Bayesian inference in ecology.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\KGJ38KVZ\\j.1461-0248.2004.00603.html}
}

@article{fielder.bence_2014,
  title = {Integration of {{Auxiliary Information}} in {{Statistical Catch-at-Age}} ({{SCA}}) {{Analysis}} of the {{Saginaw Bay Stock}} of {{Walleye}} in {{Lake Huron}}},
  author = {Fielder, David G. and Bence, James R.},
  year = {2014},
  journal = {NORTH AMERICAN JOURNAL OF FISHERIES MANAGEMENT},
  volume = {34},
  number = {5},
  pages = {970--987},
  publisher = {Wiley},
  address = {Hoboken},
  issn = {0275-5947, 1548-8675},
  doi = {10.1080/02755947.2014.938141},
  url = {https://afspubs.onlinelibrary.wiley.com/doi/10.1080/02755947.2014.938141},
  urldate = {2024-09-09},
  abstract = {Estimates of mortality rates and abundance for the Saginaw Bay stock of Walleye Sander vitreus has traditionally been performed with an analysis of tag returns using a Brownie-style analysis. An estimation approach that more fully accounted for sources of exploitation in addition to the recreational fishery in Saginaw Bay and inclusive of the rest of Lake Huron was needed. We developed a statistical catch-at-age model to accomplish this and evaluated four versions including three different treatments of natural mortality (M): a constant value, age-based M values, and time-varying M values. Deviance information criterion model selection procedures indicated that an age-based M model version was optimal. We also evaluated an integrated version that incorporated tag returns as auxiliary information for the recreational component. In this case, model selection was based on conformity between observed and predicted data and model convergence. The integrated version was ruled out due to poor agreement of the observed and predicted values and predictions of abundance that were not reflected by the fisheries. We concluded that the component of the population used for tagging may exhibit dynamics that differ from the rest of the stock. Total annual mortality of Walleyes was greatest for older ages in all fisheries and ranged from 32\% for age-2 fish to 39\% for fish of ages 10 and older. The recreational fishery accounted for the majority of fishing mortality, but the commercial trap-net fishery in the main basin of Lake Huron and bykill from other trap nets in the bay accounted for proportionally greater fishing mortality of younger ages of fish. Abundance peaked in 2007 at 4 million Walleyes age2 and older, but estimates indicated a previous period of high abundance in the late 1980s, forcing the reconsideration of the past stock as depressed and dependent on stocking.},
  langid = {english},
  keywords = {CONTRADICTORY DATA SOURCES,ERIE,GREAT-LAKES,MODEL,NATURAL MORTALITY,PERFORMANCE,RATES,ST-CLAIR,STIZOSTEDION-VITREUM-VITREUM,TIME-VARYING CATCHABILITY},
  annotation = {Web of Science ID: WOS:000344807700010}
}

@article{flowers.hightower_2013,
  type = {Journal {{Article}}},
  title = {A Novel Approach to Surveying Sturgeon Using Side-Scan Sonar and Occupancy Modeling},
  author = {Flowers, H. Jared and Hightower, Joseph E.},
  year = {2013},
  journal = {Marine and Coastal Fisheries: Dynamics, Management, and Ecosystem Science},
  volume = {5},
  number = {1},
  pages = {211--223},
  doi = {10.1080/19425120.2013.816396},
  url = {http://pubs.er.usgs.gov/publication/70148138},
  urldate = {2022-02-18},
  abstract = {Technological advances represent opportunities to enhance and supplement traditional fisheries sampling approaches. One example with growing importance for fisheries research is hydroacoustic technologies such as side-scan sonar. Advantages of side-scan sonar over traditional techniques include the ability to sample large areas efficiently and the potential to survey fish without physical handling-important for species of conservation concern, such as endangered sturgeons. Our objectives were to design an efficient survey methodology for sampling Atlantic Sturgeon~Acipenser oxyrinchus~by using side-scan sonar and to developmethods for analyzing these data. In North Carolina and South Carolina, we surveyed six rivers thought to contain varying abundances of sturgeon by using a combination of side-scan sonar, telemetry, and video cameras (i.e., to sample jumping sturgeon). Lower reaches of each river near the saltwater-freshwater interface were surveyed on three occasions (generally successive days), and we used occupancy modeling to analyze these data.We were able to detect sturgeon in five of six rivers by using these methods. Side-scan sonar was effective in detecting sturgeon, with estimated gear-specific detection probabilities ranging from 0.2 to 0.5 and river-specific occupancy estimates (per 2-km river segment) ranging from 0.0 to 0.8. Future extensions of this occupancy modeling framework will involve the use of side-scan sonar data to assess sturgeon habitat and abundance in different river systems.},
  annotation = {IP-043165},
  file = {C:\Users\jhncsu\Zotero\storage\5CFUKVYB\Flowers and Hightower - 2013 - A novel approach to surveying sturgeon using side-.pdf}
}

@article{flowers.hightower2015,
  title = {Estimating {{Sturgeon Abundance}} in the {{Carolinas Using Side-Scan Sonar}}},
  author = {Flowers, H. Jared and Hightower, Joseph E.},
  year = {2015},
  journal = {Marine and Coastal Fisheries},
  volume = {7},
  number = {1},
  pages = {1--9},
  issn = {1942-5120},
  doi = {10.1080/19425120.2014.982334},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/19425120.2014.982334},
  urldate = {2022-02-18},
  abstract = {Sturgeons (Acipenseridae) are one of the most threatened taxa worldwide, including species in North Carolina and South Carolina. Populations of Atlantic Sturgeon Acipenser oxyrinchus in the Carolinas have been significantly reduced from historical levels by a combination of intense fishing and habitat loss. There is a need for estimates of current abundance, to describe status, and for estimates of historical abundance in order to provide realistic recovery goals. In this study we used N-mixture and distance models with data acquired from side-scan sonar surveys to estimate abundance of sturgeon in six major sturgeon rivers in North Carolina and South Carolina. Estimated abundances of sturgeon greater than 1 m TL in the Carolina distinct population segment (DPS) were 2,031 using the count model and 1,912 via the distance model. The Pee Dee River had the highest overall abundance of any river at 1,944 (count model) or 1,823 (distance model). These estimates do not account for sturgeon less than 1 m TL or occurring in riverine reaches not surveyed or in marine waters. Comparing the two models, the N-mixture model produced similar estimates using less data than the distance model with only a slight reduction of estimated precision. Received May 3, 2014; accepted October 14, 2014},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\P8F7BAJF\\Flowers and Hightower - 2015 - Estimating Sturgeon Abundance in the Carolinas Usi.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\PR8L5IIC\\19425120.2014.html}
}

@article{fournier.etal_2012,
  title = {{{AD Model Builder}}: Using Automatic Differentiation for Statistical Inference of Highly Parameterized Complex Nonlinear Models},
  shorttitle = {{{AD Model Builder}}},
  author = {Fournier, David A. and Skaug, Hans J. and Ancheta, Johnoel and Ianelli, James and Magnusson, Arni and Maunder, Mark N. and Nielsen, Anders and Sibert, John},
  year = {2012},
  month = apr,
  journal = {Optimization Methods and Software},
  volume = {27},
  number = {2},
  pages = {233--249},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2011.597854},
  url = {https://doi.org/10.1080/10556788.2011.597854},
  urldate = {2024-09-11},
  abstract = {Many criteria for statistical parameter estimation, such as maximum likelihood, are formulated as a nonlinear optimization problem. Automatic Differentiation Model Builder (ADMB) is a programming framework based on automatic differentiation, aimed at highly nonlinear models with a large number of parameters. The benefits of using AD are computational efficiency and high numerical accuracy, both crucial in many practical problems. We describe the basic components and the underlying philosophy of ADMB, with an emphasis on functionality found in no other statistical software. One example of such a feature is the generic implementation of Laplace approximation of high-dimensional integrals for use in latent variable models. We also review the literature in which ADMB has been used, and discuss future development of ADMB as an open source project. Overall, the main advantages of ADMB are flexibility, speed, precision, stability and built-in methods to quantify uncertainty.},
  keywords = {ADMB,automatic differentiation,Laplace approximation,optimization,parameter estimation,separability},
  file = {C:\Users\jhncsu\Zotero\storage\F9ADU2Z3\Fournier et al. - 2012 - AD Model Builder using automatic differentiation .pdf}
}

@book{gelman.etal_2013,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2013},
  month = nov,
  edition = {3rd edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton London New York},
  abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian AnalysisNow in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors{\texthorizontalbar}all leaders in the statistics community{\texthorizontalbar}introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice.New to the Third EditionFour new chapters on nonparametric modelingCoverage of weakly informative priors and boundary-avoiding priorsUpdated discussion of cross-validation and predictive information criteriaImproved convergence monitoring and effective sample size calculations for iterative simulationPresentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagationNew and revised software codeThe book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book's web page.},
  isbn = {978-1-4398-4095-5},
  langid = {english}
}

@article{gelman.etal_2015,
  title = {Stan: {{A Probabilistic Programming Language}} for {{Bayesian Inference}} and {{Optimization}}},
  shorttitle = {Stan},
  author = {Gelman, Andrew and Lee, Daniel and Guo, Jiqiang},
  year = {2015},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {40},
  number = {5},
  eprint = {43966398},
  eprinttype = {jstor},
  pages = {530--543},
  publisher = {[American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
  issn = {1076-9986},
  url = {https://www.jstor.org/stable/43966398},
  urldate = {2024-03-26},
  abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users 'and developers' perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
  file = {C:\Users\jhncsu\Zotero\storage\EJ3M2MQR\Gelman et al. - 2015 - Stan A Probabilistic Programming Language for Bay.pdf}
}

@misc{gelman.hill_2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  month = dec,
  journal = {Higher Education from Cambridge University Press},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511790942},
  url = {https://www.cambridge.org/highereducation/books/data-analysis-using-regression-and-multilevel-hierarchical-models/32A29531C7FD730C3A68951A17C9D983},
  urldate = {2024-04-25},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  isbn = {9780511790942},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\USMHKYB9\32A29531C7FD730C3A68951A17C9D983.html}
}

@article{gelman.rubin1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  year = {1992},
  month = jan,
  journal = {Statistical Science},
  volume = {7},
  pages = {457--472},
  doi = {10.1214/ss/1177011136},
  url = {https://ui.adsabs.harvard.edu/abs/1992StaSc...7..457G},
  urldate = {2022-01-31},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  annotation = {ADS Bibcode: 1992StaSc...7..457G},
  file = {C:\Users\jhncsu\Zotero\storage\J7U32DIL\Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf}
}

@article{hall_1986,
  title = {Electrofishing {{Catch}} per {{Hour}} as an {{Indicator}} of {{Largemouth Bass Density}} in {{Ohio Impoundments}}},
  author = {Hall, Thomas J.},
  year = {1986},
  month = jul,
  journal = {North American Journal of Fisheries Management},
  volume = {6},
  number = {3},
  pages = {397--400},
  publisher = {AFS Website},
  issn = {0275-5947},
  doi = {10.1577/1548-8659(1986)6<397:ECPHAA>2.0.CO;2},
  url = {https://doi.org/10.1577/1548-8659(1986)6%3C397:ECPHAA%3E2.0.CO;2},
  urldate = {2024-11-15},
  abstract = {Catch data from 12 Ohio impoundments indicated a significant positive linear relationship between electrofishing catch per hour (CPH) of largemouth bass (Micropterus salmoides) over 199 mm long and number per hectare in a lake. The regression equation that best described this relationship was log 10(number per hectare) = 1.2274 log 10(CPH) - 0.5489. Important considerations for collection of reliable CPH data are time of year, water temperature, and electrofishing effort.},
  file = {C:\Users\jhncsu\Zotero\storage\YSCETZJ8\Hall - 1986 - Electrofishing Catch per Hour as an Indicator of L.pdf}
}

@article{hayes.brodziack1997,
  title = {Reply: {{Efficiency}} and Bias of Estimators and Sampling Designs for Determining Lengthweight Relationships of Fish},
  shorttitle = {Reply},
  author = {Hayes, D B and Brodziack, {\relax JKT}},
  year = {1997},
  month = mar,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {54},
  number = {3},
  pages = {744--745},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f96-299},
  url = {https://cdnsciencepub.com/doi/abs/10.1139/f96-299},
  urldate = {2023-02-20},
  file = {C:\Users\jhncsu\Zotero\storage\9PJGRSRC\Hayes and Brodziack - 1997 - Reply Efficiency and bias of estimators and sampl.pdf}
}

@article{hayes.etal1995,
  title = {Efficiency and Bias of Estimators and Sampling Designs for Determining Length-Weight Relationships of Fish},
  author = {Hayes, Daniel B. and Brodziak, Jon K. T. and O'Gorman, Joseph B.},
  year = {1995},
  month = jan,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {52},
  number = {1},
  pages = {84--92},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f95-008},
  url = {https://cdnsciencepub.com/doi/abs/10.1139/f95-008},
  urldate = {2023-02-20},
  abstract = {The parameters of the allometric equation used to describe the length--weight relationship in fish are usually estimated by linear regression of log-transformed data. Simulation of length--weight regressions showed that for sample sizes commonly encountered in fisheries research, estimates of the intercept are biased high. In contrast with this, estimates of mean weight-at-length are biased low. As a result, different bias-correction factors are necessary to adjust for transformation bias. When the objective is to estimate the intercept of the length--weight equation, two existing methods correct for transformation bias. The appropriate bias-correction factor for mean weight-at-length, however, is exp({$\sigma$}2/2) where {$\sigma$}2 is the residual variance of the regression. As an alternative to the log-transformation method, the properties of nonlinear least-squares regression were explored. Estimates obtained with nonlinear regression are less efficient, but are relatively robust to departures from the assumed error structure. Simulation of several sampling designs showed that greater precision without loss of accuracy is obtained as subsampling is concentrated toward the extremes of the length distribution.},
  file = {C:\Users\jhncsu\Zotero\storage\FQY2DQEJ\Hayes et al. - 1995 - Efficiency and bias of estimators and sampling des.pdf}
}

@article{hearn.etal1987,
  title = {Robust Estimation of the Natural Mortality Rate in a Completed Tagging Experiment with Variable Fishing Intensity},
  author = {Hearn, William S. and Sandland, Ronald L. and Hampton, John},
  year = {1987},
  month = jan,
  journal = {ICES Journal of Marine Science},
  volume = {43},
  number = {2},
  pages = {107--117},
  issn = {1054-3139},
  doi = {10.1093/icesjms/43.2.107},
  url = {https://doi.org/10.1093/icesjms/43.2.107},
  urldate = {2023-03-07},
  abstract = {A method is described for estimating the instantaneous natural mortality rate, assumed constant, in a fish population, using data from a completed tag-recapture experiment, that is, one in which the population of tagged fish is subject to fishing until no live tagged fish remain. Data from different experiments may be pooled in order to make a more accurate estimate of the natural mortality rate. Jackknife methods, utilizing an efficient computational algorithm, are used to reduce the bias of the estimate of the natural mortality rate and to provide a standard error for the estimate. The method is shown, by simulation studies, to be more robust to departures from constancy in fishing intensity than that of Gulland (1955; Biometrika, 42: 269--270), which assumes such constancy. A modification of the method which requires an additional assumption about the fishing intensity is also developed so that a preliminary estimate of the natural mortality rate can be obtained before the completion of an experiment. Southern bluefin tuna ( Thunnus maccoyii ) tag-recapture data are analysed to illustrate the use of the method in practice.},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\DFL4ADGT\\Hearn et al. - 1987 - Robust estimation of the natural mortality rate in.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\D8JY5YRY\\668429.html}
}

@article{hearn.etal2003,
  title = {Tag {{Reporting Rate Estimation}}: 3. {{Use}} of {{Planted Tags}} in {{One Component}} of a {{Multiple-Component Fishery}}},
  shorttitle = {Tag {{Reporting Rate Estimation}}},
  author = {Hearn, William S. and Hoenig, John M. and Pollock, Kenneth H. and Hepworth, Daniel A.},
  year = {2003},
  journal = {North American Journal of Fisheries Management},
  volume = {23},
  number = {1},
  pages = {66--77},
  issn = {1548-8675},
  doi = {10.1577/1548-8675(2003)023<0066:TRREUO>2.0.CO;2},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1577/1548-8675%282003%29023%3C0066%3ATRREUO%3E2.0.CO%3B2},
  urldate = {2022-04-18},
  abstract = {Tag return models are used to estimate survival and tag recovery rates. With additional information on tag reporting rates, one can separate the survival rate into its fishing and natural mortality rate components. One method of estimating the tag reporting rate is to secretly plant tags in fishers' catches. However, if the fishery has more than one component, it may not be possible to plant tags in all components. Nevertheless, it is possible to estimate the reporting rates of all components in a multiple-component fishery and the fishing and natural mortality rates, if at least one component has a known reporting rate and the catches are known for each component. We simulate a variety of tag return experiments in which tags are planted in one component of a multicomponent fishery. The simulations show that this method is most effective (i.e., provides good precision of parameter estimates) when a sufficient number of tagged fish are planted into a fishery component with a high reporting rate and with a high proportion of the total catch. It is also advantageous to encourage the reporting of tags in the fishery components without planted tags. We provide a method for testing various model assumptions when it is possible to plant tags in more than one component.},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\RXRL4CA7\\Hearn et al. - 2003 - Tag Reporting Rate Estimation 3. Use of Planted T.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\P7YLSFHD\\1548-8675(2003)0230066TRREUO2.0.html}
}

@article{hewitt.etal2010,
  title = {{Improving Inferences from Fisheries Capture-Recapture Studies through Remote Detection of PIT Tags}},
  author = {Hewitt, David A. and Janney, Eric C. and Hayes, Brian S. and Shively, Rip S.},
  year = {2010},
  journal = {Fisheries},
  volume = {35},
  number = {5},
  pages = {217--231},
  issn = {1548-8446},
  doi = {10.1577/1548-8446-35.5.217},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1577/1548-8446-35.5.217},
  urldate = {2022-03-23},
  abstract = {Models for capture-recapture data are commonly used in analyses of the dynamics of fish and wildlife populations, especially for estimating vital parameters such as survival. Capture-recapture methods provide more reliable inferences than other methods commonly used in fisheries studies. However, for rare or elusive fish species, parameter estimation is often hampered by small probabilities of re-encountering tagged fish when encounters are obtained through traditional sampling methods. We present a case study that demonstrates how remote antennas for passive integrated transponder (PIT) tags can increase encounter probabilities and the precision of survival estimates from capture-recapture models. Between 1999 and 2007, trammel nets were used to capture and tag over 8,400 endangered adult Lost River suckers (Deltistes luxatus) during the spawning season in Upper Klamath Lake, Oregon. Despite intensive sampling at relatively discrete spawning areas, encounter probabilities from Cormack-Jolly-Seber models were consistently low ({$<$} 0.2) and the precision of apparent annual survival estimates was poor. Beginning in 2005, remote PIT tag antennas were deployed at known spawning locations to increase the probability of re-encountering tagged fish. We compare results based only on physical recaptures with results based on both physical recaptures and remote detections to demonstrate the substantial improvement in estimates of encounter probabilities (approaching 100\%) and apparent annual survival provided by the remote detections. The richer encounter histories provided robust inferences about the dynamics of annual survival and have made it possible to explore more realistic models and hypotheses about factors affecting the conservation and recovery of this endangered species. Recent advances in technology related to PIT tags have paved the way for creative implementation of large-scale tagging studies in systems where they were previously considered impracticable.},
  langid = {spanish},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\V7EGX5PT\\Hewitt et al. - 2010 - Improving Inferences from Fisheries Capture-Recapt.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\PCL7U2NH\\1548-8446-35.5.html}
}

@article{hightower.etal2001,
  title = {Use of {{Telemetry Methods}} to {{Estimate Natural}} and {{Fishing Mortality}} of {{Striped Bass}} in {{Lake Gaston}}, {{North Carolina}}},
  author = {Hightower, Joseph E. and Jackson, James R. and Pollock, Kenneth H.},
  year = {2001},
  month = jul,
  journal = {Transactions of the American Fisheries Society},
  volume = {130},
  number = {4},
  pages = {557--567},
  publisher = {Taylor \& Francis},
  issn = {0002-8487},
  doi = {10.1577/1548-8659(2001)130<0557:UOTMTE>2.0.CO;2},
  url = {https://doi.org/10.1577/1548-8659(2001)130<0557:UOTMTE>2.0.CO;2},
  urldate = {2022-03-22},
  abstract = {Natural mortality can substantially affect fish population dynamics, but the rate is difficult to estimate because natural deaths are rarely observed and it is difficult to separate the effects of natural and fishing mortality on abundance. We developed a new telemetry approach for estimating natural and fishing mortality rates and applied it to the population of striped bass Morone saxatilis in Lake Gaston, North Carolina and Virginia. Our analyses were based on a sample size of 51 telemetered striped bass that were known to be alive and in Lake Gaston at least 1 month after capture and surgery. Relocations of live fish and fish that died of natural causes were used to estimate natural and fishing mortality rates and the probability of relocating telemetered fish. Fishing mortality rates varied seasonally, but few natural deaths were observed, so the best model incorporated a constant annual instantaneous natural mortality rate (M; {\textpm}SE) of 0.14 {\textpm} 0.02. With the uncertainty in model selection accounted for, the average annual M was 0.16 {\textpm} 0.04 for 1997 and 0.12 {\textpm} 0.04 for 1998. Estimated annual fishing mortality rates (F) were 0.74 {\textpm} 0.13 for 1997 and 0.34 {\textpm} 0.18 for 1998. This telemetry approach for estimating mortality rates does not rely on angler reporting of tagged fish. The relative standard errors for M (24--33\%) were comparable to those obtained from traditional tagging methods with large sample sizes. This approach is most applicable in closed systems, where fishing mortality estimates are not biased by emigration. A high relocation probability is critical to reliably establishing seasonal changes in mortality.},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\ZG554KGI\\Hightower et al. - 2001 - Use of Telemetry Methods to Estimate Natural and F.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\SUCIX5ZJ\\1548-8659(2001)1300557UOTMTE2.0.html}
}

@article{hightower.etal2015,
  title = {Estimated {{Survival}} of {{Subadult}} and {{Adult Atlantic Sturgeon}} in {{Four River Basins}} in the {{Southeastern United States}}},
  author = {Hightower, Joseph E. and Loeffler, Michael and Post, William C. and Peterson, Douglas L.},
  year = {2015},
  journal = {Marine and Coastal Fisheries},
  volume = {7},
  number = {1},
  pages = {514--522},
  issn = {1942-5120},
  doi = {10.1080/19425120.2015.1088491},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/19425120.2015.1088491},
  urldate = {2022-03-23},
  abstract = {Prompted by concerns about the status of Atlantic Sturgeon Acipenser oxyrinchus oxyrinchus, in 2012 the National Oceanic and Atmospheric Administration listed one distinct population segment (DPS) as threatened (Gulf of Maine) and listed the remaining four DPSs as endangered (New York Bight, Chesapeake Bay, Carolina, and South Atlantic). To provide information for recovery planning, we estimated the survival of subadult and adult Atlantic Sturgeon in two river basins within the Carolina DPS (Roanoke and Cape Fear rivers, North Carolina) and two basins within the South Atlantic DPS (Ashepoo--Combahee--Edisto rivers [ACE], South Carolina; Altamaha River, Georgia). Estimated detection probability varied strongly by season but was similar among river basins, likely reflecting a winter migration into marine waters with minimal receiver coverage. Apparent monthly survival was very high and precisely estimated for the Roanoke River (0.985; 95\% credible interval [CI] = 0.970--0.995), Cape Fear River (0.979; 95\% CI = 0.971--0.986), ACE (0.989; 95\% CI = 0.979--0.993), and Altamaha River (0.985; 95\% CI = 0.973--0.994) basins. A pooled estimate for 87 adults from all four basins was 0.988 (95\% CI = 0.982--0.992). The monthly rates implied annual apparent survival rates of 0.839 (Roanoke River basin), 0.778 (Cape Fear River basin), 0.871 (ACE basin), and 0.842 (Altamaha River basin); the pooled estimate for adults was 0.860. Our estimated survival rates were similar to other recent estimates for Atlantic Sturgeon but lower than recent estimates for several populations of Gulf Sturgeon A. oxyrinchus desotoi. Recovery of Atlantic Sturgeon in these southeastern rivers will occur more quickly if survival can be increased to a level that is consistent with published estimates of true natural mortality (0.03--0.07; annual survival {$\geq$} 0.93). Received March 18, 2015; accepted August 26, 2015},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\M8DF3TDL\\Hightower et al. - 2015 - Estimated Survival of Subadult and Adult Atlantic .pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\WYSS3KFZ\\19425120.2015.html}
}

@article{hightower.grossman_1985,
  title = {Comparison of {{Constant Effort Harvest Policies}} for {{Fish Stocks}} with {{Variable Recruitment}}},
  author = {Hightower, Joseph E. and Grossman, Gary D.},
  year = {1985},
  month = may,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {42},
  number = {5},
  pages = {982--988},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f85-123},
  url = {https://cdnsciencepub.com/doi/abs/10.1139/f85-123},
  urldate = {2023-04-12},
  abstract = {Environmental variability may have a substantial influence on marine fish stocks, primarily by affecting survival to the time of recruitment. Simulation studies at low, intermediate, and high levels of variability in recruitment were used to compare alternative constant effort policies for anchovy (Engraulis capensis), Atlantic menhaden (Brevoortia tyrannus), and Pacific ocean perch (Sebastes alutus) fisheries. These policies were either to maintain effort at the level producing maximum sustainable yield (fMSY), or to permit levels of effort 25--100\% greater than fMSY. An increase in effort of 25\% above fMSY typically did not reduce annual yield significantly; however, a significant reduction in yield was apparent in all cases when effort increased by 75--100\%. When recruitment is highly variable, comparable yields may be obtained at several levels of fishing effort. In such cases, environmental variability provides the fishery manager with considerable flexibility to enhance social or economic benefits without decreasing yields significantly.},
  file = {C:\Users\jhncsu\Zotero\storage\DLRKCPE9\Hightower and Grossman - 1985 - Comparison of Constant Effort Harvest Policies for.pdf}
}

@article{hightower.harris2017,
  title = {Estimating {{Fish Mortality Rates Using Telemetry}} and {{Multistate Models}}},
  author = {Hightower, Joseph E. and Harris, Julianne E.},
  year = {2017},
  journal = {Fisheries},
  volume = {42},
  number = {4},
  pages = {210--219},
  issn = {1548-8446},
  doi = {10.1080/03632415.2017.1276347},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1080/03632415.2017.1276347},
  urldate = {2022-03-23},
  abstract = {We simulated and evaluated multistate capture--recapture models to estimate mortality rates using telemetry data. Four field designs were considered: (A) fixed receivers to estimate total instantaneous mortality (Z), (B) manual searches to estimate instantaneous fishing (F) and natural (M) mortality, (C) fixed receivers combined with external high-reward tags to estimate F and M, and (D) manual searches combined with external high-reward tags to estimate M and fishing mortality rates associated with harvest (Fh) and catch-and-release death (Fcr) as well as the probability of death due to catch and release ({$\alpha$}). Estimates generally appeared to be unbiased for a simulated study with five periods and releases of telemetered fish at the start of periods 1--4. Compared to estimating Z, larger sample sizes are needed to achieve reliable estimates of component rates (F and M). Estimates of component rates were more precise when that source of mortality was directly observed (M in design B, F in design C). The field design using fixed receivers and high-reward tags should be especially useful in practice, because manual searches are not required to estimate F and M. Multistate models are useful for clarifying the connection between field observations and ecological processes. Reliable estimates of mortality rates, coupled with information on behavior, habitat use, and movement, make telemetry a highly valuable tool for improving fisheries management and stock assessment.},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\L3A7QBCX\\Hightower and Harris - 2017 - Estimating Fish Mortality Rates Using Telemetry an.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\JKNIKGII\\03632415.2017.html}
}

@article{hightower2015,
  title = {Estimated {{Survival}} of {{Subadult}} and {{Adult Atlantic Sturgeon}} in {{Four River Basins}} in the {{Southeastern United States}}},
  author = {Hightower, Joseph E.},
  year = {2015},
  month = jan,
  journal = {Marine and Coastal Fisheries: Dynamics, Management, and Ecosystem Science},
  volume = {7},
  number = {7},
  pages = {514--522},
  publisher = {American Fisheries Society},
  issn = {1942-5120},
  doi = {10.1080/19425120.2015.1088491},
  url = {http://bioone.org/journals/marine-and-coastal-fisheries/volume-7/issue-7/19425120.2015.1088491/Estimated-Survival-of-Subadult-and-Adult-Atlantic-Sturgeon-in-Four/10.1080/19425120.2015.1088491.full},
  urldate = {2022-03-23},
  abstract = {Prompted by concerns about the status of Atlantic Sturgeon Acipenser oxyrinchus oxyrinchus, in 2012 the National Oceanic and Atmospheric Administration listed one distinct population segment (DPS) as threatened (Gulf of Maine) and listed the remaining four DPSs as endangered (New York Bight, Chesapeake Bay, Carolina, and South Atlantic). To provide information for recovery planning, we estimated the survival of subadult and adult Atlantic Sturgeon in two river basins within the Carolina DPS (Roanoke and Cape Fear rivers, North Carolina) and two basins within the South Atlantic DPS (Ashepoo---Combahee---Edisto rivers [ACE], South Carolina; Altamaha River, Georgia). Estimated detection probability varied strongly by season but was similar among river basins, likely reflecting a winter migration into marine waters with minimal receiver coverage. Apparent monthly survival was very high and precisely estimated for the Roanoke River (0.985; 95\% credible interval [CI] = 0.970-0.995), Cape Fear River (0.979; 95\% CI = 0.971-0.986), ACE (0.989; 95\% CI = 0.979-0.993), and Altamaha River (0.985; 95\% CI = 0.973-0.994) basins. A pooled estimate for 87 adults from all four basins was 0.988 (95\% CI = 0.982-0.992). The monthly rates implied annual apparent survival rates of 0.839 (Roanoke River basin), 0.778 (Cape Fear River basin), 0.871 (ACE basin), and 0.842 (Altamaha River basin); the pooled estimate for adults was 0.860. Our estimated survival rates were similar to other recent estimates for Atlantic Sturgeon but lower than recent estimates for several populations of Gulf Sturgeon A. oxyrinchus desotoi. Recovery of Atlantic Sturgeon in these southeastern rivers will occur more quickly if survival can be increased to a level that is consistent with published estimates of true natural mortality (0.03--0.07; annual survival {$\geq$} 0.93).},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\8C52GRDX\\Hightower - 2015 - Estimated Survival of Subadult and Adult Atlantic .pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\2X2M8SL2\\19425120.2015.1088491.html}
}

@book{hilborn.walters_1992,
  title = {Quantitative {{Fisheries Stock Assessment}}},
  author = {Hilborn, Ray and Walters, Carl J.},
  year = {1992},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4615-3598-0},
  url = {http://link.springer.com/10.1007/978-1-4615-3598-0},
  urldate = {2023-04-09},
  isbn = {978-1-4020-1845-9 978-1-4615-3598-0},
  langid = {english},
  keywords = {biomass,fish,fisheries management,growth}
}

@article{hooten.hobbs2015,
  title = {A Guide to {{Bayesian}} Model Selection for Ecologists},
  author = {Hooten, M. B. and Hobbs, N. T.},
  year = {2015},
  journal = {Ecological Monographs},
  volume = {85},
  number = {1},
  pages = {3--28},
  issn = {1557-7015},
  doi = {10.1890/14-0661.1},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1890/14-0661.1},
  urldate = {2022-10-04},
  abstract = {The steady upward trend in the use of model selection and Bayesian methods in ecological research has made it clear that both approaches to inference are important for modern analysis of models and data. However, in teaching Bayesian methods and in working with our research colleagues, we have noticed a general dissatisfaction with the available literature on Bayesian model selection and multimodel inference. Students and researchers new to Bayesian methods quickly find that the published advice on model selection is often preferential in its treatment of options for analysis, frequently advocating one particular method above others. The recent appearance of many articles and textbooks on Bayesian modeling has provided welcome background on relevant approaches to model selection in the Bayesian framework, but most of these are either very narrowly focused in scope or inaccessible to ecologists. Moreover, the methodological details of Bayesian model selection approaches are spread thinly throughout the literature, appearing in journals from many different fields. Our aim with this guide is to condense the large body of literature on Bayesian approaches to model selection and multimodel inference and present it specifically for quantitative ecologists as neutrally as possible. We also bring to light a few important and fundamental concepts relating directly to model selection that seem to have gone unnoticed in the ecological literature. Throughout, we provide only a minimal discussion of philosophy, preferring instead to examine the breadth of approaches as well as their practical advantages and disadvantages. This guide serves as a reference for ecologists using Bayesian methods, so that they can better understand their options and can make an informed choice that is best aligned with their goals for inference.},
  langid = {english},
  keywords = {Akaike information criterion,Bayes factors,cross-validation,deviance information criterion,model averaging,multi-model inference,regularization,shrinkage},
  file = {C:\Users\jhncsu\Zotero\storage\GI6S4GMV\14-0661.html}
}

@article{jiang.etal2007,
  title = {Tag {{Return Models Allowing}} for {{Harvest}} and {{Catch}} and {{Release}}: {{Evidence}} of {{Environmental}} and {{Management Impacts}} on {{Striped Bass Fishing}} and {{Natural Mortality Rates}}},
  shorttitle = {Tag {{Return Models Allowing}} for {{Harvest}} and {{Catch}} and {{Release}}},
  author = {Jiang, Honghua and Pollock, Kenneth H. and Brownie, Cavell and Hoenig, John M. and Latour, Robert J. and Wells, Brian K. and Hightower, Joseph E.},
  year = {2007},
  journal = {North American Journal of Fisheries Management},
  volume = {27},
  number = {2},
  pages = {387--396},
  issn = {1548-8675},
  doi = {10.1577/M06-089.1},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1577/M06-089.1},
  urldate = {2022-04-10},
  abstract = {Catch-and-release fisheries have become very important in the management of overexploited recreational fish stocks. Tag return studies, where the tag is removed regardless of fish disposition, have been used to assess the effectiveness of restoration efforts for these fisheries. We extend the instantaneous rate formulation of tag return models to allow for catch and release as well as harvest. The key point of our methods is that, given an estimate of the tag reporting rate, the fishing mortality rate (F) is separated into two components: the mortality on harvested fish and the ``mortality'' on tags (because the tags are removed) of fish released alive. The total fishing mortality rate for untagged fish is the sum of the Fs due to harvest and hooking mortality suffered by fish released alive. Natural mortality rates can also be estimated. Both age-independent models and age-dependent models are constructed, and the age-dependent models are illustrated by application to data from a study of striped bass Morone saxatilis in Chesapeake Bay from 1991 to 2003 by the Maryland Department of Natural Resources. By fitting models of the natural mortality rate with limited age and year dependence, we demonstrate an overall decrease in natural mortality rates as fish age and provide evidence of an increase in natural mortality beginning in the late 1990s, when an outbreak of the disease mycobacteriosis is thought to have begun. Our results indicate that fishing mortality is age dependent; selectivity increases up to age 6, when fish appear to be fully recruited to the fishery. There is also evidence of an increase in fishing mortality since 1995, when regulations were relaxed.},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\KFHIH5SB\\Jiang et al. - 2007 - Tag Return Models Allowing for Harvest and Catch a.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\VAXK2UGQ\\M06-089.html}
}

@article{johnson.etal_2015,
  title = {Time-Varying Natural Mortality in Fisheries Stock Assessment Models: Identifying a Default Approach},
  shorttitle = {Time-Varying Natural Mortality in Fisheries Stock Assessment Models},
  author = {Johnson, Kelli F. and Monnahan, Cole C. and McGilliard, Carey R. and {Vert-pre}, Katyana A. and Anderson, Sean C. and Cunningham, Curry J. and {Hurtado-Ferro}, Felipe and Licandeo, Roberto R. and Muradian, Melissa L. and Ono, Kotaro and Szuwalski, Cody S. and Valero, Juan L. and Whitten, Athol R. and Punt, A. E.},
  year = {2015},
  month = jan,
  journal = {ICES JOURNAL OF MARINE SCIENCE},
  volume = {72},
  number = {1},
  pages = {137--150},
  publisher = {Oxford Univ Press},
  address = {Oxford},
  issn = {1054-3139, 1095-9289},
  doi = {10.1093/icesjms/fsu055},
  url = {https://academic.oup.com/icesjms/article/72/1/137/823490},
  urldate = {2024-09-09},
  abstract = {Atypical assumption used inmost fishery stock assessments is that natural mortality (M) is constant across time and age. However, M is rarely constant in reality as a result of the combined impacts of exploitation history, predation, environmental factors, and physiological trade-offs. Misspecification or poor estimation of M can lead to bias in quantities estimated using stock assessment methods, potentially resulting in biased estimates of fishery reference points and catch limits, with the magnitude of bias being influenced by life history and trends in fishing mortality. Monte Carlo simulations were used to evaluate the ability of statistical age-structured population models to estimate spawning-stock biomass, fishing mortality, and total allowable catch when the true M was age-invariant, but time-varying. Configurations of the stock assessment method, implemented in Stock Synthesis, included a single age-and time-invariant M parameter, specified at one of the three levels (high, medium, and low) or an estimated M. The min-max (i.e. most robust) approach to specifying M when it is thought to vary across time was to estimate M. The least robust approach for most scenarios examined was to fix M at a high value, suggesting that the consequences of misspecifying M are asymmetric.},
  langid = {english},
  keywords = {AGE,CATCH,CATCHABILITY,COD,EFFECTIVE SAMPLE-SIZE,ERRORS,FISH,JASUS-EDWARDSII,model misspecification,natural mortality,PERFORMANCE,population models,reference points,simulation,Stock Synthesis,time-varying,VIRTUAL POPULATION ANALYSIS},
  annotation = {Web of Science ID: WOS:000350153200015},
  file = {C:\Users\jhncsu\Zotero\storage\L92BB7UQ\Johnson et al. - 2015 - Time-varying natural mortality in fisheries stock .pdf}
}

@book{kéry_2010,
  title = {Introduction to {{WinBUGS}} for {{Ecologists}}: {{Bayesian}} Approach to Regression, {{ANOVA}}, Mixed Models and Related Analyses},
  shorttitle = {Introduction to {{WinBUGS}} for {{Ecologists}}},
  author = {K{\'e}ry, Marc},
  year = {2010},
  month = jul,
  edition = {1st edition},
  publisher = {Academic Press},
  address = {Amsterdam ; Boston},
  isbn = {978-0-12-378605-0},
  langid = {english}
}

@book{kéry.schaub_2011,
  title = {Bayesian {{Population Analysis}} Using {{WinBUGS}}: {{A Hierarchical Perspective}}},
  shorttitle = {Bayesian {{Population Analysis}} Using {{WinBUGS}}},
  author = {K{\'e}ry, Marc and Schaub, Michael},
  year = {2011},
  month = oct,
  edition = {1st edition},
  publisher = {Academic Press},
  address = {Boston},
  isbn = {978-0-12-387020-9},
  langid = {english}
}

@book{law.kelton_1982,
  title = {Simulation {{Modeling}} and {{Analysis}}},
  author = {Law, Averill M. and Kelton, W. David},
  year = {1982},
  publisher = {McGraw-Hill},
  googlebooks = {xOBQAAAAMAAJ},
  isbn = {978-0-07-036696-1},
  langid = {english}
}

@article{lee.etal_2011,
  title = {Estimating Natural Mortality within a Fisheries Stock Assessment Model: {{An}} Evaluation Using Simulation Analysis Based on Twelve Stock Assessments},
  shorttitle = {Estimating Natural Mortality within a Fisheries Stock Assessment Model},
  author = {Lee, Hui-Hua and Maunder, Mark N. and Piner, Kevin R. and Methot, Richard D.},
  year = {2011},
  month = apr,
  journal = {Fisheries Research},
  volume = {109},
  number = {1},
  pages = {89--94},
  issn = {0165-7836},
  doi = {10.1016/j.fishres.2011.01.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0165783611000403},
  urldate = {2024-10-20},
  abstract = {Natural mortality (M) is one of the most influential and difficult to estimate number of losses in fisheries stock assessment and management. Typically, natural mortality is estimated using indirect methods, such as correlation with measurable life history factors and rarely relies on direct data such as tagging studies. In contemporary stock assessments, natural mortality may be estimated within the model by integrating different types of data into the analysis. We evaluated the estimability of M using simulation analyses based on 12 groundfish stock assessments conducted using Stock Synthesis. The advantages of utilizing this set of peer-reviewed assessment models were that various types of data were used over a wide range of model parameterization. Our results suggest that, in many cases, M is estimable with appropriate data. Profile likelihood analyses suggested that informative length or age composition data is needed to reliably estimate M.},
  keywords = {Natural mortality,Pacific Coast groundfish stocks,Simulation analyses,Stock assessment,Stock Synthesis},
  file = {C:\Users\jhncsu\Zotero\storage\33QZQH5X\S0165783611000403.html}
}

@article{lemoine_2019,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {1600-0706},
  doi = {10.1111/oik.05985},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/oik.05985},
  urldate = {2024-11-06},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  langid = {english},
  keywords = {Bayesian statistics,frequentist statistics,Markov chain Monte Carlo,vague priors},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\S726HFHE\\Lemoine - 2019 - Moving beyond noninformative priors why and how t.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\8VH5RAPB\\oik.html}
}

@book{link.barker_2010,
  title = {Bayesian {{Inference}}: {{With Ecological Applications}}},
  shorttitle = {Bayesian {{Inference}}},
  author = {Link, William A. and Barker, Richard J.},
  year = {2010},
  edition = {1st edition},
  publisher = {Academic Press},
  address = {Amsterdam Boston London},
  abstract = {This text is written to provide a mathematically sound but accessible and engaging introduction to Bayesian inference specifically for environmental scientists, ecologists and wildlife biologists. It emphasizes the power and usefulness of Bayesian methods in an ecological context.The advent of fast personal computers and easily available software has~simplified the use of~Bayesian and hierarchical~models . One obstacle remains for ecologists and wildlife biologists, namely the near absence of Bayesian texts written specifically for them. The book includes many relevant examples, is supported by software and examples on a companion website and will become an essential grounding in this approach~for~students and research ecologists.Engagingly written text specifically designed to demystify a complex subjectExamples drawn from ecology and wildlife researchAn essential grounding for graduate and research ecologists in the increasingly prevalent Bayesian approach to inferenceCompanion website with analytical software and examplesLeading authors with world-class reputations in ecology and biostatistics},
  isbn = {978-0-12-374854-6},
  langid = {english}
}

@article{link.eaton2012,
  title = {On Thinning of Chains in {{MCMC}}},
  author = {Link, William A. and Eaton, Mitchell J.},
  year = {2012},
  journal = {Methods in Ecology and Evolution},
  volume = {3},
  number = {1},
  pages = {112--115},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210X.2011.00131.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2011.00131.x},
  urldate = {2022-01-28},
  abstract = {1. Markov chain Monte Carlo (MCMC) is a simulation technique that has revolutionised the analysis of ecological data, allowing the fitting of complex models in a Bayesian framework. Since 2001, there have been nearly 200 papers using MCMC in publications of the Ecological Society of America and the British Ecological Society, including more than 75 in the journal Ecology and 35 in the Journal of Applied Ecology. 2. We have noted that many authors routinely `thin' their simulations, discarding all but every kth sampled value; of the studies we surveyed with details on MCMC implementation, 40\% reported thinning. 3. Thinning is often unnecessary and always inefficient, reducing the precision with which features of the Markov chain are summarised. The inefficiency of thinning MCMC output has been known since the early 1990's, long before MCMC appeared in ecological publications. 4. We discuss the background and prevalence of thinning, illustrate its consequences, discuss circumstances when it might be regarded as a reasonable option and recommend against routine thinning of chains unless necessitated by computer memory limitations.},
  langid = {english},
  keywords = {Markov chain Monte Carlo,thinning,WinBUGS},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\4D9HKKH8\\Link and Eaton - 2012 - On thinning of chains in MCMC.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\YWWRNI85\\j.2041-210X.2011.00131.html}
}

@article{link.etal_2002,
  title = {Of {{Bugs}} and {{Birds}}: {{Markov Chain Monte Carlo}} for {{Hierarchical Modeling}} in {{Wildlife Research}}},
  shorttitle = {Of {{Bugs}} and {{Birds}}},
  author = {Link, William A. and Cam, Emmanuelle and Nichols, James D. and Cooch, Evan G.},
  year = {2002},
  journal = {The Journal of Wildlife Management},
  volume = {66},
  number = {2},
  eprint = {3803160},
  eprinttype = {jstor},
  pages = {277--291},
  publisher = {[Wiley, Wildlife Society]},
  issn = {0022-541X},
  doi = {10.2307/3803160},
  url = {https://www.jstor.org/stable/3803160},
  urldate = {2024-04-29},
  abstract = {Markov chain Monte Carlo (MCMC) is a statistical innovation that allows researchers to fit far more complex models to data than is feasible using conventional methods. Despite its widespread use in a variety of scientific fields, MCMC appears to be underutilized in wildlife applications. This may be due to a misconception that MCMC requires the adoption of a subjective Bayesian analysis, or perhaps simply to its lack of familiarity among wildlife researchers. We introduce the basic ideas of MCMC and software BUGS (Bayesian inference using Gibbs sampling), stressing that a simple and satisfactory intuition for MCMC does not require extraordinary mathematical sophistication. We illustrate the use of MCMC with an analysis of the association between latent factors governing individual heterogeneity in breeding and survival rates of kittiwakes (Rissa tridactyla). We conclude with a discussion of the importance of individual heterogeneity for understanding population dynamics and designing management plans.},
  file = {C:\Users\jhncsu\Zotero\storage\SBITB2KS\Link et al. - 2002 - Of Bugs and Birds Markov Chain Monte Carlo for Hi.pdf}
}

@article{lorenzen1996,
  title = {The Relationship between Body Weight and Natural Mortality in Juvenile and Adult Fish: A Comparison of Natural Ecosystems and Aquaculture},
  shorttitle = {The Relationship between Body Weight and Natural Mortality in Juvenile and Adult Fish},
  author = {Lorenzen, K.},
  year = {1996},
  journal = {Journal of Fish Biology},
  volume = {49},
  number = {4},
  pages = {627--642},
  issn = {1095-8649},
  doi = {10.1111/j.1095-8649.1996.tb00060.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1095-8649.1996.tb00060.x},
  urldate = {2022-02-23},
  abstract = {The relationship between body weight and natural mortality in juvenile and adult fish was analysed for different aquatic ecosystems: lakes, rivers, the ocean, and pond, cage and tank aquaculture systems. Mortality was modelled as a power function of weight, and the parameters b (exponent) and Mu (mortality at the unit weight of 1 g) estimated for fish in the six ecosystems, as well as within selected populations, species and families. At the ecosystem level, no significant differences in parameters were found between lakes, rivers and the ocean and a joint mortality-weight relationship for all natural ecosystems was estimated with parameters b=-0.288 (90\% CL[-0.315, -0.261]) and Mu=3.00 (90\% CL[2.70, 3.30]) year-1. Among the culture systems, mortality-weight relationships in ponds and cages were not significantly different and a joint relationship was estimated. The weight exponents of mortality in ponds/cages and tanks were very similar at about b=-0.43, and significantly more negative than in natural ecosystems. Mortalities at unit weight were significantly lower in tanks (0.91 year-1) than in ponds/cages (2.24 year-1), and both were significantly lower than in natural ecosystems. No systematic differences were found between the mortality-weight relationships determined for individual populations, species or families, and fish in the respective ecosystems. It is hypothesized that aquaculture mortality-weight relationships indicate the allometric scaling of non-predation mortality, which is therefore more strongly size dependent than predation mortality. If non predation mortality in natural ecosystems shows a similar scaling with body weight, then the allometric exponent of predation mortality must be less negative than that observed for total natural mortality. Implications of the established mortality-weight relationships for aquaculture and culture-based fisheries are discussed.},
  langid = {english},
  keywords = {allometry,aquaculture,body weight,ecosystem,mortality,predation},
  file = {C:\Users\jhncsu\Zotero\storage\F9LP7C45\j.1095-8649.1996.tb00060.html}
}

@article{lunn.etal2000,
  title = {{{WinBUGS}} - {{A Bayesian}} Modelling Framework: {{Concepts}}, Structure, and Extensibility},
  shorttitle = {{{WinBUGS}} - {{A Bayesian}} Modelling Framework},
  author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  year = {2000},
  month = oct,
  journal = {Statistics and Computing},
  volume = {10},
  number = {4},
  pages = {325--337},
  issn = {1573-1375},
  doi = {10.1023/A:1008929526011},
  url = {https://doi.org/10.1023/A:1008929526011},
  urldate = {2022-11-20},
  abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
  langid = {english},
  keywords = {BUGS,directed acyclic graphs,Markov chain Monte Carlo,object-orientation,run-time linking,type extension,WinBUGS}
}

@article{lunn.etal2009,
  title = {The {{BUGS}} Project: {{Evolution}}, Critique and Future Directions},
  shorttitle = {The {{BUGS}} Project},
  author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {25},
  pages = {3049--3067},
  issn = {1097-0258},
  doi = {10.1002/sim.3680},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3680},
  urldate = {2022-01-27},
  abstract = {BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian modelling,BUGS,graphical models,OpenBUGS,WinBUGS},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\HRRS6W3N\\Lunn et al. - 2009 - The BUGS project Evolution, critique and future d.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\H4S4HQZE\\sim.html}
}

@book{lunn.etal2012,
  title = {The {{BUGS Book}}: {{A Practical Introduction}} to {{Bayesian Analysis}}},
  shorttitle = {The {{BUGS Book}}},
  author = {Lunn, David and Jackson, Chris and Best, Nicky and Thomas, Andrew and Spiegelhalter, David},
  year = {2012},
  month = oct,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  langid = {english}
}

@article{mantyniemi.etal2005,
  title = {Bayesian Removal Estimation of a Population Size under Unequal Catchability},
  author = {M{\"a}ntyniemi, Samu and Romakkaniemi, Atso and Arjas, Elja},
  year = {2005},
  month = feb,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {62},
  pages = {291--300},
  doi = {10.1139/f04-195},
  abstract = {We introduce a Bayesian probability model for the estimation of the size of an animal population from removal data. The model is based on the assumption that in the removal sampling, catchability may vary between individuals, which appears to be necessary for a realistic description of many biological populations. Heterogeneous catchability among individuals leads to a situation where the mean catchability in the population gradually decreases as the number of removals increases. Under this assumption, the model can be fitted to any removal data, i.e., there are no limitations regarding the total catch, the number of removals, or the decline of the catch. Using a published data set from removal experiments of a known population size, the model is shown to be able to estimate the population size appropriately in all cases considered. It is also shown that regardless of the statistical approach, a model that assumes equal catchability of individuals generally leads to an underestimation of the population. The example indicates that if there is only vague prior information about the variation of catchability among individuals, a very high number of successive removals may be needed to correctly estimate the population size.},
  file = {C:\Users\jhncsu\Zotero\storage\TNEDV27M\Mäntyniemi et al. - 2005 - Bayesian removal estimation of a population size u.pdf}
}

@article{maunder.punt_2013,
  title = {A Review of Integrated Analysis in Fisheries Stock Assessment},
  author = {Maunder, Mark N. and Punt, Andre E.},
  year = {2013},
  month = may,
  journal = {FISHERIES RESEARCH},
  volume = {142},
  pages = {61--74},
  publisher = {Elsevier},
  address = {Amsterdam},
  issn = {0165-7836, 1872-6763},
  doi = {10.1016/j.fishres.2012.07.025},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0165783612002627},
  urldate = {2024-09-09},
  abstract = {Limited data, and the requirement to provide science-based advice for exploited populations, have led to the development of statistical methods that combine several sources of information into a single analysis. This approach, "integrated analysis", was first formulated by Fournier and Archibald in 1982. Contemporary use of integrated analysis involves using all available data, in as raw a form as appropriate, in a single analysis. Analyses that were traditionally carried out independently are now conducted simultaneously through likelihood functions that include multiple data sources. For example, the traditional analysis of converting catch-at-length data into catch-at-age data for use in an age-structured population dynamics models can be avoided by including the basic data used in this conversion, length-frequency and conditional age-at-length data, in the likelihood function. This allows for consistency in assumptions and permits the uncertainty associated with both data sources to be propagated to final model outputs, such as catch limits under harvest control rules. The development of the AD Model Builder software has greatly facilitated the use of integrated analyses, and there are now several general stock assessment models (e.g., Stock Synthesis) that allow many data types and model assumptions to be analyzed simultaneously. In this paper, we define integrated analysis, describe its history and development, give several examples, and describe the advantages of and problems with integrated analysis. (C) 2012 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {ASSESSMENT MODELS,Bayesian,CONTRADICTORY DATA SOURCES,Data assimilation,DATA ASSIMILATION,Fisheries stock assessment,GENERAL FRAMEWORK,Integrated analysis,Maximum likelihood,Meta-analysis,Multivariate nonlinear regression,NATURAL MORTALITY,PARAMETER-ESTIMATION,SCHAEFER MODEL,TASMANIAN ROCK LOBSTER,VARIANCE-ESTIMATION,VIRTUAL POPULATION ANALYSIS},
  annotation = {Web of Science ID: WOS:000317811500009}
}

@article{mccarthy.masters_2005,
  title = {Profiting from Prior Information in {{Bayesian}} Analyses of Ecological Data},
  author = {McCarthy, Michael A. and Masters, Pip},
  year = {2005},
  journal = {Journal of Applied Ecology},
  volume = {42},
  number = {6},
  pages = {1012--1019},
  issn = {1365-2664},
  doi = {10.1111/j.1365-2664.2005.01101.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2664.2005.01101.x},
  urldate = {2024-11-07},
  abstract = {1 Most ecological studies include prior information only implicitly, usually in their design or the discussion of results. In this study, two examples demonstrate that using Bayesian statistics to incorporate basic ecological principles and prior data can be very cost-effective for increasing confidence in ecological research. 2 The first example is based on examining the effects of an experimental manipulation of the habitat of mulgara Dasycercus cristicauda, a marsupial of inland Australia. The second example is based on observational mark--recapture data to estimate the annual survival of the European dipper Cinclus cinclus, a passerine in France. 3 In the mulgara example, the prior information obtained from an observational study increased confidence that there was an adverse effect of experimental habitat manipulation on the species. The results suggested that the capture rate of mulgara was reduced to approximately one-quarter by reduction of vegetation cover. 4 In the European dipper example, prior information based on the body mass of the species and estimates of annual survival of other European passerines was shown to be worth between 1 and 5 years of mark--recapture field data. 5 Synthesis and applications. Body mass can be used to predict annual survival of European passerines and other animals. Results of observational studies can provide prior information in experimental studies of impacts of habitat change. By using Bayesian methods, such prior information, if represented in a coherent and logical way, can be cost-effective for adding certainty to ecological studies.},
  langid = {english},
  keywords = {Cinclus cinclus,Dasycercus cristicauda,habitat manipulation,mark-recapture,Markov chain Monte Carlo,survival},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\SBXA4KUV\\McCARTHY and Masters - 2005 - Profiting from prior information in Bayesian analy.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\P948YFHI\\j.1365-2664.2005.01101.html}
}

@book{mccarthy2007,
  title = {Bayesian {{Methods}} for {{Ecology}}},
  author = {McCarthy, Michael A.},
  year = {2007},
  month = may,
  edition = {1st edition},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York},
  isbn = {978-0-521-61559-4},
  langid = {english}
}

@article{methot.wetzell_2013,
  title = {Stock Synthesis: {{A}} Biological and Statistical Framework for Fish Stock Assessment and Fishery Management},
  author = {Methot, Jr., Richard D. and Wetzell, Chantell R.},
  year = {2013},
  journal = {Fisheries Research},
  volume = {142},
  pages = {86--99},
  doi = {https://doi-org.prox.lib.ncsu.edu/10.1016/j.fishres.2012.10.012}
}

@article{millar.meyer2000,
  title = {Bayesian State-Space Modeling of Age-Structured Data: Fitting a Model Is Just the Beginning},
  shorttitle = {Bayesian State-Space Modeling of Age-Structured Data},
  author = {Millar, Russell B and Meyer, Renate},
  year = {2000},
  month = jan,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {57},
  number = {1},
  pages = {43--50},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f99-169},
  url = {https://cdnsciencepub.com/doi/10.1139/f99-169},
  urldate = {2022-12-27},
  abstract = {Explicit modeling of process variability in the dynamics of fisheries is motivated by a desire to incorporate more realism into stock assessment models, and much recent research effort has been devoted to the computational features of fitting state-space models for this purpose. Here, we extend the Bayesian application of nonlinear state-space modeling to sequential population analysis of age-structured data using a model formulation that allows for unreported catches and incidental fishing mortality. It is shown that, once a familiarity with the general-purpose Bayesian software BUGS is acquired, implementing a state-space model is a relatively simple task. Indeed, this application requires just 18 lines of code in its entirety and does not require the programmer to know the formulae for any prior density functions or likelihoods. Consequently, we suggest that this methodology may permit the implementation phase of nonlinear state-space modeling to be relegated, thereby allowing more effort to be devoted to the challenging issues of model checking, selection/averaging, sensitivity, and prior specification.}
}

@article{monnahan.etal_2017,
  title = {Faster Estimation of {{Bayesian}} Models in Ecology Using {{Hamiltonian Monte Carlo}}},
  author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
  year = {2017},
  journal = {Methods in Ecology and Evolution},
  volume = {8},
  number = {3},
  pages = {339--348},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12681},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12681},
  urldate = {2024-03-26},
  abstract = {Bayesian inference is a powerful tool to better understand ecological processes across varied subfields in ecology, and is often implemented in generic and flexible software packages such as the widely used BUGS family (BUGS, WinBUGS, OpenBUGS and JAGS). However, some models have prohibitively long run times when implemented in BUGS. A relatively new software platform called Stan uses Hamiltonian Monte Carlo (HMC), a family of Markov chain Monte Carlo (MCMC) algorithms which promise improved efficiency and faster inference relative to those used by BUGS. Stan is gaining traction in many fields as an alternative to BUGS, but adoption has been slow in ecology, likely due in part to the complex nature of HMC. Here, we provide an intuitive illustration of the principles of HMC on a set of simple models. We then compared the relative efficiency of BUGS and Stan using population ecology models that vary in size and complexity. For hierarchical models, we also investigated the effect of an alternative parameterization of random effects, known as non-centering. For small, simple models there is little practical difference between the two platforms, but Stan outperforms BUGS as model size and complexity grows. Stan also performs well for hierarchical models, but is more sensitive to model parameterization than BUGS. Stan may also be more robust to biased inference caused by pathologies, because it produces diagnostic warnings where BUGS provides none. Disadvantages of Stan include an inability to use discrete parameters, more complex diagnostics and a greater requirement for hands-on tuning. Given these results, Stan is a valuable tool for many ecologists utilizing Bayesian inference, particularly for problems where BUGS is prohibitively slow. As such, Stan can extend the boundaries of feasible models for applied problems, leading to better understanding of ecological processes. Fields that would likely benefit include estimation of individual and population growth rates, meta-analyses and cross-system comparisons and spatiotemporal models.},
  langid = {english},
  keywords = {Bayesian inference,hierarchical modelling,Markov chain Monte Carlo,no-U-turn sampler,Stan},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\9R3JENC9\\Monnahan et al. - 2017 - Faster estimation of Bayesian models in ecology us.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\7ZLVQN2T\\2041-210X.html}
}

@article{newman.etal_2023,
  title = {State-Space Models for Ecological Time-Series Data: {{Practical}} Model-Fitting},
  shorttitle = {State-Space Models for Ecological Time-Series Data},
  author = {Newman, Ken and King, Ruth and Elvira, V{\'i}ctor and {de Valpine}, Perry and McCrea, Rachel S. and Morgan, Byron J. T.},
  year = {2023},
  journal = {Methods in Ecology and Evolution},
  volume = {14},
  number = {1},
  pages = {26--42},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13833},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13833},
  urldate = {2024-11-15},
  abstract = {State-space models are an increasingly common and important tool in the quantitative ecologists' armoury, particularly for the analysis of time-series data. This is due to both their flexibility and intuitive structure, describing the different individual processes of a complex system, thus simplifying the model specification step. State-space models are composed of two processes (a) the system (or state) process that describes the dynamics of the true underlying state of the system over time; and (b) the observation process that links the observed data with the current true state of the system at that time. Specification of the general model structure consists of considering each distinct ecological process within the system and observation processes, which are then automatically combined within the state-space structure. There is typically a trade-off between the complexity of the model and the associated model-fitting process. Simpler model specifications permit the application of simpler model-fitting tools; whereas more complex model specifications, with nonlinear dynamics and/or non-Gaussian stochasticity often require more sophisticated model-fitting algorithms to be applied. We provide a brief overview of general state-space models before focusing on the different model-fitting tools available. In particular for different general state-space model structures we discuss established model-fitting tools that are available. We also offer practical guidance for choosing a specific fitting procedure.},
  copyright = {{\copyright} 2022 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {hidden Markov model,Kalman filter,Laplace approximation,likelihood-free methods,Markov chain Monte Carlo,sampling-based methods,sequential Monte Carlo},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\5CPCHBVG\\Newman et al. - 2023 - State-space models for ecological time-series data.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\8K7L8BDR\\2041-210X.html}
}

@book{ogle_2018,
  title = {Introductory {{Fisheries Analyses}} with {{R}}},
  author = {Ogle, Derek H.},
  year = {2018},
  series = {Chapman \& {{Hall}}/{{CRC The R Series}}},
  edition = {First edition.},
  publisher = {{Boca Raton, FL : Chapman and Hall/CRC, [2018].}},
  url = {https://catalog.lib.ncsu.edu/catalog/NCSU4637798},
  isbn = {978-1-315-37198-6 978-1-4822-3520-3},
  file = {C:\Users\jhncsu\Zotero\storage\3RWU6ABN\NCSU4637798.html}
}

@article{plummer2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  author = {Plummer, M.},
  year = {2003},
  journal = {Proceedings of the 3rd International Workshop on Distributed Statistical Computing},
  doi = {10.1023/A:1008929526011},
  url = {https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf}
}

@article{pollock.etal_2001,
  title = {Tag {{Reporting Rate Estimation}}: 1. {{An Evaluation}} of the {{High-Reward Tagging Method}}},
  shorttitle = {Tag {{Reporting Rate Estimation}}},
  author = {Pollock, Kenneth H. and Hoenig, John M. and Hearn, William S. and Calingaert, Brian},
  year = {2001},
  journal = {North American Journal of Fisheries Management},
  volume = {21},
  number = {3},
  pages = {521--532},
  issn = {1548-8675},
  doi = {10.1577/1548-8675(2001)021<0521:TRREAE>2.0.CO;2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1577/1548-8675%282001%29021%3C0521%3ATRREAE%3E2.0.CO%3B2},
  urldate = {2024-02-10},
  abstract = {Tag-return models can be used to estimate survival rates and tag recovery rates. The additional knowledge of an estimated tag reporting rate allows one to separate total mortality into fishing and natural mortality rates. This paper examines the use of high-reward tags in tagging studies. We find that many of the problems encountered in tagging studies can be avoided if tagged animals are released in small batches in as many locations as possible rather than in large batches at a few locations. Often, the use of substantial monetary rewards for the return of standard tags may be justified as cost effective because of the higher tag return rates they induce. The high-reward tagging method is an important method for estimating the tag reporting rate for standard tags. For this method it is assumed that high-reward tags are reported 100\% of the time. This assumption is investigated. Other assumptions of the method are also considered, and particular attention is paid to whether the reporting rate of standard tags may change when a high-reward tagging study is initiated. This is of particular concern in cases in which standard tags are used for all study years and high-reward tags are only used in some subset of the study years. If the natural mortality rate is assumed to be constant over all years, then fishing and natural mortality together with two tag reporting rates can be estimated. Simulation analysis shows that fishing mortality estimates are unbiased in this case but have significantly higher coefficients of variation in the years without high-reward tags. Natural mortality estimates are unbiased and reasonably efficient, but this is crucially dependent on the assumption that natural mortality is constant over time. We make detailed recommendations for improving the design of reward tagging studies in general.},
  copyright = {{\copyright} 2001 American Fisheries Society},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\2T4CV9PJ\\Pollock et al. - 2001 - Tag Reporting Rate Estimation 1. An Evaluation of.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\DS67LYZU\\1548-8675(2001)0210521TRREAE2.0.html}
}

@misc{pollock.pine_2007,
  title = {The Design and Analysis of Field Studies to Estimate Catch and Release Mortality. {{Fisheries Management}} and {{Ecology}}},
  author = {Pollock, K. H. and Pine, W. E. P.},
  year = {2007},
  abstract = {mortality},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\ZP4M8JIG\\Pollock and Ine - 2007 - The design and analysis of field studies to estima.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\ST7K57MZ\\download.html}
}

@article{pollock.pine2007,
  title = {The Design and Analysis of Field Studies to Estimate Catch-and-release Mortality},
  author = {Pollock, Kenneth and Pine, William},
  year = {2007},
  month = apr,
  journal = {Fisheries Management and Ecology},
  volume = {14},
  pages = {123--130},
  doi = {10.1111/j.1365-2400.2007.00532.x},
  abstract = {Abstract The practice of catch and release (CR) as a fisheries management tool to reduce fishing mortality is widely applied in both freshwater and marine fisheries, whether from shifts in angler attitudes related to harvest or from the increasing use of harvest restrictions such as closed seasons or length limits. This approach assumes that for CR fishing policies to benefit the stock, CR will result in much lower mortality than would otherwise occur. There are many challenges in the design of CR studies to assess mortality, and in many practical settings it is difficult to obtain accurate and precise estimates. The focus of this article is on the design and quantitative aspects of estimating CR mortality, the need for a comprehensive approach that explicitly states all components of CR mortality, and the assumptions behind these methods. A general conceptual model for CR mortality that is applicable to containment and tagging-based studies with a slight modification is presented. This article reviews the design and analysis of containment and tagging studies to estimate CR mortality over both the short and long term and then compares these two approaches. Additionally, the potential population-level impacts of CR mortality are discussed. A recurring theme is the difficulty of designing studies to estimate CR mortality comprehensively and the need for additional research into both statistical model development and field study design.}
}

@article{ponisio.etal_2020,
  title = {One Size Does Not Fit All: {{Customizing MCMC}} Methods for Hierarchical Models Using {{NIMBLE}}},
  shorttitle = {One Size Does Not Fit All},
  author = {Ponisio, Lauren C. and {de Valpine}, Perry and Michaud, Nicholas and Turek, Daniel},
  year = {2020},
  journal = {Ecology and Evolution},
  volume = {10},
  number = {5},
  pages = {2385--2416},
  issn = {2045-7758},
  doi = {10.1002/ece3.6053},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6053},
  urldate = {2024-03-26},
  abstract = {Improved efficiency of Markov chain Monte Carlo facilitates all aspects of statistical analysis with Bayesian hierarchical models. Identifying strategies to improve MCMC performance is becoming increasingly crucial as the complexity of models, and the run times to fit them, increases. We evaluate different strategies for improving MCMC efficiency using the open-source software NIMBLE (R package nimble) using common ecological models of species occurrence and abundance as examples. We ask how MCMC efficiency depends on model formulation, model size, data, and sampling strategy. For multiseason and/or multispecies occupancy models and for N-mixture models, we compare the efficiency of sampling discrete latent states vs. integrating over them, including more vs. fewer hierarchical model components, and univariate vs. block-sampling methods. We include the common MCMC tool JAGS in comparisons. For simple models, there is little practical difference between computational approaches. As model complexity increases, there are strong interactions between model formulation and sampling strategy on MCMC efficiency. There is no one-size-fits-all best strategy, but rather problem-specific best strategies related to model structure and type. In all but the simplest cases, NIMBLE's default or customized performance achieves much higher efficiency than JAGS. In the two most complex examples, NIMBLE was 10--12 times more efficient than JAGS. We find NIMBLE is a valuable tool for many ecologists utilizing Bayesian inference, particularly for complex models where JAGS is prohibitively slow. Our results highlight the need for more guidelines and customizable approaches to fit hierarchical models to ensure practitioners can make the most of occupancy and other hierarchical models. By implementing model-generic MCMC procedures in open-source software, including the NIMBLE extensions for integrating over latent states (implemented in the R package nimbleEcology), we have made progress toward this aim.},
  langid = {english},
  keywords = {dynamic occupancy,latent states,Markov chain Monte Carlo,multispecies occupancy,N-mixture},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\VNEFRDQ3\\Ponisio et al. - 2020 - One size does not fit all Customizing MCMC method.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\TGWCXZNT\\ece3.html}
}

@article{pregler.etal_2019,
  title = {State-Space Analysis of Power to Detect Regional Brook Trout Population Trends over Time},
  author = {Pregler, Kasey C. and Hanks, R. Daniel and Childress, Evan S. and Hitt, Nathaniel P. and Hocking, Daniel J. and Letcher, Benjamin H. and Wagner, Tyler and Kanno, Yoichiro},
  year = {2019},
  month = nov,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {76},
  number = {11},
  pages = {2145--2155},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/cjfas-2018-0241},
  url = {https://cdnsciencepub.com/doi/full/10.1139/cjfas-2018-0241},
  urldate = {2023-03-27},
  abstract = {Threats to aquatic biodiversity are expressed at broad spatial scales, but identifying regional trends in abundance is challenging owing to variable sampling designs and temporal and spatial variation in abundance. We compiled a regional data set of brook trout (Salvelinus fontinalis) counts across their southern range representing 326 sites from eight states between 1982 and 2014 and conducted a statistical power analysis using Bayesian state-space models to evaluate the ability to detect temporal trends by characterizing posterior distributions with three approaches. A combination of monitoring periods, number of sites and electrofishing passes, decline magnitude, and different revisit patterns were tested. Power increased with monitoring periods and decline magnitude. Trends in adults were better detected than young-of-the-year fish, which showed greater interannual variation in abundance. The addition of weather covariates to account for the temporal variation increased power only slightly. Single- and three-pass electrofishing methods were similar in power. Finally, power was higher for sampling designs with more frequent revisits over the duration of the monitoring program. Our results provide guidance for broad-scale monitoring designs for temporal trend detection.},
  file = {C:\Users\jhncsu\Zotero\storage\47AF2S6J\Pregler et al. - 2019 - State-space analysis of power to detect regional b.pdf}
}

@book{quinn.deriso_1999,
  title = {Quantitative {{Fish Dynamics}}},
  author = {Quinn, Terrance J. and Deriso, Richard B.},
  year = {1999},
  month = mar,
  series = {Biological {{Resource Management}}},
  publisher = {Oxford University Press},
  address = {Oxford, New York},
  abstract = {This book serves as an advanced text on fisheries and fishery population dynamics and as a reference for fisheries scientists. It provides a thorough treatment of contemporary topics in quantitative fisheries science and emphasizes the link between biology and theory by explaining the assumptions inherent in the quantitative methods. The analytical methods are accessible to a wide range of biologists, and the book includes numerous examples. The book is unique in covering such advanced topics as optimal harvesting, migratory stocks, age-structured models, and size models.              ,                This book serves as an advanced text on fisheries and fishery population dynamics and as a reference for fisheries scientists. It provides a thorough treatment of contemporary topics in quantitative fisheries science and emphasizes the link between biology and theory by explaining the assumptions inherent in the quantitative methods. The analytical methods are accessible to a wide range of biologists, and the book includes numerous examples. The book is unique in covering such advanced topics as optimal harvesting, migratory stocks, age-structured models, and size models.},
  isbn = {978-0-19-507631-8},
  file = {C:\Users\jhncsu\Zotero\storage\L3AVLF73\quantitative-fish-dynamics-9780195076318.html}
}

@article{regehr.etal_2018,
  title = {Integrated {{Population Modeling Provides}} the {{First Empirical Estimates}} of {{Vital Rates}} and {{Abundance}} for {{Polar Bears}} in the {{Chukchi Sea}}},
  author = {Regehr, Eric V. and Hostetter, Nathan J. and Wilson, Ryan R. and Rode, Karyn D. and Martin, Michelle St and Converse, Sarah J.},
  year = {2018},
  month = nov,
  journal = {Scientific Reports},
  volume = {8},
  pages = {16780},
  doi = {10.1038/s41598-018-34824-7},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC6235872/},
  urldate = {2024-11-05},
  abstract = {Large carnivores are imperiled globally, and characteristics making them vulnerable to extinction (e.g., low densities and expansive ranges) also make it difficult to estimate demographic parameters needed for management. Here we develop an ...},
  langid = {english},
  pmid = {30429493},
  file = {C:\Users\jhncsu\Zotero\storage\DUCK97DH\Regehr et al. - 2018 - Integrated Population Modeling Provides the First .pdf}
}

@book{ricker1975,
  title = {Computation and Interpretation of Biological Statistics of Fish Populations},
  author = {Ricker, William Edwin},
  year = {1975},
  series = {Bulletin},
  number = {191},
  publisher = {{Dept. of Fisheries and Oceans : Minister of Supply and Services Canada}},
  address = {Ottawa},
  collaborator = {Canada and Canada},
  isbn = {978-0-662-01440-9},
  lccn = {SH223 B6b9 no. 191},
  keywords = {Fish populations,Sampling (Statistics),Statistical methods},
  annotation = {OCLC: ocm20967850}
}

@article{robson.regier1964,
  title = {Sample {{Size}} in {{Petersen Mark}}--{{Recapture Experiments}}},
  author = {Robson, D. S. and Regier, H. A.},
  year = {1964},
  journal = {Transactions of the American Fisheries Society},
  volume = {93},
  number = {3},
  pages = {215--226},
  issn = {1548-8659},
  doi = {10.1577/1548-8659(1964)93[215:SSIPME]2.0.CO;2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1577/1548-8659%281964%2993%5B215%3ASSIPME%5D2.0.CO%3B2},
  urldate = {2022-01-27},
  abstract = {The efficient planning of a Petersen-type mark and recapture experiment requires some knowledge of the order of magnitude of the population size N. Sample sizes M and C of the mark and recapture samples, respectively, may then be ascertained on the basis of a guessed value of N to achieve any desired degree of accuracy with any specified degree of confidence. Restrictions on the sample sizes M and C are that MC must exceed 4 times the guessed value of N, and the total costs of M and C must be equal. Graphs and formulas are given defining sample size to attain preassigned levels of accuracy and precision of population estimation. A method of choosing sample sizes such that experimental costs are minimized is described.},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\BTLL3EQF\1548-8659(1964)93[215SSIPME]2.0.html}
}

@book{ross_2022,
  title = {An {{Introduction}} to {{Bayesian Reasoning}} and {{Methods}}},
  author = {Ross, Kevin},
  year = {2022},
  month = mar,
  url = {https://bookdown.org/kevin_davisross/bayesian-reasoning-and-methods/},
  urldate = {2024-10-27},
  abstract = {This textbook presents an introduction to Bayesian reasoning and methods},
  file = {C:\Users\jhncsu\Zotero\storage\Q9T97N3B\bayesian-reasoning-and-methods.html}
}

@article{royle.dorazio_2008,
  title = {Hierarchical Modeling and Inference in Ecology: {{The}} Analysis of Data from Populations, Metapopulations and Communities},
  shorttitle = {Hierarchical Modeling and Inference in Ecology},
  author = {Royle, J. Andrew and Dorazio, Robert M.},
  year = {2008},
  doi = {10.1016/B978-0-12-374097-7.50001-5},
  url = {https://pubs.usgs.gov/publication/5200344},
  urldate = {2024-11-18},
  abstract = {A guide to data collection, modeling and inference strategies for biological survey data using Bayesian and classical statistical methods. This book describes a general and flexible framework for modeling and inference in ecological systems based on hierarchical models, with a strict focus on the use of probability models and parametric inference. Hierarchical models represent a paradigm shift in the application of statistics to ecological inference problems because they combine explicit models of ecological system structure or dynamics with models of how ecological systems are observed. The principles of hierarchical modeling are developed and applied to problems in population, metapopulation, community, and metacommunity systems. The book provides the first synthetic treatment of many recent methodological advances in ecological modeling and unifies disparate methods and procedures. The authors apply principles of hierarchical modeling to ecological problems, including * occurrence or occupancy models for estimating species distribution * abundance models based on many...},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\SZ9I4ND4\5200344.html}
}

@article{schaub.etal_2024,
  title = {Lessons to Be Learned by Comparing Integrated Fisheries Stock Assessment Models ({{SAMs}}) with Integrated Population Models ({{IPMs}})},
  author = {Schaub, Michael and Maunder, Mark N. and K{\'e}ry, Marc and Thorson, James T. and Jacobson, Eiren K. and Punt, Andr{\'e} E.},
  year = {2024},
  month = apr,
  journal = {Fisheries Research},
  volume = {272},
  pages = {106925},
  issn = {0165-7836},
  doi = {10.1016/j.fishres.2023.106925},
  url = {https://www.sciencedirect.com/science/article/pii/S0165783623003181},
  urldate = {2024-02-05},
  abstract = {Integrated fisheries stock assessment models (SAMs) and integrated population models (IPMs) are used in biological and ecological systems to estimate abundance and demographic rates. The approaches are fundamentally very similar, but historically have been considered as separate endeavors, resulting in a loss of shared vision, practice and progress. We review the two approaches to identify similarities and differences, with a view to identifying key lessons that would benefit more generally the overarching topic of population ecology. We present a case study for each of SAM (snapper from the west coast of New Zealand) and IPM (woodchat shrikes from Germany) to highlight differences and similarities. The key differences between SAMs and IPMs appear to be the objectives and parameter estimates required to meet these objectives, the size and spatial scale of the populations, and the differing availability of various types of data. In addition, up to now, typical SAMs have been applied in aquatic habitats, while most IPMs stem from terrestrial habitats. SAMs generally aim to assess the level of sustainable exploitation of fish populations, so absolute abundance or biomass must be estimated, although some estimate only relative trends. Relative abundance is often sufficient to understand population dynamics and inform conservation actions, which is the main objective of IPMs. IPMs are often applied to small populations of conservation concern, where demographic uncertainty can be important, which is more conveniently implemented using Bayesian approaches. IPMs are typically applied at small to moderate spatial scales (1 to 104 km2), with the possibility of collecting detailed longitudinal individual data, whereas SAMs are typically applied to large, economically valuable fish stocks at very large spatial scales (104 to 106 km2) with limited possibility of collecting detailed individual data. There is a sense in which a SAM is more data- (or information-) hungry than an IPM because of its goal to estimate absolute biomass or abundance, and data at the individual level to inform demographic rates are more difficult to obtain in the (often marine) systems where most SAMs are applied. SAMs therefore require more 'tuning' or assumptions than IPMs, where the 'data speak for themselves', and consequently techniques such as data weighting and model evaluation are more nuanced for SAMs than for IPMs. SAMs would benefit from being fit to more disaggregated data to quantify spatial and individual variation and allow richer inference on demographic processes. IPMs would benefit from more attempts to estimate absolute abundance, for example by using unconditional models for capture-recapture data.},
  keywords = {Data integration,Management,Parameter estimation,Population dynamics,Population model,Uncertainty},
  file = {C:\Users\jhncsu\Zotero\storage\8T7MY84K\S0165783623003181.html}
}

@article{scherrer.etal2021,
  title = {Estimation of Growth Parameters Integrating Tag-Recapture, Length-Frequency, and Direct Aging Data Using Likelihood and {{Bayesian}} Methods for the Tropical Deepwater Snapper {{Pristipomoides}} Filamentosus in {{Hawaii}}},
  author = {Scherrer, Stephen R. and Kobayashi, Donald R. and Weng, Kevin C. and Okamoto, Henry Y. and Oishi, Francis G. and Franklin, Erik C.},
  year = {2021},
  month = jan,
  journal = {Fisheries Research},
  volume = {233},
  pages = {105753},
  issn = {01657836},
  doi = {10.1016/j.fishres.2020.105753},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0165783620302708},
  urldate = {2023-02-07},
  abstract = {Pristipomoides filamentosus is an economically and culturally important species of deepwater snapper found throughout the tropical Indo-Pacific. From 1989--1993, the State of Hawaii initiated a tagging program with fish opportunistically recaptured by scientists and fishers to quantify growth and other life history parameters. Over approximately 10 years, 10.5 \% of 4179 tagged P. filamentosus were recaptured. We used these data to compare von Bertalanffy growth parameters estimated using Bayesian and likelihood approaches. Next, we defined an objective cost function to estimate growth parameters that integrated the tagging data with direct aging and length frequency data used in previous regional growth studies. Our results reconcile 30+ years of effort from various methods to estimate growth parameters for P. filametosus in Hawaii (L{$\infty$} = 68.14 cm FL [95 \% Confidence Interval (CI): 65.42--69.54] and K = 0.22 yr- 1 [CI: 0.20--0.25]), demonstrate the importance of individual variability in the species due primarily to the asymptotic length parameter L{$\infty$}, and suggest the effects of sexual dimorphism on growth as a focus of future inquiry. These results have direct management implications as growth is a critical input for age-based stock assessment models and often used as a proxy for other life history traits.},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\QTXTQW95\Scherrer et al. - 2021 - Estimation of growth parameters integrating tag-re.pdf}
}

@article{scherrer.etal2021a,
  title = {Estimation of Growth Parameters Integrating Tag-Recapture, Length-Frequency, and Direct Aging Data Using Likelihood and {{Bayesian}} Methods for the Tropical Deepwater Snapper {{Pristipomoides}} Filamentosus in {{Hawaii}}},
  author = {Scherrer, Stephen R. and Kobayashi, Donald R. and Weng, Kevin C. and Okamoto, Henry Y. and Oishi, Francis G. and Franklin, Erik C.},
  year = {2021},
  month = jan,
  journal = {Fisheries Research},
  volume = {233},
  pages = {105753},
  issn = {0165-7836},
  doi = {10.1016/j.fishres.2020.105753},
  url = {https://www.sciencedirect.com/science/article/pii/S0165783620302708},
  urldate = {2023-02-16},
  abstract = {Pristipomoides filamentosus is an economically and culturally important species of deepwater snapper found throughout the tropical Indo-Pacific. From 1989--1993, the State of Hawaii initiated a tagging program with fish opportunistically recaptured by scientists and fishers to quantify growth and other life history parameters. Over approximately 10 years, 10.5 \% of 4179 tagged P. filamentosus were recaptured. We used these data to compare von Bertalanffy growth parameters estimated using Bayesian and likelihood approaches. Next, we defined an objective cost function to estimate growth parameters that integrated the tagging data with direct aging and length frequency data used in previous regional growth studies. Our results reconcile 30+ years of effort from various methods to estimate growth parameters for P. filametosus in Hawaii (L{$\infty$} = 68.14 cm FL [95 \% Confidence Interval (CI): 65.42--69.54] and K = 0.22 yr-1 [CI: 0.20--0.25]), demonstrate the importance of individual variability in the species due primarily to the asymptotic length parameter L{$\infty$}, and suggest the effects of sexual dimorphism on growth as a focus of future inquiry. These results have direct management implications as growth is a critical input for age-based stock assessment models and often used as a proxy for other life history traits.},
  langid = {english},
  keywords = {Bayesian,Integrated model,Maximum Likelihood,von Bertalanffy growth},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\EGN6V7DE\\Scherrer et al. - 2021 - Estimation of growth parameters integrating tag-re.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\7RIL42JI\\S0165783620302708.html}
}

@article{spiegelhalter.etal_2002,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Van Der Linde, Angelika},
  year = {2002},
  journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
  volume = {64},
  number = {4},
  pages = {583--639},
  publisher = {Blackwell Publishers},
  address = {Oxford, UK},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00353},
  url = {https://ncsu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1Lb9NAEB6h5lIJ8SzCPIwPCHExtb12vHtsLNKeKiCgCi6rXXutltIkil1U_j0z49gkERIgDo6daO1NZue1k5lvAETyJgp3dEJVoRqMFbIH2gM0gSbKnaxKpSpT12m2E9vp2p5SaUwHFzHE30hQWH2TvBvb9DJ_yPKu5JgSI0VG4J84C0HpTz7LTcUsujIsFaIhTdZIP7-5f8tIjYjeN32-IiVPmgbpV3eNL7Y809HKLV25Y8O23V62W9O7cNn_RE5XoSYJm3n7h4yVyAAZPS7kf5DjHtxZu7fBUceP9-GWmz-AffJoO0DohxBOzA9HlZvBVReebIJFHXBDnoDz290NbgwCM6-C-qI9gE_Ttx-Lk3DdsyEsM_QckK45dbNGyytLXNfI1lmCLowjYLDa5TW-UbaWUSVlakuZmTjJq8hJAj5F5WzFI7htKLd_3nINYPUYAvT_jcDnlMQrqXQqtdZkeexspErhEg9e96ullx1Gh-73NkQJTZTQTAkPXvFqDuPM6pJS2_JMn50e6_dnJyKevTvWUw8OeLmHgQLVsYzGHvhby_9rQvL0kOc8eNHzg0ZJpb9fzNwtrhstFNUp5_h1C2aT4Vb7zXxdrJrG6u9amHGKLzifofQZPF3gQR8t8cik0OhS6vP2yoOXm0w2PIwRkFRC-3q69CD-m2HFGhCegBBaD0JmrT8RU3-YzSZ4fvKP45_CPvfU4UDWM9hrV9fuOewR7_swOiqKL6c-S67PkRafUa9_ApW0Qv8},
  urldate = {2023-12-06},
  abstract = {We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure P-D for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general P-D approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding P-D to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.;We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.;We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure "p""D" for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general "p""D" approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding "p""D" to the posterior mean deviance gives a "deviance information criterion" for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis. Copyright 2002 Royal Statistical Society.;Summary We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.;We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pDfor the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pDapproximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pDto the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.;},
  langid = {english},
  keywords = {Approximation,Bayesian method,Bayesian model comparison,Bayesian networks,Decision theory,Deviance,Deviance information criterion,Effective number of parameters,Estimators,Exact sciences and technology,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Markovian processes,Mathematics,Model dimension,Modeling,Monte Carlo simulation,Multilevel models,Parameterization,Parametric models,Physical Sciences,Predictive modeling,Probability and statistics,Sample size,Science & Technology,Sciences and techniques of general use,Spatial models,Statistical methods,Statistical models,Statistics,Statistics & Probability},
  file = {C:\Users\jhncsu\Zotero\storage\AJYLK79X\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf}
}

@article{stich.etal_2015,
  title = {Life, {{Death}}, and {{Resurrection}}: {{Accounting}} for {{State Uncertainty}} in {{Survival Estimation}} from {{Tagged Grass Carp}}},
  shorttitle = {Life, {{Death}}, and {{Resurrection}}},
  author = {Stich, Daniel S. and Jiao, Yan and Murphy, Brian R.},
  year = {2015},
  journal = {North American Journal of Fisheries Management},
  volume = {35},
  number = {2},
  pages = {321--330},
  issn = {1548-8675},
  doi = {10.1080/02755947.2014.996685},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/02755947.2014.996685},
  urldate = {2024-01-14},
  abstract = {Information about Grass Carp Ctenopharyngodon idella survival would be useful for improving the management of fish used for aquatic weed control. Reliable methods for estimating annual poststocking survival of Grass Carp from radiotelemetry data do not exist because the fish remain sedentary for prolonged periods between movements, giving the false impression of death, only to be observed alive (i.e., ``resurrected'') at a later date. We constructed a state-space, multistate mark--recapture survival model accounting for uncertainty in the live/dead states of tagged Grass Carp in a large (8,500 ha) reservoir, and we estimated monthly and annual survival. Model results were compared with life history-based methods for estimating survival, and survival estimates that were corrected for state misclassification were compared with uncorrected estimates. Corrected estimates of annual survival (mean = 0.23; 95\% credible interval [CRI] = 0.15--0.41) contained less bias than uncorrected estimates (0.12; 95\% CRI = 0.08--0.18). However, both corrected and uncorrected estimates were substantially lower than the survival expected based on life history theory (mean = 0.69; 95\% confidence interval = 0.52--0.78), suggesting that mark--recapture survival estimates for Grass Carp might be negatively biased due to tag shedding, tag-related mortality, or both. Our model effectively reduced bias in monthly and annual survival estimates due to state misclassification, illustrating the potential for application of existing mark--recapture frameworks to estimate Grass Carp survival with telemetry data, despite the behavioral idiosyncrasies of the species. Furthermore, these methods may have application for studies of other animals that undergo periodic quiescence between movements, such as salmonids, ictalurids, and reef fishes. To account for bias resulting from tag loss, future mark--recapture studies of Grass Carp could incorporate tag shedding rates within the framework developed here. Received July 19, 2014; accepted December 3, 2014},
  copyright = {{\copyright} 2015 American Fisheries Society},
  langid = {english},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\PI3I9GWB\\Stich et al. - 2015 - Life, Death, and Resurrection Accounting for Stat.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\YSPDAC93\\02755947.2014.html}
}

@article{then.etal_2015,
  title = {Evaluating the Predictive Performance of Empirical Estimators of Natural Mortality Rate Using Information on over 200 Fish Species},
  author = {Then, Amy Y and Hoenig, John M and Hall, Norman G and Hewitt, David A and {Handling editor: Ernesto Jardim}},
  year = {2015},
  month = jan,
  journal = {ICES Journal of Marine Science},
  volume = {72},
  number = {1},
  pages = {82--92},
  issn = {1054-3139},
  doi = {10.1093/icesjms/fsu136},
  url = {https://doi.org/10.1093/icesjms/fsu136},
  urldate = {2024-10-22},
  abstract = {Many methods have been developed in the last 70 years to predict the natural mortality rate, M, of a stock based on empirical evidence from comparative life history studies. These indirect or empirical methods are used in most stock assessments to (i) obtain estimates of M in the absence of direct information, (ii) check on the reasonableness of a direct estimate of M, (iii) examine the range of plausible M estimates for the stock under consideration, and (iv) define prior distributions for Bayesian analyses. The two most cited empirical methods have appeared in the literature over 2500 times to date. Despite the importance of these methods, there is no consensus in the literature on how well these methods work in terms of prediction error or how their performance may be ranked. We evaluate estimators based on various combinations of maximum age (tmax), growth parameters, and water temperature by seeing how well they reproduce \&gt;200 independent, direct estimates of M. We use tenfold cross-validation to estimate the prediction error of the estimators and to rank their performance. With updated and carefully reviewed data, we conclude that a tmax-based estimator performs the best among all estimators evaluated. The tmax-based estimators in turn perform better than the Alverson--Carney method based on tmax and the von Bertalanffy K coefficient, Pauly's method based on growth parameters and water temperature and methods based just on K. It is possible to combine two independent methods by computing a weighted mean but the improvement over the tmax-based methods is slight. Based on cross-validation prediction error, model residual patterns, model parsimony, and biological considerations, we recommend the use of a tmax-based estimator (M=4.899tmax-0.916, prediction error = 0.32) when possible and a growth-based method (M=4.118K0.73L{$\infty-$}0.33 , prediction error = 0.6, length in cm) otherwise.},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\M8HE8KI8\\Then et al. - 2015 - Evaluating the predictive performance of empirical.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\LLBTBGUA\\2804320.html}
}

@article{vetter1988,
  title = {Estimation of {{Natural Mortality}} in {{Fish Stocks}} - a {{Review}}},
  author = {Vetter, Ef},
  year = {1988},
  month = jan,
  journal = {Fishery Bulletin},
  volume = {86},
  number = {1},
  pages = {25--43},
  publisher = {Natl Marine Fisheries Service Scientific Publ Office},
  address = {Seattle},
  issn = {0090-0656},
  url = {http://www.webofscience.com/wos/woscc/full-record/WOS:A1988P107100002},
  urldate = {2022-05-02},
  langid = {english},
  annotation = {WOS:A1988P107100002}
}

@article{wang.etal1995,
  title = {A Maximum Likelihood Approach for Estimating Growth from Tag--Recapture Data},
  author = {Wang, You-Gan and Thomas, Mervyn R. and Somers, Ian F.},
  year = {1995},
  month = feb,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {52},
  number = {2},
  pages = {252--259},
  issn = {0706-652X, 1205-7533},
  doi = {10.1139/f95-025},
  url = {http://www.nrcresearchpress.com/doi/10.1139/f95-025},
  urldate = {2023-03-07},
  abstract = {The Fabens method is commonly used to estimate growth parameters \&I and 1, in the von Bertalanffy model from tag-recapture data. However, the Fabens method s f estimation has an inherent bias when individual growth is variable. This paper presents an asymptotically unbiassed method using a maximum likelihood approach that takes account s f individual variability in both maximum length and age-at-tagging. It is assumed that each individual's growth follows a von Bertalanffy curve with its own maximum length and age-at-tagging. The parameter k is assumed to be a constant to ensure that the mean growth follows a von Bertalanffy curve and to avoid overparameterizatkm. Our method also makes more efficient use sf the measurements at tag and recapture and includes diagnostic techniques for checking distributional assumptions. The method is reasonably robust and performs better than the Fabens method when individual growth differs from the von Bertalanffy relationship. When measurement error is negligible, the estimation involves maximizing the profile likelihood of one parameter only. The method is applied to tag-recapture data for the grooved tiger prawn (Penaeus semisukcatus) from the Gulf of Carpentaria, Australia.},
  langid = {english},
  file = {C:\Users\jhncsu\Zotero\storage\559SLQIM\Wang et al. - 1995 - A maximum likelihood approach for estimating growt.pdf}
}

@article{wang1997,
  title = {Comment: {{Efficiency}} and Bias of Estimators and Sampling Designs for Determining Lengthweight Relationships of Fishes},
  shorttitle = {Comment},
  author = {Wang, Y -G},
  year = {1997},
  month = mar,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {54},
  number = {3},
  pages = {742--743},
  publisher = {NRC Research Press},
  issn = {0706-652X},
  doi = {10.1139/f96-298},
  url = {https://cdnsciencepub.com/doi/abs/10.1139/f96-298},
  urldate = {2023-02-20},
  file = {C:\Users\jhncsu\Zotero\storage\X25TGGNS\Wang - 1997 - Comment Efficiency and bias of estimators and sam.pdf}
}

@book{xie2015,
  title = {Dynamic {{Documents}} with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {2nd},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida},
  url = {http://yihui.name/knitr/}
}

@article{zhang.etal2009,
  title = {Use of {{Bayesian}} Hierarchical Models to Estimate Northern Abalone, {{Haliotis}} Kamtschatkana, Growth Parameters from Tag-Recapture Data},
  author = {Zhang, Zane and Lessard, Joanne and Campbell, Alan},
  year = {2009},
  month = jan,
  journal = {Fisheries Research},
  volume = {95},
  number = {2},
  pages = {289--295},
  issn = {0165-7836},
  doi = {10.1016/j.fishres.2008.09.035},
  url = {https://www.sciencedirect.com/science/article/pii/S0165783608003184},
  urldate = {2023-02-16},
  abstract = {Bayesian hierarchical models were developed to estimate the growth parameters of northern abalone, Haliotis kamtschatkana, using tag-recapture data with a mixture of single and multiple recaptures. Individual variability in the growth parameters L{$\infty$} and k of the von Bertalanffy model was incorporated in the analyses. The models developed fit the data well based on the Bayesian p-values. Variability in L{$\infty$} for individuals was high relative to the variability in L{$\infty$} for the population, and variability in k for individuals was about the same as the variability in k for the population. Simulations showed that estimates of the growth parameters were accurate (relative biases {$<$}5\%), when variability in both L{$\infty$} and k or just in L{$\infty$} was accounted for. The ``true'' values of the parameters, L{$\infty$} and k, were contained in the estimated 95\% credibility intervals in 90--94 out of 100 simulation runs on 100 simulated data sets. Overall, allowing for variability for both L{$\infty$} and k resulted in moderately more accurate estimates than allowing for just L{$\infty$}. On the contrary, estimates were unreliable when variability in just k was considered. Using the WinBUGS software program, the calculation procedure was rather simple irrespective of which growth parameter was modeled with variability.},
  langid = {english},
  keywords = {Abalone,Bayesian,Growth,Hierarchical,Simulation,Variability},
  file = {C\:\\Users\\jhncsu\\Zotero\\storage\\4VZRC2SA\\Zhang et al. - 2009 - Use of Bayesian hierarchical models to estimate no.pdf;C\:\\Users\\jhncsu\\Zotero\\storage\\4DL2I4AQ\\S0165783608003184.html}
}
