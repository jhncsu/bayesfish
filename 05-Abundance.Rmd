# Abundance {#Abundance}

Information about abundance can be invaluable for making policy decisions. For fish populations that support commercial or recreational fishing, that information could determine the appropriate level of harvest. For rare or declining populations, the information might be used to develop a restoration plan. It is rarely possible to conduct a census (counting every individual in a target population), so models are used to account for incomplete sampling. For example, we would expect to capture only a fraction of the target population even if we sampled the entire shoreline of a lake using electrofishing. The probability of capturing an individual fish during a sampling event is referred to as the capture probability\index{capture probability}. It can also be thought of as the sampling fraction, or the fraction of the target population that is caught or counted [@williams.etal_2002]. In some cases, the probability represents detection rather than capture, for example, the probability of detecting an individual fish with a transmitter during a search. Estimating capture or detection probabililties allows us to correct for imperfect sampling in order to estimate population parameters of interest [@williams.etal_2002].

In this chapter, we consider a few simple field approaches for estimating population size. These methods can be used in freshwater or marine systems, but they do require that the study area be closed (no migration in or out, no recruitment\index{recruitment} (additions through reproduction), and no mortality for the duration of the study). Later chapters will consider open models that jointly estimate population abundance and mortality over longer time frames.

## Removal {#Removal}

The removal method\index{removal method} is a good approach for estimating population size within a study area when a substantial fraction of the study population can be captured in a single sample. One common example is use of backpack electrofishing in streams that can be blocked on both ends to provide a temporarily closed system. Fish captured in each sample (or "pass") are held out temporarily and the declining catches from the diminishing population provide the data for estimating abundance. Consider a case with a true (study area) population of 100 and a capture probability\index{capture probability} of 0.4. The expected sequence of catches would be 40 (100 * 0.4), 24 ((100-40) * 0.4), 14.4 ((60-24)*0.4), etc. A model fit to these data provides estimates of initial population size and capture probability\index{capture probability} that best mimic the pattern of catches. The fit would be very good if we used the above expected values, but it is more challenging (and realistic) when the catch sequence includes random variation.

We illustrate the removal method\index{removal method} using a simulation to generate sample data. Here the true values are arbitrary, but roughly similar to estimates obtained by @bryant_2000 for Alaska streams. If planning a field study, your assumed population level should be based on prior work or literature review. The assumed capture probability\index{capture probability} should also be realistic, but it is a bit different from population size in that it is under the investigator's control. Thus, a better estimate can be achieved by increasing sampling effort; for example, by using more electrofishing units or setting more traps. The other study design variable controlled by the investigator is the number of samples. Here we use five removal samples, which should provide high quality results.

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Generate simulated removals
N.removals <- 5 # Number of removal samples
N.true <- 100    # True population size
CapProb <- 0.4   # Capture probability
N.remaining <- N.true # Initialize N.remaining, before removals begin
Catch <- numeric() # Creates empty vector for storing catch data
for (j in 1:N.removals){
  Catch[j] <- rbinom(1, N.remaining, CapProb)
  N.remaining <- N.remaining - Catch[j]
  } #j
```

Running the above lines of code generates the sequence of simulated catches. (Note that these lines of simulation code would be replaced by an assignment statement (e.g., <code>Catch <- c(14, 4, 6, 2, 3)</code>) if we were analyzing data from an actual field study). A 'for' loop is used to step through the simulated sampling sequence. For each value of j, we generate a binomially-distributed\index{binomial distribution} random catch using the <code>rbinom()</code> function, then remove that number of fish from the remaining population. One example run produced a catch vector of 36, 25, 20, 6 and 6. The high assumed value for capture probability\index{capture probability} produces very good data; thus, these values are close to what would be expected. Try running the code using other values for capture probability. Is there a lower bound below which you would not expect the method to work well?

\newpage
The next step is to fit the removal model using JAGS. The prior distribution\index{prior distribution} for capture probability\index{capture probability} is straightforward (uniform\index{uniform distribution} 0-1), but the choice is a bit more subjective for population size (N.est). We know that N.est cannot be less than the total catch, so we can use a slightly informative prior\index{informative prior distribution} (with an arbitrary high value for the upper bound). Results from test runs were similar when using 0 as the lower bound, and when different values were used for the upper bound. Another alternative that provides similar results is the vague lognormal prior used in Section \@ref(FirstExampleBayesian). 

```{r eval=FALSE}

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code for fitting model
sink("RemovalModel.txt")
  cat("
model{

# Priors
 CapProb.est ~ dunif(0,1)
 N.est ~ dunif(TotalCatch, 2000)

 N.remaining[1] <- trunc(N.est)
 for(j in 1:N.removals){
      Catch[j]~dbin(CapProb.est, N.remaining[j]) # jth removal
      N.remaining[j+1] <- N.remaining[j]-Catch[j]
  } #j

}
      ",fill=TRUE)
  sink()

     # Bundle data
  TotalCatch <- sum(Catch[])
  jags.data <- list("Catch", "N.removals", "TotalCatch")


  # Initial values
  jags.inits <- function(){ list(CapProb.est=runif(1, min=0, max=1),
                                  N.est=runif(n=1, min=TotalCatch,
                                  max=2000))}

  model.file <- 'RemovalModel.txt'

  # Parameters monitored
  jags.params <- c("N.est", "CapProb.est")

   # Call JAGS from R
  jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                  n.chains = 3, n.thin = 1, n.iter = 40000,
                  model.file)
  print(jagsfit)
  plot(jagsfit)
```

JAGS requires an integer size variable in <code>dbin()</code>, so we use the <code>trunc()</code> function to truncate the population estimate at time 1 (N.remaining[1]). The looping code is the same as in the simulation, except that JAGS does not allow a value to be repeatedly changed in a loop, so we set up the remaining population size as a vector. We provide to JAGS the simulated catch vector, the number of removal samples, and the total catch. Initial values are generated using the same distributions as the priors. We monitor the estimates of population size and capture probability.

Initial runs using fewer updates occasionally resulted in poor convergence, so n.iter was increased to 40000. Removing n.burnin uses the default of discarding half the updates. Running the simulation and analysis code multiple times for these simulation parameters shows that the estimates are consistently good, with narrow credible intervals. An example using fixed catch values is included as Appendix \@ref(RemovalRealData).

As a fishery biologist, you could use this combination of simulation and analysis to plan your field study. You could look at different choices for the two design variables (number of removal samples, capture probability\index{capture probability}) to decide what would produce reliable results. @white.etal_1982 suggest that capture probability\index{capture probability} should be at least 0.2, with better results obtained at 0.4 or higher with 3-6 removal samples. Try runs using the above code, modified to test different numbers of removal samples and assumed capture probabilities. It is important to acknowledge that this is an optimistic case, in that the data exactly follow binomial\index{binomial distribution} sampling. In a real field study, the model will be an approximation so any design choices should be viewed as lower bounds. For example, if four removal samples appear to be sufficient, consider that a minimum or perhaps low. Another consideration is that we have made the simplifying assumption that capture probability\index{capture probability} is constant across samples. @bryant_2000 found that capture probability\index{capture probability} was generally constant when using traps, but it may decline with disruptive methods such as electrofishing or seining, or if vulnerability varies among individuals [@mantyniemi.etal2005]. If capture probability\index{capture probability} declines as additional samples are taken, a more complex function would be needed for capture probability\index{capture probability}.

## Mark-recapture {#MarkRecap}

Population size within a study area can also be estimated through a two-sample mark-recapture\index{mark-recapture} experiment. Fish captured in the first sample are given a tag or mark, and the population estimate is based on the fraction of marked fish in the second sample. We change one line of the code from the example in Section \@ref(FirstExampleBayesian) to produce a random number of recaptured fish rather than the expected value. Now the combined code (simulation plus analysis) can be rerun multiple times to build intuition about the reliability of the experimental results. This is especially useful when the assumed population size or capture probability\index{capture probability} is low. The true value can be compared to the 95% credible interval\index{credible interval} (2.5 and 97.5 percentiles, printed to the Console) or 80% credible interval shown in the plots. You can look at either the marked fraction or N.hat, as they covary (low estimate for marked fraction produces high estimate for N.hat).

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Simulate experiment
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Caught and marked in first sample
n2 <- N.true * p.true # Caught and examined for marks in second sample
m2 <- rbinom(n=1, size=n2, prob=p.true) # Marked fish in second sample

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code
sink("TwoSampleCR.txt")
cat("
    model {

    # Priors
    MarkedFraction ~ dunif(0, 1)

    # Calculated value
    N.hat <- n1 /MarkedFraction

    # Likelihood
    # Binomial distribution for observed recaptures
    m2 ~ dbin(MarkedFraction, n2)
    }

    ",fill = TRUE)
sink()

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(MarkedFraction=runif(1, min=0, max=1))}

model.file <- 'TwoSampleCR.txt'

# Parameters monitored
jags.params <- c("N.hat", "MarkedFraction")

# Call JAGS from R
jagsfit <- jags(data=jags.data, inits=jags.inits, jags.params,
                n.chains = 3, n.thin = 1, n.iter = 2000,
                n.burnin = 1000, model.file)
print(jagsfit, digits=3)
plot(jagsfit)

par(mfrow=c(3,1))
hist(jagsfit$BUGSoutput$sims.array[,,1], main="", 
     xlab="Marked fraction")
hist(jagsfit$BUGSoutput$sims.array[,,2], main="", 
     xlab="Population estimate")
plot(jagsfit$BUGSoutput$sims.array[,,1], 
     jagsfit$BUGSoutput$sims.array[,,2], xlab="Marked fraction",
     ylab="Population estimate")
```

The last four lines of code illustrate that it can be instructive to examine not only the posterior distributions\index{posterior distribution} but also the underlying relationship(s); see bivariate plot in Figure \@ref(fig:PopEst-covary). Here it is clear that N.hat is inversely related to the model parameter for marked fraction, but it may be less obvious in situations with more complex models.

```{r PopEst-covary, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Estimated posterior distributions for marked fraction and population size and bivariate plot of posterior samples from two-sample mark-recapture study.', fig.show="hold", out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(12345) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1, 2))

Covary <- function() {
    # Priors
    MarkedFraction ~ dunif(0, 1)

    # Calculated value
    N.hat <- n1 /MarkedFraction

    # Likelihood
    # Binomial distribution for observed recaptures
    m2 ~ dbin(MarkedFraction, n2)
}

# Simulate experiment
N.true <- 400  # Population size
p.true <- 0.2 # Capture probability (fraction of population sampled)
n1 <- N.true * p.true # Number caught and marked in first sample
n2 <- N.true * p.true # Number caught and examined for marks in second sample
m2 <- rbinom(n=1, size=n2, prob=p.true) # Marked fish in second sample

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("n1", "n2", "m2")

# Initial values.
jags.inits <- function(){ list(MarkedFraction=runif(1, min=0, max=1))}

# Fit the model
MCR_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 2000, n.burnin = 1000,
    model.file = Covary,
    parameters.to.save = c("MarkedFraction", "N.hat")
  )

par(mfrow=c(3,1))
hist(MCR_jags$BUGSoutput$sims.array[,,1], main="", xlab="Marked fraction")
hist(MCR_jags$BUGSoutput$sims.array[,,2], main="", xlab="Population estimate")
plot(MCR_jags$BUGSoutput$sims.array[,,1], MCR_jags$BUGSoutput$sims.array[,,2],
     xlab="Marked fraction", ylab="Population estimate")

```

If you were managing this fish population (e.g., a community lake), how might you use the estimate of abundance? Try a few runs (simulation and model fitting) to see whether this level of uncertainty would be tolerable for management. In doing so, keep in mind that this assumed capture probability\index{capture probability} (0.2) is quite high and would be difficult to achieve in many field situations. Is there a rough lower bound for capture probability\index{capture probability}, below which the cost and effort of doing a study would not be justified?

An example with empirical data is included as Appendix \@ref(MarkRecapRealData).

## Binomial-mixture {#NMix}

A population estimate can sometimes be obtained from count data when replicate survey samples are taken [@royle_2004; @kéry.schaub_2012]. One fisheries example is the use of side-scan sonar to count adult Atlantic sturgeon (*Acipenser oxyrinchus oxyrinchus*), which are large and distinctive enough to be identifiable on side-scan images [@flowers.hightower_2013; @flowers.hightower_2015]. Sites were assumed to be closed (no movement in or out, no mortality) for the brief duration of the survey. Each site had an unknown number of sturgeon, but it was not assumed that every sturgeon was counted (detected). Counts could vary among survey passes due to a variety of factors, including a fish's orientation, overlap with other fish or structure, or distortion in the image. Thus the counting process was modeled using a binomial distribution\index{binomial distribution}, where "size" is the number of sturgeon present at the site, and "prob" is the probability of being detected on that survey pass. The detection probability\index{detection probability} is comparable to a capture probability\index{capture probability} but here sampling was unobtrusive. This type of model is referred to as a binomial-mixture model\index{binomial-mixture model} (or N-mixture model\index{N-mixture model|see{binomial-mixture model}}). It attempts to separate the biological parameter (site abundance) from the observational parameter (detection probability\index{detection probability}) [@royle_2004; @kéry.schaub_2012].

Consider a site with 20 individuals and a detection probability\index{detection probability} of 0.5. In an actual field survey, there would typically be only a few replicate passes per site. Here we simulate a large sample of replicates to get a smooth distribution of counts:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

# Simulate experiment
N.true <- 20  # Number of individuals at site
p.true <- 0.5 # Detection probability
Replicates <- 1000
Counts <- rbinom(n=Replicates, size=N.true, prob=p.true)
  # Vector of replicate counts

par(mfrow=c(1,1)) # Ensure plot window is reset to default setting
hist(Counts, main="")
table(Counts)
```

The expected count is 10 (N.true * p.true); the histogram and table show the frequency of different values. One run produced minimum and maximum counts of 3 and 16. Counting 20 would be possible but extremely unlikely. One way to estimate empirically the probability of counting 15 or more is the following expression, which uses square brackets to subset the Counts vector: <code>length(Counts[Counts>15])/Replicates</code>. Try <code>Counts[Counts>15]</code> in the Console to make clear how this works. Experiment with different detection probabilities\index{detection probability} to find a level that produces occasional counts of 20 individuals.

To simulate a full field study, we need to add code for multiple sites. The number of visits per site can vary but, for simplicity, let's assume 30 sites and three replicate visits to each site. @kéry.schaub_2012 recommended at least 10-20 sites and at least two observations per site. It is assumed that the number of sturgeon per site varies about an underlying mean, so having a large sample of sites will aid in estimating that mean. There are a variety of ways to model the variation in abundance among sites. Here we choose the simplest option of using a Poisson distribution\index{Poisson distribution}, which has only one parameter to estimate:

```{r eval=FALSE}
rm(list=ls()) # Clear Environment

Sites <- 30
Replicates <- 3
lam.true <- 20 # Mean number per site
p.true <- 0.5 # Detection probability

Counts <- matrix(data=NA, nrow=Sites, ncol=Replicates)
Site.N <- rpois(n = Sites, lambda = lam.true) # True abundance per site
for (i in 1:Sites){
  Counts[i,] <- rbinom(n = Replicates, size = Site.N[i], prob = p.true)
  } #i
head(Site.N)
head(Counts)
```

The <code>matrix()</code> function creates an empty matrix (values "Not Available"), which will hold the replicate counts for each site. The simulation has a hierarchical\index{hierarchical model} structure. The number of sturgeon at each site is drawn from a Poisson distribution\index{Poisson distribution}, using the shared mean lam.true and the <code>rpois()</code> function. We then loop over sites to generate the binomially-distributed\index{binomial distribution} replicate counts, given abundance at that site. We can use the <code>head()</code> function to look at the relationship between true abundance (Site.N) and counts for the first few sites. In one example run, the first site had 25 sturgeon and produced replicate counts of 11, 14, and 13. Try running <code>rbinom(n=3, size=25, prob=x)</code> multiple times in the Console with x=0.5 versus a probability near 0 or 1. The magnitude and variation among replicate counts within a site provide information about the detection probability\index{detection probability} and true site abundance. 

```{r eval=FALSE}
# Load necessary library packages
library(rjags)
library(R2jags)

# Specify model in BUGS language
sink("N-mix.txt")
cat("
model {

# Priors
  lam.est ~ dunif(0, 100)  # uninformative prior for mean abundance
  p.est ~ dunif(0, 1)

# Likelihood
# Biological model for true abundance
for (i in 1:Sites) {
   N.est[i] ~ dpois(lam.est)
   # Observation model for replicated counts
   for (j in 1:Replicates) {
      Counts[i,j] ~ dbin(p.est, N.est[i])
      } # j
   } # i
totalN <- sum(N.est[])     
  # Calculated variable: total population size across all sites
}
",fill = TRUE)
sink()

# Bundle data
jags.data <- list("Counts", "Sites", "Replicates")

# Initial values
Nst <- apply(Counts, MARGIN=1, max) + 1	# Kéry and Schaub
jags.inits <- function(){ list(lam.est=runif(n=1, min=0, max=100),
                               N.est=Nst, p.est=runif(1, min=0, max=1))}

model.file <- 'N-mix.txt'

# Parameters monitored
jags.params <- c("lam.est", "p.est", 
                 #"N.est",  # To see individual site estimates
                 "totalN")

# Call JAGS from R
jagsfit <- jags(data=jags.data, jags.params, inits=jags.inits,
                n.chains = 3, n.thin=1, n.iter = 100000,
                model.file)
print(jagsfit)
par(mfrow=c(1,1)) # Reset plot panel
plot(jagsfit)

# Estimated posterior distribution for total population size
hist(jagsfit$BUGSoutput$sims.list$totalN, 
     xlab="Estimated total pop size", main="")
abline(v=sum(Site.N), lty=3, lwd=3)

```

The JAGS code includes an uninformative prior\index{uninformative prior distribution} for detection probability\index{detection probability}. The prior for mean site abundance is more subjective, but the upper bound could be based on the literature or knowledge about the system. As in the simulation code, the JAGS code has a hierarchical\index{hierarchical model} structure. Estimated overall mean abundance is used to draw site-specific abundance estimates from the shared Poisson distribution\index{Poisson distribution}. The observations are binomially distributed\index{binomial distribution}, given the estimated abundance at each site. It is sometimes of interest to look at total abundance over sites, so the site estimates are summed (totalN).

It is important to provide initial values for site abundance. JAGS will terminate the run if a nonsensical event occurs (e.g., count greater than the current estimate of abundance at a site). @kéry.schaub_2012 provide example code using the <code>apply()</code> function, which is a generic function for applying (hence the name) other functions. In this case, the <code>max()</code> function is applied to the Count matrix by row (argument MARGIN=1). The returned maximum value for each row is incremented by 1, implying that there is likely to be at least one more sturgeon present than the maximum observed count. The remaining parameters are initialized using settings that match the priors. The final lines of R code show the estimated posterior distribution\index{posterior distribution} for total abundance (summed over sites). The abline function is used to add to the plot the true total abundance (dotted vertical line).

Estimates are generally reliable for these assumed parameter values and study design settings. Convergence\index{convergence} was sometimes slow, so a large number of updates was used. It is worthwhile to run the combined code (simulation plus analysis) several times to see variation in the estimates and their credible intervals. Median estimates of mean site abundance and detection probability tend to be close to the assumed values, although credible intervals are fairly wide. In sample runs, the mode of the posterior distribution for total abundance (Figure \@ref(fig:NmixPlot)) was close to the true value, but the distribution was skewed and had a high upper bound for the credible interval.

```{r NmixPlot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.cap='Estimated posterior distribution for total population size (summed over sites) and true total (dotted vertical line) based on binomial-mixture study.', out.width="90%"}

rm(list=ls()) # Clear Environment
set.seed(11111) # Ensure that book will have a reasonable result
par(mar = c(4, 4, 1.5, .1)) # Figure cut off. BLTR. Try top 1.5 instead of 1

NMix <- function() {
# Priors
  lam.est ~ dunif(0, 100)  # uninformative prior for mean abundance at each site
  p.est ~ dunif(0, 1)

# Likelihood
# Biological model for true abundance
for (i in 1:Sites) {
   N.est[i] ~ dpois(lam.est)
   # Observation model for replicated counts
   for (j in 1:Replicates) {
      Counts[i,j] ~ dbin(p.est, N.est[i])
      } # j
   } # i
totalN <- sum(N.est[])  # Calculated variable: total pop. size across all sites
}

Sites <- 30
Replicates <- 3
lam.true <- 20 # Mean number per site
p.true <- 0.5 # Detection probability

Counts <- matrix(NA, nrow=Sites, ncol=Replicates)
Site.N <- rpois(n = Sites, lambda = lam.true) # True abundance at each site
for (i in 1:Sites){
  Counts[i,] <- rbinom(n = Replicates, size = Site.N[i], prob = p.true)
} #i

# Load necessary library packages
library(rjags)   # Package for fitting JAGS models from within R
library(R2jags)  # Package for fitting JAGS models. Requires rjags

# JAGS code

# Bundle data
jags.data <- list("Counts", "Sites", "Replicates")

# Initial values.
Nst <- apply(Counts, 1, max) + 1    # Kéry and Schaub
jags.inits <- function(){ list(lam.est=runif(n=1, min=0, max=100),
                               N.est=Nst, p.est=runif(1, min=0, max=1))}

# Fit the model
NMix_jags <- 
  jags(
    data = jags.data, inits=jags.inits,
    n.chains = 3, n.thin = 1, n.iter = 100000,
    model.file = NMix,
    parameters.to.save = c("lam.est", "p.est", "totalN")
  )

# Estimated posterior distribution for total population size
hist(NMix_jags$BUGSoutput$sims.list$totalN, 
     xlab="Estimated total population size", main="")
abline(v=sum(Site.N), lty=3, lwd=3)

```

This model is a good illustration of the potentially problematic nature of pV\index{pV}, the estimated number of effective parameters\index{effective parameters}. A nominal count of parameters would be around 32, to account for detection probability\index{detection probability}, mean site abundance, and the site-specific abundance estimates using that mean. However, estimates from several runs were around 60. @kéry.schaub_2012 note that hierachical models (such as this one) can be problematic for estimating the number of effective parameters\index{effective parameters} (and therefore for DIC\index{Deviance Information Criterion (DIC)}). We can look at pV\index{pV} for the various models in this book, to get some experience about when to trust pV\index{pV} and DIC\index{Deviance Information Criterion (DIC)} estimates.

As usual, keep in mind that this is a very optimistic scenario. We are estimating mean abundance and detection probability\index{detection probability} using 30 sites that function as spatial replicates. The data are generated using a detection probability\index{detection probability} that is high and constant over sites. In an actual field study, there would be additional variation in detection probability\index{detection probability} among sites, and a binomial distribution\index{binomial distribution} would only be an approximation to the field sampling process that generates the observed counts. We have also assumed that variation among sites follows a Poisson distribution\index{Poisson distribution}. Other distributions that allow for more variation among sites, such as a negative binomial\index{negative binomial distribution}, are possible and can be much more challenging to fit.


## Exercises

1. For the removal experiment, create a table of estimated credible intervals for population size, using several replicate simulations and a range of values for capture probability. What are the lower limits below which estimates are judged not to be reliable?

2. For a two-sample mark-recapture experiment with a true population size of 1000, produce a table of credible intervals for N.hat for different assumed capture probabilities. What sampling intensity would produce an estimate with uncertainty close to the Robson and Regier target for management (+/- 25%)?

3. Modify the binomial-mixture example to compare credible intervals for estimated mean abundance (lam.est) using fewer sites but more replicates, preserving the same total number of observations (e.g., ten sites and nine replicate survey visits). Can you suggest potential advantages and disadvantages of using fewer sites?
